{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f5adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import warnings\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def check_cuda_environment():\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available. This notebook requires a GPU.\")\n",
    "    \n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    \n",
    "    try:\n",
    "        test_tensor = torch.tensor([1.0, 2.0, 3.0], device='cuda')\n",
    "        test_result = test_tensor + 1\n",
    "        print(f\"CUDA test operation successful: {test_result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"CUDA test operation failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        nvidia_smi = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        print(\"NVIDIA-SMI output:\")\n",
    "        print(nvidia_smi.stdout)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run nvidia-smi: {e}\")\n",
    "    \n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 2**30:.2f} GiB\")\n",
    "    print(f\"Allocated GPU memory: {torch.cuda.memory_allocated(0) / 2**30:.2f} GiB\")\n",
    "    print(f\"Reserved GPU memory: {torch.cuda.memory_reserved(0) / 2**30:.2f} GiB\")\n",
    "\n",
    "try:\n",
    "    check_cuda_environment()\n",
    "except Exception as e:\n",
    "    print(f\"Error with PyTorch or CUDA setup: {e}\")\n",
    "    print(\"Try reinstalling PyTorch: pip install torch==2.7.0 --index-url https://download.pytorch.org/whl/cu124\")\n",
    "    raise\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"PATH: {os.environ.get('PATH')}\")\n",
    "print(f\"Available disk space: {shutil.disk_usage('/').free / (2**30):.2f} GiB\")\n",
    "\n",
    "if os.path.exists('/workspace/XAI/torch.py') or os.path.exists('/workspace/XAI/torch.pyc'):\n",
    "    print(\"Warning: Found 'torch.py' or 'torch.pyc' in /workspace/XAI. Please rename or remove it.\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared GPU memory cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea46cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"/workspace/data\", file_name=\"merged_data.csv\"):\n",
    "    \"\"\"Load CSV data with fallback path.\"\"\"\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    alt_path = \"/workspace/XAI-2/Predict Future Sales/merged_data.csv\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        if os.path.exists(alt_path):\n",
    "            file_path = alt_path\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File not found at {file_path} or {alt_path}\")\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"Loaded data from {file_path}\")\n",
    "        print(f\"Dataset shape: {data.shape}\")\n",
    "        print(f\"Columns: {list(data.columns)}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load {file_path}: {e}\")\n",
    "    \n",
    "    expected_columns = ['date', 'shop_id', 'item_id', 'item_name', 'item_cnt_day', \n",
    "                        'item_price', 'item_category_id', 'shop_name', 'item_category_name', 'date_block_num']\n",
    "    missing_cols = [col for col in expected_columns if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing expected columns: {missing_cols}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b7b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datetime import datetime\n",
    "\n",
    "def get_russian_holidays():\n",
    "    \"\"\"Return DataFrame of Russian holidays and shopping events (2013–2015).\"\"\"\n",
    "    holidays = [\n",
    "        (\"2013-01-01\", \"New Year\"), (\"2014-01-01\", \"New Year\"), (\"2015-01-01\", \"New Year\"),\n",
    "        (\"2013-02-23\", \"Defender Day\"), (\"2014-02-23\", \"Defender Day\"), (\"2015-02-23\", \"Defender Day\"),\n",
    "        (\"2013-03-08\", \"Women's Day\"), (\"2014-03-08\", \"Women's Day\"), (\"2015-03-08\", \"Women's Day\"),\n",
    "        (\"2013-06-12\", \"Russia Day\"), (\"2014-06-12\", \"Russia Day\"), (\"2015-06-12\", \"Russia Day\"),\n",
    "        (\"2013-11-29\", \"Black Friday\"), (\"2014-11-28\", \"Black Friday\"), (\"2015-11-27\", \"Black Friday\")\n",
    "    ]\n",
    "    holiday_df = pl.DataFrame({\n",
    "        \"date\": [datetime.strptime(date, \"%Y-%m-%d\") for date, _ in holidays],\n",
    "        \"holiday\": [name for _, name in holidays]\n",
    "    }).with_columns([\n",
    "        pl.col(\"date\").dt.month().cast(pl.Int32).alias(\"month\"),\n",
    "        pl.col(\"date\").dt.year().cast(pl.Int32).alias(\"year\")\n",
    "    ])\n",
    "    return holiday_df\n",
    "\n",
    "holiday_df = get_russian_holidays()\n",
    "print(f\"Holiday DataFrame created with {len(holiday_df)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca5b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import polars as pl\n",
    "\n",
    "def initial_preprocessing(df):\n",
    "    \"\"\"Perform initial data cleaning and filtering, including shop selection.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert to Polars\n",
    "    df = pl.from_pandas(df)\n",
    "    print(f\"Initial dataset size: {len(df)}\")\n",
    "    print(f\"Unique shops: {df['shop_id'].n_unique()}, items: {df['item_id'].n_unique()}\")\n",
    "    print(f\"Date block range: {df['date_block_num'].min()}–{df['date_block_num'].max()}\")\n",
    "    \n",
    "    # Parse dates\n",
    "    df = df.with_columns(\n",
    "        pl.col('date').str.strptime(pl.Date, \"%d.%m.%Y\")\n",
    "    ).with_columns([\n",
    "        pl.col('date').dt.month().cast(pl.Int32).alias('month'),\n",
    "        pl.col('date').dt.year().cast(pl.Int32).alias('year')\n",
    "    ])\n",
    "    \n",
    "    # Filter to date_block_num <= 32\n",
    "    df = df.filter(pl.col('date_block_num') <= 32)\n",
    "    print(f\"Dataset size after date filter: {len(df)}\")\n",
    "    \n",
    "    # Filter to top 54 shops by total sales volume\n",
    "    shop_sales = df.group_by('shop_id').agg(\n",
    "        total_sales=pl.col('item_cnt_day').sum()\n",
    "    ).sort('total_sales', descending=True).head(54)\n",
    "    valid_shops = shop_sales['shop_id'].to_list()\n",
    "    df = df.filter(pl.col('shop_id').is_in(valid_shops))\n",
    "    print(f\"Dataset size after shop filter: {len(df)}\")\n",
    "    print(f\"Unique shops after processing: {df['shop_id'].n_unique()}\")\n",
    "    \n",
    "    # Optimize dtypes\n",
    "    df = df.with_columns([\n",
    "        pl.col('date_block_num').cast(pl.Int16),\n",
    "        pl.col('shop_id').cast(pl.Int32),\n",
    "        pl.col('item_id').cast(pl.Int32),\n",
    "        pl.col('item_category_id').cast(pl.Int32),\n",
    "        pl.col('item_cnt_day').cast(pl.Float32),\n",
    "        pl.col('item_price').cast(pl.Float32),\n",
    "        pl.col('month').cast(pl.Int32),\n",
    "        pl.col('year').cast(pl.Int32)\n",
    "    ])\n",
    "    \n",
    "    print(f\"Initial preprocessing time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = initial_preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fac64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_negative_sales_and_outliers(df):\n",
    "    \"\"\"Apply rolling Winsorization and handle negative sales at daily level.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Diagnostics: Inspect item_cnt_day distribution\n",
    "    print(\"item_cnt_day distribution before processing:\")\n",
    "    print(df['item_cnt_day'].describe())\n",
    "    print(f\"Rows with item_cnt_day > 100: {df.filter(pl.col('item_cnt_day') > 100).height}\")\n",
    "    print(f\"Rows with item_cnt_day > 500: {df.filter(pl.col('item_cnt_day') > 500).height}\")\n",
    "    \n",
    "    # Check for sparse shop-item pairs\n",
    "    shop_item_counts = df.group_by(['shop_id', 'item_id']).agg(\n",
    "        day_count=pl.col('date').count()\n",
    "    )\n",
    "    print(f\"Shop-item pairs with < 30 days: {shop_item_counts.filter(pl.col('day_count') < 30).height}\")\n",
    "    \n",
    "    # Sort for rolling operations\n",
    "    df = df.sort(['shop_id', 'item_id', 'date'])\n",
    "    \n",
    "    # Rolling Winsorization (30-day window, 95th percentile) with static cap\n",
    "    df = df.with_columns(\n",
    "        rolling_quantile=pl.col('item_cnt_day').rolling_quantile(\n",
    "            quantile=0.95, window_size=30, min_periods=1\n",
    "        ).over(['shop_id', 'item_id'])\n",
    "    ).with_columns(\n",
    "        item_cnt_day_winsor=pl.col('item_cnt_day').clip(None, pl.min_horizontal(pl.col('rolling_quantile'), 1000))\n",
    "    )\n",
    "    outlier_count = df.filter(pl.col('item_cnt_day') > pl.col('item_cnt_day_winsor')).height\n",
    "    print(f\"Outliers capped: {outlier_count} ({outlier_count / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    # Diagnostics: Inspect rolling quantile and winsorized values\n",
    "    print(\"Rolling quantile stats:\")\n",
    "    print(df['rolling_quantile'].describe())\n",
    "    print(f\"Max item_cnt_day_winsor: {df['item_cnt_day_winsor'].max()}\")\n",
    "    \n",
    "    # Handle negative sales\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col('item_cnt_day_winsor') < 0)\n",
    "          .then(pl.col('item_cnt_day_winsor').abs())\n",
    "          .otherwise(0)\n",
    "          .alias('returns'),\n",
    "        pl.col('item_cnt_day_winsor').clip(lower_bound=0).alias('item_cnt_day_winsor')\n",
    "    ])\n",
    "    print(f\"Negative sales after processing: {df.filter(pl.col('item_cnt_day_winsor') < 0).height}\")\n",
    "    \n",
    "    # Drop temporary column\n",
    "    df = df.drop('rolling_quantile')\n",
    "    \n",
    "    print(f\"Negative sales and outlier handling time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = handle_negative_sales_and_outliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_to_monthly(df):\n",
    "    \"\"\"Aggregate data to monthly level.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Aggregate to monthly\n",
    "    df = df.group_by(['date_block_num', 'shop_id', 'item_id', 'item_category_id', 'month', 'year']).agg([\n",
    "        pl.col('item_cnt_day_winsor').sum().alias('item_cnt_day_winsor'),\n",
    "        pl.col('returns').sum().alias('returns'),\n",
    "        pl.col('item_price').mean().alias('item_price')\n",
    "    ])\n",
    "    \n",
    "    # Diagnostics: Inspect aggregated item_cnt_day_winsor\n",
    "    print(\"item_cnt_day_winsor distribution after aggregation:\")\n",
    "    print(df['item_cnt_day_winsor'].describe())\n",
    "    \n",
    "    print(f\"Dataset size after aggregation: {len(df)}\")\n",
    "    print(f\"Unique shops: {df['shop_id'].n_unique()}, items: {df['item_id'].n_unique()}\")\n",
    "    print(f\"Aggregation time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = aggregate_to_monthly(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db487dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_grid(df):\n",
    "    \"\"\"Create full shop-item-month grid.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    shops = df['shop_id'].unique().to_list()\n",
    "    items = df['item_id'].unique().to_list()\n",
    "    date_blocks = list(range(33))  # 0–32\n",
    "    \n",
    "    grid = pl.DataFrame({'shop_id': shops}).join(\n",
    "        pl.DataFrame({'item_id': items}), how='cross'\n",
    "    ).join(\n",
    "        pl.DataFrame({'date_block_num': date_blocks}), how='cross'\n",
    "    ).with_columns([\n",
    "        pl.col('shop_id').cast(pl.Int32),\n",
    "        pl.col('item_id').cast(pl.Int32),\n",
    "        pl.col('date_block_num').cast(pl.Int16),\n",
    "        ((pl.col('date_block_num') % 12) + 1).cast(pl.Int32).alias('month'),\n",
    "        ((pl.col('date_block_num') // 12) + 2013).cast(pl.Int32).alias('year')\n",
    "    ])\n",
    "    \n",
    "    df = grid.join(\n",
    "        df, on=['shop_id', 'item_id', 'date_block_num', 'month', 'year'], how='left'\n",
    "    ).with_columns([\n",
    "        pl.col('item_cnt_day_winsor').fill_null(0),\n",
    "        pl.col('returns').fill_null(0),\n",
    "        pl.col('item_price').fill_null(pl.col('item_price').mean().over('item_id')).fill_null(0),\n",
    "        pl.col('item_category_id').fill_null(pl.col('item_category_id').first().over('item_id')).fill_null(0)\n",
    "    ]).with_columns(\n",
    "        pl.datetime(pl.col('year'), pl.col('month'), 1).alias('date')\n",
    "    )\n",
    "    \n",
    "    print(f\"Grid size: {len(grid)}, after merge: {len(df)}\")\n",
    "    print(f\"Unique shops: {df['shop_id'].n_unique()}, items: {df['item_id'].n_unique()}\")\n",
    "    print(f\"Grid creation time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = create_full_grid(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_imputation(df, col):\n",
    "    \"\"\"Apply seasonality-aware imputation to a column.\"\"\"\n",
    "    df = df.with_columns(\n",
    "        pl.col(col).interpolate().over(['shop_id', 'item_id']).alias(f'{col}_interp')\n",
    "    ).with_columns(\n",
    "        seasonal_value=pl.col(col).shift(12).over(['shop_id', 'item_id', 'month']),\n",
    "        ma_value=pl.col(col).rolling_mean(window_size=12, min_periods=1).over(['shop_id', 'item_id'])\n",
    "    ).with_columns(\n",
    "        pl.when(pl.col(f'{col}_interp').is_null() & pl.col('seasonal_value').is_not_null())\n",
    "          .then(pl.col('seasonal_value'))\n",
    "          .when(pl.col(f'{col}_interp').is_null())\n",
    "          .then(pl.col('ma_value'))\n",
    "          .otherwise(pl.col(f'{col}_interp'))\n",
    "          .alias(col)\n",
    "    ).drop([f'{col}_interp', 'seasonal_value', 'ma_value'])\n",
    "    return df\n",
    "\n",
    "def apply_imputation(df, cols):\n",
    "    \"\"\"Impute missing values for specified columns.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for col in cols:\n",
    "        df = seasonal_imputation(df, col)\n",
    "    df = df.with_columns([pl.col(col).fill_null(0) for col in cols])\n",
    "    \n",
    "    print(f\"Imputation time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "numerical_cols = ['item_cnt_day_winsor', 'returns', 'item_price']\n",
    "df = apply_imputation(df, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d9e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_holiday_features(df, holiday_df):\n",
    "    \"\"\"Add holiday features to the dataset.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = df.join(\n",
    "        holiday_df.select(['year', 'month', 'holiday']),\n",
    "        on=['year', 'month'], how='left'\n",
    "    ).with_columns(\n",
    "        is_holiday=pl.col('holiday').is_not_null().cast(pl.Int8),\n",
    "        holiday=pl.col('holiday').fill_null('None')\n",
    "    )\n",
    "    \n",
    "    print(f\"Holiday feature addition time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = add_holiday_features(df, holiday_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bba005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sparse_products(df):\n",
    "    \"\"\"Exclude shop-item pairs with >30% missing data.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    shop_item_missing = df.group_by(['shop_id', 'item_id']).agg(\n",
    "        missing_ratio=pl.col('item_cnt_day_winsor').eq(0).mean()\n",
    "    )\n",
    "    valid_shop_items = shop_item_missing.filter(pl.col('missing_ratio') <= 0.3).select(['shop_id', 'item_id'])\n",
    "    initial_size = len(df)\n",
    "    df = df.join(valid_shop_items, on=['shop_id', 'item_id'], how='inner')\n",
    "    \n",
    "    print(f\"Records dropped due to >30% missing: {initial_size - len(df)}\")\n",
    "    print(f\"Records after filtering: {len(df)}, shop-item pairs: {len(valid_shop_items)}\")\n",
    "    print(f\"Filtering time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = filter_sparse_products(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e38c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df):\n",
    "    \"\"\"Create lag features for 1–3 months.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = df.sort(['shop_id', 'item_id', 'date'])\n",
    "    for lag in [1, 2, 3]:\n",
    "        df = df.with_columns([\n",
    "            pl.col('item_cnt_day_winsor').shift(lag).over(['shop_id', 'item_id']).alias(f'lag_sales_{lag}'),\n",
    "            pl.col('returns').shift(lag).over(['shop_id', 'item_id']).alias(f'lag_returns_{lag}'),\n",
    "            pl.col('item_price').shift(lag).over(['shop_id', 'item_id']).alias(f'lag_price_{lag}')\n",
    "        ])\n",
    "    \n",
    "    print(f\"Lag feature creation time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = create_lag_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9390be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def scale_and_save_data(df, numerical_cols):\n",
    "    \"\"\"Apply robust scaling and save datasets.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    monthly_sales = df.to_pandas()\n",
    "    del df\n",
    "    \n",
    "    monthly_sales.to_parquet('/workspace/XAI-2/processed_data/monthly_sales_unscaled.parquet')\n",
    "    print(\"Saved unscaled data\")\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    train_data = monthly_sales[monthly_sales['date_block_num'] < 30][numerical_cols]\n",
    "    print(f\"Scaler training data shape: {train_data.shape}\")\n",
    "    \n",
    "    scaler.fit(train_data)\n",
    "    monthly_sales[numerical_cols] = scaler.transform(monthly_sales[numerical_cols])\n",
    "    \n",
    "    if monthly_sales[numerical_cols].isna().any().any():\n",
    "        raise ValueError(\"NaNs introduced during scaling\")\n",
    "    \n",
    "    with open('/workspace/XAI-2/processed_data/scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    dtypes = {col: 'float32' for col in numerical_cols}\n",
    "    dtypes.update({\n",
    "        'shop_id': 'int32', 'item_id': 'int32', 'item_category_id': 'int32',\n",
    "        'date_block_num': 'int16', 'month': 'int32', 'year': 'int32', 'is_holiday': 'int8'\n",
    "    })\n",
    "    monthly_sales = monthly_sales.astype(dtypes, errors='ignore')\n",
    "    \n",
    "    print(f\"Scaling and saving time: {time.time() - start_time:.2f} seconds\")\n",
    "    return monthly_sales\n",
    "\n",
    "monthly_sales = scale_and_save_data(df, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1444950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_save_sets(df):\n",
    "    \"\"\"Split data into train/val/test per paper (months 0–30, 31, 32) and save X and y.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_df = df[df['date_block_num'] <= 30]\n",
    "    val_df = df[df['date_block_num'] == 31]\n",
    "    test_df = df[df['date_block_num'] == 32]\n",
    "    \n",
    "    target_col = 'item_cnt_day_winsor'\n",
    "    exclude_cols = [target_col, 'holiday']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[[target_col]]\n",
    "    X_val = val_df[feature_cols]\n",
    "    y_val = val_df[[target_col]]\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df[[target_col]]\n",
    "    \n",
    "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "    \n",
    "    output_dir = '/workspace/XAI-2/processed_data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    X_train.to_parquet(os.path.join(output_dir, 'X_train_processed.parquet'))\n",
    "    y_train.to_parquet(os.path.join(output_dir, 'y_train_processed.parquet'))\n",
    "    X_val.to_parquet(os.path.join(output_dir, 'X_val_processed.parquet'))\n",
    "    y_val.to_parquet(os.path.join(output_dir, 'y_val_processed.parquet'))\n",
    "    X_test.to_parquet(os.path.join(output_dir, 'X_test_processed.parquet'))\n",
    "    y_test.to_parquet(os.path.join(output_dir, 'y_test_processed.parquet'))\n",
    "    \n",
    "    df.to_parquet('/workspace/XAI-2/raw_data/processed_sales.parquet')\n",
    "    \n",
    "    print(f\"Split and save time: {time.time() - start_time:.2f} seconds\")\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_and_save_sets(monthly_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae22f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(processed_df, expected_size=2935849, raw_cv_target=2.8, processed_cv_target=1.9):\n",
    "    \"\"\"Validate processed dataset against paper's expectations.\"\"\"\n",
    "    start_time = time.time()\n",
    "    results = {}\n",
    "    \n",
    "    # Dataset Size\n",
    "    results['dataset_size'] = {\n",
    "        'raw_size': len(processed_df),\n",
    "        'expected_processed_size': expected_size,\n",
    "        'status': 'PASS' if abs(len(processed_df) - expected_size) / expected_size <= 0.1 else 'FAIL'\n",
    "    }\n",
    "    print(f\"Dataset Size - Raw: {len(processed_df)}, Expected Processed: {expected_size}\")\n",
    "    print(f\"Dataset Size - Status: {results['dataset_size']['status']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Sparsity\n",
    "    zero_sales = processed_df.filter(pl.col('item_cnt_day_winsor') == 0).height\n",
    "    sparsity_ratio = zero_sales / len(processed_df)\n",
    "    results['sparsity'] = {\n",
    "        'zero_sales': zero_sales,\n",
    "        'sparsity_ratio': sparsity_ratio,\n",
    "        'status': 'PASS' if 0.2 <= sparsity_ratio <= 0.5 else 'FAIL'\n",
    "    }\n",
    "    print(f\"Sparsity - Zero sales: {zero_sales} ({sparsity_ratio:.2%})\")\n",
    "    print(f\"Sparsity - Status: {results['sparsity']['status']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Coefficient of Variation\n",
    "    processed_cv = processed_df['item_cnt_day_winsor'].std() / processed_df['item_cnt_day_winsor'].mean()\n",
    "    results['processed_cv'] = {\n",
    "        'processed_cv': processed_cv,\n",
    "        'target': processed_cv_target,\n",
    "        'status': 'PASS' if abs(processed_cv - processed_cv_target) / processed_cv_target <= 0.2 else 'FAIL'\n",
    "    }\n",
    "    print(f\"Processed CV - Value: {processed_cv:.2f}, Target: {processed_cv_target}\")\n",
    "    print(f\"Processed CV - Status: {results['processed_cv']['status']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Null Values\n",
    "    null_counts = processed_df.select(pl.col('item_cnt_day_winsor', 'returns', 'item_price').is_null().sum()).to_dict()\n",
    "    results['null_values'] = {\n",
    "        'null_counts': null_counts,\n",
    "        'status': 'PASS' if all(count == 0 for count in null_counts.values()) else 'FAIL'\n",
    "    }\n",
    "    print(f\"Null Values - Counts: {null_counts}\")\n",
    "    print(f\"Null Values - Status: {results['null_values']['status']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Negative Values\n",
    "    negative_count = processed_df.filter(pl.col('item_cnt_day_winsor') < 0).height\n",
    "    results['negative_values'] = {\n",
    "        'negative_count': negative_count,\n",
    "        'status': 'PASS' if negative_count == 0 else 'FAIL'\n",
    "    }\n",
    "    print(f\"Negative Values - Count: {negative_count}\")\n",
    "    print(f\"Negative Values - Status: {results['negative_values']['status']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Outliers\n",
    "    q1 = processed_df['item_cnt_day_winsor'].quantile(0.25)\n",
    "    q3 = processed_df['item_cnt_day_winsor'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    outlier_count = processed_df.filter(\n",
    "        (pl.col('item_cnt_day_winsor') < q1 - 1.5 * iqr) | (pl.col('item_cnt_day_winsor') > q3 + 1.5 * iqr)\n",
    "    ).height\n",
    "    results['outliers'] = {\n",
    "        'outlier_count': outlier_count,\n",
    "        'status': 'PASS' if outlier_count / len(processed_df) < 0.1 else 'FAIL'\n",
    "    }\n",
    "    print(f\"Outliers - Count: {outlier_count} ({outlier_count / len(processed_df):.2%})\")\n",
    "    print(f\"Outliers - Status: {results['outliers']['status']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Shop and Item Counts\n",
    "    shop_count = processed_df['shop_id'].n_unique()\n",
    "    item_count = processed_df['item_id'].n_unique()\n",
    "    results['shop_item_counts'] = {\n",
    "        'shops': shop_count,\n",
    "        'items': item_count,\n",
    "        'status': 'PASS' if shop_count == 54 and 10000 <= item_count <= 15000 else 'FAIL'\n",
    "    }\n",
    "    print(f\"Shops: {shop_count}, Items: {item_count}\")\n",
    "    print(f\"Shop/Item Counts - Status: {results['shop_item_counts']['status']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"Validation time: {time.time() - start_time:.2f} seconds\")\n",
    "    return results\n",
    "\n",
    "# Load processed data for validation\n",
    "processed_df = pl.from_pandas(monthly_sales)\n",
    "validation_results = validate_dataset(processed_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
