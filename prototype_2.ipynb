{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "16b48f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "GPU device: NVIDIA RTX A6000\n",
      "GPU count: 1\n",
      "Current device: 0\n",
      "CUDA test operation successful: tensor([2., 3., 4.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA-SMI output:\n",
      "Mon May 12 18:28:05 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:53:00.0 Off |                  Off |\n",
      "| 30%   32C    P0             23W /  300W |     316MiB /  49140MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "Total GPU memory: 47.43 GiB\n",
      "Allocated GPU memory: 0.00 GiB\n",
      "Reserved GPU memory: 0.00 GiB\n",
      "Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n",
      "Python executable: /workspace/XAI-2/XAI/nvenv/bin/python\n",
      "PATH: /workspace/XAI-2/XAI/nvenv/bin:/root/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/bin/remote-cli:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "Available disk space: 18.50 GiB\n",
      "Using device: cuda\n",
      "Cleared GPU memory cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import warnings\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def check_cuda_environment():\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available. This notebook requires a GPU.\")\n",
    "    \n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    \n",
    "    try:\n",
    "        test_tensor = torch.tensor([1.0, 2.0, 3.0], device='cuda')\n",
    "        test_result = test_tensor + 1\n",
    "        print(f\"CUDA test operation successful: {test_result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"CUDA test operation failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        nvidia_smi = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        print(\"NVIDIA-SMI output:\")\n",
    "        print(nvidia_smi.stdout)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run nvidia-smi: {e}\")\n",
    "    \n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 2**30:.2f} GiB\")\n",
    "    print(f\"Allocated GPU memory: {torch.cuda.memory_allocated(0) / 2**30:.2f} GiB\")\n",
    "    print(f\"Reserved GPU memory: {torch.cuda.memory_reserved(0) / 2**30:.2f} GiB\")\n",
    "\n",
    "try:\n",
    "    check_cuda_environment()\n",
    "except Exception as e:\n",
    "    print(f\"Error with PyTorch or CUDA setup: {e}\")\n",
    "    print(\"Try reinstalling PyTorch: pip install torch==2.7.0 --index-url https://download.pytorch.org/whl/cu124\")\n",
    "    raise\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"PATH: {os.environ.get('PATH')}\")\n",
    "print(f\"Available disk space: {shutil.disk_usage('/').free / (2**30):.2f} GiB\")\n",
    "\n",
    "if os.path.exists('/workspace/XAI-2/XAI/torch.py') or os.path.exists('/workspace/XAI-2/XAI/torch.pyc'):\n",
    "    print(\"Warning: Found 'torch.py' or 'torch.pyc' in /workspace/XAI-2/XAI. Please rename or remove it.\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared GPU memory cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7329b675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from /workspace/XAI-2/XAI/Predict Future Sales/merged_data.csv\n",
      "Dataset shape: (2935849, 10)\n",
      "Columns: ['date', 'date_block_num', 'shop_id', 'item_id', 'item_price', 'item_cnt_day', 'item_name', 'item_category_id', 'item_category_name', 'shop_name']\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_dir=\"/workspace/XAI-2/XAI/Predict Future Sales\", file_name=\"merged_data.csv\"):\n",
    "    \"\"\"Load CSV data with fallback path.\"\"\"\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    alt_path = \"/workspace/XAI-2/Predict Future Sales/merged_data.csv\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        if os.path.exists(alt_path):\n",
    "            file_path = alt_path\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File not found at {file_path} or {alt_path}\")\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"Loaded data from {file_path}\")\n",
    "        print(f\"Dataset shape: {data.shape}\")\n",
    "        print(f\"Columns: {list(data.columns)}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load {file_path}: {e}\")\n",
    "    \n",
    "    expected_columns = ['date', 'shop_id', 'item_id', 'item_name', 'item_cnt_day', \n",
    "                        'item_price', 'item_category_id', 'shop_name', 'item_category_name', 'date_block_num']\n",
    "    missing_cols = [col for col in expected_columns if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing expected columns: {missing_cols}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "69c6d3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls in item_name before imputation: 84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 0 nulls in item_name with 'Unknown'\n"
     ]
    }
   ],
   "source": [
    "def handle_null_names(df):\n",
    "    \"\"\"Impute nulls in item_name with 'Unknown' and verify nulls.\"\"\"\n",
    "    print(f\"Nulls in item_name before imputation: {df['item_name'].isna().sum()}\")\n",
    "    df['item_name'] = df['item_name'].fillna('Unknown')\n",
    "    print(f\"Imputed {df['item_name'].isna().sum()} nulls in item_name with 'Unknown'\")\n",
    "    return df\n",
    "\n",
    "data = handle_null_names(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "03428825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holiday DataFrame created with 15 entries\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from datetime import datetime\n",
    "\n",
    "def get_russian_holidays():\n",
    "    \"\"\"Return DataFrame of Russian holidays and shopping events (2013–2015).\"\"\"\n",
    "    holidays = [\n",
    "        (\"2013-01-01\", \"New Year\"), (\"2014-01-01\", \"New Year\"), (\"2015-01-01\", \"New Year\"),\n",
    "        (\"2013-02-23\", \"Defender Day\"), (\"2014-02-23\", \"Defender Day\"), (\"2015-02-23\", \"Defender Day\"),\n",
    "        (\"2013-03-08\", \"Women's Day\"), (\"2014-03-08\", \"Women's Day\"), (\"2015-03-08\", \"Women's Day\"),\n",
    "        (\"2013-06-12\", \"Russia Day\"), (\"2014-06-12\", \"Russia Day\"), (\"2015-06-12\", \"Russia Day\"),\n",
    "        (\"2013-11-29\", \"Black Friday\"), (\"2014-11-28\", \"Black Friday\"), (\"2015-11-27\", \"Black Friday\")\n",
    "    ]\n",
    "    holiday_df = pl.DataFrame({\n",
    "        \"date\": [datetime.strptime(date, \"%Y-%m-%d\") for date, _ in holidays],\n",
    "        \"holiday\": [name for _, name in holidays]\n",
    "    }).with_columns([\n",
    "        pl.col(\"date\").dt.month().cast(pl.Int32).alias(\"month\"),\n",
    "        pl.col(\"date\").dt.year().cast(pl.Int32).alias(\"year\")\n",
    "    ])\n",
    "    return holiday_df\n",
    "\n",
    "holiday_df = get_russian_holidays()\n",
    "print(f\"Holiday DataFrame created with {len(holiday_df)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ba1d6647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset size: 2935849\n",
      "Unique shops: 60, items: 21807\n",
      "Date block range: 0–33\n",
      "Dataset size after date filter: 2882335\n",
      "Dataset size after shop filter: 2868195\n",
      "Unique shops after processing: 54\n",
      "Initial preprocessing time: 1.81 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import polars as pl\n",
    "\n",
    "def initial_preprocessing(df):\n",
    "    \"\"\"Perform initial data cleaning and filtering, including shop selection.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert to Polars\n",
    "    df = pl.from_pandas(df)\n",
    "    print(f\"Initial dataset size: {len(df)}\")\n",
    "    print(f\"Unique shops: {df['shop_id'].n_unique()}, items: {df['item_id'].n_unique()}\")\n",
    "    print(f\"Date block range: {df['date_block_num'].min()}–{df['date_block_num'].max()}\")\n",
    "    \n",
    "    # Parse dates\n",
    "    df = df.with_columns(\n",
    "        pl.col('date').str.strptime(pl.Date, \"%d.%m.%Y\")\n",
    "    ).with_columns([\n",
    "        pl.col('date').dt.month().cast(pl.Int32).alias('month'),\n",
    "        pl.col('date').dt.year().cast(pl.Int32).alias('year')\n",
    "    ])\n",
    "    \n",
    "    # Filter to date_block_num <= 32\n",
    "    df = df.filter(pl.col('date_block_num') <= 32)\n",
    "    print(f\"Dataset size after date filter: {len(df)}\")\n",
    "    \n",
    "    # Filter to top 54 shops by total sales volume\n",
    "    shop_sales = df.group_by('shop_id').agg(\n",
    "        total_sales=pl.col('item_cnt_day').sum()\n",
    "    ).sort('total_sales', descending=True).head(54)\n",
    "    valid_shops = shop_sales['shop_id'].to_list()\n",
    "    df = df.filter(pl.col('shop_id').is_in(valid_shops))\n",
    "    print(f\"Dataset size after shop filter: {len(df)}\")\n",
    "    print(f\"Unique shops after processing: {df['shop_id'].n_unique()}\")\n",
    "    \n",
    "    # Optimize dtypes\n",
    "    df = df.with_columns([\n",
    "        pl.col('date_block_num').cast(pl.Int16),\n",
    "        pl.col('shop_id').cast(pl.Int32),\n",
    "        pl.col('item_id').cast(pl.Int32),\n",
    "        pl.col('item_category_id').cast(pl.Int32),\n",
    "        pl.col('item_cnt_day').cast(pl.Float32),\n",
    "        pl.col('item_price').cast(pl.Float32),\n",
    "        pl.col('month').cast(pl.Int32),\n",
    "        pl.col('year').cast(pl.Int32)\n",
    "    ])\n",
    "    \n",
    "    print(f\"Initial preprocessing time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = initial_preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "42630ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_cnt_day distribution before processing:\n",
      "shape: (9, 2)\n",
      "┌────────────┬────────────┐\n",
      "│ statistic  ┆ value      │\n",
      "│ ---        ┆ ---        │\n",
      "│ str        ┆ f64        │\n",
      "╞════════════╪════════════╡\n",
      "│ count      ┆ 2.868195e6 │\n",
      "│ null_count ┆ 0.0        │\n",
      "│ mean       ┆ 1.240954   │\n",
      "│ std        ┆ 2.28652    │\n",
      "│ min        ┆ -22.0      │\n",
      "│ 25%        ┆ 1.0        │\n",
      "│ 50%        ┆ 1.0        │\n",
      "│ 75%        ┆ 1.0        │\n",
      "│ max        ┆ 1000.0     │\n",
      "└────────────┴────────────┘\n",
      "Rows with item_cnt_day > 100: 135\n",
      "Rows with item_cnt_day > 500: 11\n",
      "Negative sales before processing: 7186 (0.25%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shop-item pairs with < 30 days: 395283\n",
      "Z-score multiplier used: 4.5\n",
      "Outliers capped: 11090 (0.39%)\n",
      "item_cnt_day_winsor stats:\n",
      "shape: (9, 2)\n",
      "┌────────────┬────────────┐\n",
      "│ statistic  ┆ value      │\n",
      "│ ---        ┆ ---        │\n",
      "│ str        ┆ f64        │\n",
      "╞════════════╪════════════╡\n",
      "│ count      ┆ 2.868195e6 │\n",
      "│ null_count ┆ 0.0        │\n",
      "│ mean       ┆ 1.231432   │\n",
      "│ std        ┆ 1.727064   │\n",
      "│ min        ┆ -22.0      │\n",
      "│ 25%        ┆ 1.0        │\n",
      "│ 50%        ┆ 1.0        │\n",
      "│ 75%        ┆ 1.0        │\n",
      "│ max        ┆ 300.0      │\n",
      "└────────────┴────────────┘\n",
      "Max item_cnt_day_winsor: 300.0\n",
      "Clipped values count: 14\n",
      "Negative sales after processing: 0\n",
      "Processed CV after clipping: 1.40\n",
      "Negative sales and outlier handling time: 0.68 seconds\n"
     ]
    }
   ],
   "source": [
    "def handle_negative_sales_and_outliers(df):\n",
    "    \"\"\"Apply z-score-based clipping and handle negative sales at daily level.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Diagnostics: Inspect item_cnt_day distribution\n",
    "    print(\"item_cnt_day distribution before processing:\")\n",
    "    print(df['item_cnt_day'].describe())\n",
    "    print(f\"Rows with item_cnt_day > 100: {df.filter(pl.col('item_cnt_day') > 100).height}\")\n",
    "    print(f\"Rows with item_cnt_day > 500: {df.filter(pl.col('item_cnt_day') > 500).height}\")\n",
    "    negative_count = df.filter(pl.col('item_cnt_day') < 0).height\n",
    "    print(f\"Negative sales before processing: {negative_count} ({negative_count / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    # Check for sparse shop-item pairs\n",
    "    shop_item_counts = df.group_by(['shop_id', 'item_id']).agg(\n",
    "        day_count=pl.col('date').count()\n",
    "    )\n",
    "    print(f\"Shop-item pairs with < 30 days: {shop_item_counts.filter(pl.col('day_count') < 30).height}\")\n",
    "    \n",
    "    # Sort for consistent processing\n",
    "    df = df.sort(['shop_id', 'item_id', 'date'])\n",
    "    \n",
    "    # Z-score-based clipping per shop-item pair\n",
    "    print(\"Z-score multiplier used: 4.5\")\n",
    "    df = df.with_columns([\n",
    "        pl.col('item_cnt_day').mean().over(['shop_id', 'item_id']).alias('mean'),\n",
    "        pl.col('item_cnt_day').std().over(['shop_id', 'item_id']).alias('std')\n",
    "    ]).with_columns(\n",
    "        item_cnt_day_winsor=pl.col('item_cnt_day').clip(\n",
    "            pl.col('mean') - 4.5 * pl.col('std'),\n",
    "            pl.min_horizontal(pl.col('mean') + 4.5 * pl.col('std'), 300)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Count outliers capped\n",
    "    outlier_count = df.filter((pl.col('item_cnt_day') < pl.col('item_cnt_day_winsor')) | \n",
    "                             (pl.col('item_cnt_day') > pl.col('item_cnt_day_winsor'))).height\n",
    "    print(f\"Outliers capped: {outlier_count} ({outlier_count / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    # Diagnostics: Inspect winsorized values\n",
    "    print(\"item_cnt_day_winsor stats:\")\n",
    "    print(df['item_cnt_day_winsor'].describe())\n",
    "    print(f\"Max item_cnt_day_winsor: {df['item_cnt_day_winsor'].max()}\")\n",
    "    print(f\"Clipped values count: {df.filter(pl.col('item_cnt_day_winsor') == 300).height}\")\n",
    "    \n",
    "    # Handle negative sales\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col('item_cnt_day_winsor') < 0)\n",
    "          .then(pl.col('item_cnt_day_winsor').abs())\n",
    "          .otherwise(0)\n",
    "          .alias('returns'),\n",
    "        pl.col('item_cnt_day_winsor').clip(lower_bound=0).alias('item_cnt_day_winsor')\n",
    "    ])\n",
    "    print(f\"Negative sales after processing: {df.filter(pl.col('item_cnt_day_winsor') < 0).height}\")\n",
    "    \n",
    "    # Calculate processed CV\n",
    "    processed_cv = df['item_cnt_day_winsor'].std() / df['item_cnt_day_winsor'].mean()\n",
    "    print(f\"Processed CV after clipping: {processed_cv:.2f}\")\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    df = df.drop(['mean', 'std'])\n",
    "    \n",
    "    print(f\"Negative sales and outlier handling time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = handle_negative_sales_and_outliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d88c20df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after daily deduplication: 2868168\n",
      "Daily duplicates after deduplication: 0\n",
      "Monthly duplicates found: 519897\n",
      "Duplicate details:\n",
      "shape: (5, 4)\n",
      "┌────────────────┬─────────┬─────────┬───────┐\n",
      "│ date_block_num ┆ shop_id ┆ item_id ┆ count │\n",
      "│ ---            ┆ ---     ┆ ---     ┆ ---   │\n",
      "│ i16            ┆ i32     ┆ i32     ┆ u32   │\n",
      "╞════════════════╪═════════╪═════════╪═══════╡\n",
      "│ 5              ┆ 30      ┆ 11859   ┆ 3     │\n",
      "│ 27             ┆ 37      ┆ 4178    ┆ 2     │\n",
      "│ 23             ┆ 31      ┆ 18587   ┆ 2     │\n",
      "│ 23             ┆ 4       ┆ 12472   ┆ 2     │\n",
      "│ 19             ┆ 15      ┆ 18114   ┆ 3     │\n",
      "└────────────────┴─────────┴─────────┴───────┘\n",
      "item_cnt_day_winsor distribution after aggregation:\n",
      "shape: (9, 2)\n",
      "┌────────────┬───────────┐\n",
      "│ statistic  ┆ value     │\n",
      "│ ---        ┆ ---       │\n",
      "│ str        ┆ f64       │\n",
      "╞════════════╪═══════════╡\n",
      "│ count      ┆ 1.56857e6 │\n",
      "│ null_count ┆ 0.0       │\n",
      "│ mean       ┆ 2.255657  │\n",
      "│ std        ┆ 7.887975  │\n",
      "│ min        ┆ 0.0       │\n",
      "│ 25%        ┆ 1.0       │\n",
      "│ 50%        ┆ 1.0       │\n",
      "│ 75%        ┆ 2.0       │\n",
      "│ max        ┆ 1000.0    │\n",
      "└────────────┴───────────┘\n",
      "Dataset size after aggregation: 1568570\n",
      "Unique shops: 54, items: 21309\n",
      "Aggregation time: 0.67 seconds\n"
     ]
    }
   ],
   "source": [
    "def aggregate_to_monthly(df):\n",
    "    \"\"\"Aggregate data to monthly level with duplicate handling and monthly cap.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Handle daily duplicates\n",
    "    df = df.group_by(['date', 'shop_id', 'item_id', 'item_category_id', 'month', 'year', 'date_block_num']).agg(\n",
    "        item_cnt_day_winsor=pl.col('item_cnt_day_winsor').sum(),\n",
    "        returns=pl.col('returns').sum(),\n",
    "        item_price=pl.col('item_price').mean()\n",
    "    )\n",
    "    print(f\"Dataset size after daily deduplication: {len(df)}\")\n",
    "    \n",
    "    # Log daily duplicates (should be 0 after deduplication)\n",
    "    daily_duplicates = df.group_by(['date', 'shop_id', 'item_id']).agg(count=pl.count()).filter(pl.col('count') > 1)\n",
    "    print(f\"Daily duplicates after deduplication: {daily_duplicates.height}\")\n",
    "    \n",
    "    # Log monthly duplicates\n",
    "    duplicates = df.group_by(['date_block_num', 'shop_id', 'item_id']).agg(count=pl.count()).filter(pl.col('count') > 1)\n",
    "    print(f\"Monthly duplicates found: {duplicates.height}\")\n",
    "    print(f\"Duplicate details:\\n{duplicates.head()}\")\n",
    "    \n",
    "    # Aggregate to monthly\n",
    "    df = df.group_by(['date_block_num', 'shop_id', 'item_id', 'item_category_id', 'month', 'year']).agg([\n",
    "        pl.col('item_cnt_day_winsor').sum().alias('item_cnt_day_winsor'),\n",
    "        pl.col('returns').sum().alias('returns'),\n",
    "        pl.col('item_price').mean().alias('item_price')\n",
    "    ]).with_columns(\n",
    "        pl.col('item_cnt_day_winsor').clip(upper_bound=1000).alias('item_cnt_day_winsor')\n",
    "    )\n",
    "    \n",
    "    # Recreate date column\n",
    "    df = df.with_columns(\n",
    "        pl.datetime(pl.col('year'), pl.col('month'), 1).alias('date')\n",
    "    )\n",
    "    \n",
    "    # Diagnostics: Inspect aggregated item_cnt_day_winsor\n",
    "    print(\"item_cnt_day_winsor distribution after aggregation:\")\n",
    "    print(df['item_cnt_day_winsor'].describe())\n",
    "    \n",
    "    print(f\"Dataset size after aggregation: {len(df)}\")\n",
    "    print(f\"Unique shops: {df['shop_id'].n_unique()}, items: {df['item_id'].n_unique()}\")\n",
    "    print(f\"Aggregation time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = aggregate_to_monthly(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a07434f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after grid creation: ['shop_id', 'item_id', 'date_block_num', 'month', 'year', 'item_category_id', 'item_cnt_day_winsor', 'returns', 'item_price', 'date']\n",
      "Grid size: 37972638, after merge: 37972638\n",
      "Zero sales introduced: 36404950 (95.87%)\n",
      "Non-zero sales: 1567688 (4.13%)\n",
      "Unique shops: 54, items: 21309\n",
      "Grid creation time: 4.79 seconds\n"
     ]
    }
   ],
   "source": [
    "def create_full_grid(df):\n",
    "    \"\"\"Create full shop-item-month grid.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    shops = df['shop_id'].unique().to_list()\n",
    "    items = df['item_id'].unique().to_list()\n",
    "    date_blocks = list(range(33))  # 0–32\n",
    "    \n",
    "    grid = pl.DataFrame({'shop_id': shops}).join(\n",
    "        pl.DataFrame({'item_id': items}), how='cross'\n",
    "    ).join(\n",
    "        pl.DataFrame({'date_block_num': date_blocks}), how='cross'\n",
    "    ).with_columns([\n",
    "        pl.col('shop_id').cast(pl.Int32),\n",
    "        pl.col('item_id').cast(pl.Int32),\n",
    "        pl.col('date_block_num').cast(pl.Int16),\n",
    "        ((pl.col('date_block_num') % 12) + 1).cast(pl.Int32).alias('month'),\n",
    "        ((pl.col('date_block_num') // 12) + 2013).cast(pl.Int32).alias('year')\n",
    "    ])\n",
    "    \n",
    "    df = grid.join(\n",
    "        df, on=['shop_id', 'item_id', 'date_block_num', 'month', 'year'], how='left'\n",
    "    ).with_columns([\n",
    "        pl.col('item_cnt_day_winsor').fill_null(0),\n",
    "        pl.col('returns').fill_null(0),\n",
    "        pl.col('item_price').fill_null(pl.col('item_price').mean().over('item_id')).fill_null(0),\n",
    "        pl.col('item_category_id').fill_null(pl.col('item_category_id').first().over('item_id')).fill_null(0)\n",
    "    ]).with_columns(\n",
    "        pl.datetime(pl.col('year'), pl.col('month'), 1).alias('date')\n",
    "    )\n",
    "    \n",
    "    # Verify date column\n",
    "    if 'date' not in df.columns:\n",
    "        raise ValueError(\"date column missing after grid creation\")\n",
    "    print(f\"Columns after grid creation: {df.columns}\")\n",
    "    \n",
    "    zero_sales = df.filter(pl.col('item_cnt_day_winsor') == 0).height\n",
    "    non_zero_sales = df.filter(pl.col('item_cnt_day_winsor') > 0).height\n",
    "    print(f\"Grid size: {len(grid)}, after merge: {len(df)}\")\n",
    "    print(f\"Zero sales introduced: {zero_sales} ({zero_sales / len(df) * 100:.2f}%)\")\n",
    "    print(f\"Non-zero sales: {non_zero_sales} ({non_zero_sales / len(df) * 100:.2f}%)\")\n",
    "    print(f\"Unique shops: {df['shop_id'].n_unique()}, items: {df['item_id'].n_unique()}\")\n",
    "    print(f\"Grid creation time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = create_full_grid(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9ab9abf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_cnt_day_winsor after imputation:\n",
      "shape: (9, 2)\n",
      "┌────────────┬─────────────┐\n",
      "│ statistic  ┆ value       │\n",
      "│ ---        ┆ ---         │\n",
      "│ str        ┆ f64         │\n",
      "╞════════════╪═════════════╡\n",
      "│ count      ┆ 3.7972638e7 │\n",
      "│ null_count ┆ 0.0         │\n",
      "│ mean       ┆ 0.138108    │\n",
      "│ std        ┆ 1.703088    │\n",
      "│ min        ┆ -7.9473e-8  │\n",
      "│ 25%        ┆ 0.0         │\n",
      "│ 50%        ┆ 0.0         │\n",
      "│ 75%        ┆ 0.0         │\n",
      "│ max        ┆ 1000.0      │\n",
      "└────────────┴─────────────┘\n",
      "Imputation time: 53.62 seconds\n"
     ]
    }
   ],
   "source": [
    "def seasonal_imputation(df, col):\n",
    "    \"\"\"Apply seasonality-aware imputation to a column, treating zeros as pseudo-nulls.\"\"\"\n",
    "    df = df.with_columns(\n",
    "        seasonal_value=pl.col(col).shift(12).over(['shop_id', 'item_id', 'month']),\n",
    "        ma_value=pl.col(col).rolling_mean(window_size=12, min_periods=1).over(['shop_id', 'item_id'])\n",
    "    ).with_columns(\n",
    "        pl.when(pl.col(col).eq(0) & pl.col('seasonal_value').is_not_null())\n",
    "          .then(pl.col('seasonal_value'))\n",
    "          .when(pl.col(col).eq(0))\n",
    "          .then(pl.col('ma_value'))\n",
    "          .otherwise(pl.col(col))\n",
    "          .alias(col)\n",
    "    ).drop(['seasonal_value', 'ma_value'])\n",
    "    return df\n",
    "\n",
    "def apply_imputation(df, cols):\n",
    "    \"\"\"Impute missing values for specified columns.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for col in cols:\n",
    "        df = seasonal_imputation(df, col)\n",
    "    df = df.with_columns([pl.col(col).fill_null(0) for col in cols])\n",
    "    \n",
    "    print(\"item_cnt_day_winsor after imputation:\")\n",
    "    print(df['item_cnt_day_winsor'].describe())\n",
    "    print(f\"Imputation time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "numerical_cols = ['item_cnt_day_winsor', 'returns', 'item_price']\n",
    "df = apply_imputation(df, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6a2c6883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holiday feature addition time: 0.84 seconds\n"
     ]
    }
   ],
   "source": [
    "def add_holiday_features(df, holiday_df):\n",
    "    \"\"\"Add holiday features to the dataset.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = df.join(\n",
    "        holiday_df.select(['year', 'month', 'holiday']),\n",
    "        on=['year', 'month'], how='left'\n",
    "    ).with_columns(\n",
    "        is_holiday=pl.col('holiday').is_not_null().cast(pl.Int8),\n",
    "        holiday=pl.col('holiday').fill_null('None')\n",
    "    )\n",
    "    \n",
    "    print(f\"Holiday feature addition time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = add_holiday_features(df, holiday_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d8d5d690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-filter sparsity: 82.92%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records dropped due to >60% missing or low sales: 34490709\n",
      "Records after filtering: 3481929, shop-item pairs: 105513\n",
      "Sparsity after filtering: 976653 (28.05%)\n",
      "Unique items: 9870\n",
      "Item count status: FAIL\n",
      "Filtering time: 0.60 seconds\n"
     ]
    }
   ],
   "source": [
    "def filter_sparse_products(df):\n",
    "    \"\"\"Exclude shop-item pairs with >60% missing data and low sales.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Pre-filter sparsity\n",
    "    print(f\"Pre-filter sparsity: {df.filter(pl.col('item_cnt_day_winsor') == 0).height / len(df):.2%}\")\n",
    "    \n",
    "    shop_item_missing = df.group_by(['shop_id', 'item_id']).agg(\n",
    "        missing_ratio=pl.col('item_cnt_day_winsor').eq(0).mean(),\n",
    "        total_sales=pl.col('item_cnt_day_winsor').sum()\n",
    "    )\n",
    "    valid_shop_items = shop_item_missing.filter(\n",
    "        (pl.col('missing_ratio') <= 0.60) & \n",
    "        (pl.col('total_sales') > 10)\n",
    "    ).select(['shop_id', 'item_id'])\n",
    "    \n",
    "    initial_size = len(df)\n",
    "    df = df.join(valid_shop_items, on=['shop_id', 'item_id'], how='inner')\n",
    "    \n",
    "    zero_sales = df.filter(pl.col('item_cnt_day_winsor') == 0).height\n",
    "    item_count = df['item_id'].n_unique()\n",
    "    shop_item_count = len(valid_shop_items)\n",
    "    print(f\"Records dropped due to >60% missing or low sales: {initial_size - len(df)}\")\n",
    "    print(f\"Records after filtering: {len(df)}, shop-item pairs: {shop_item_count}\")\n",
    "    print(f\"Sparsity after filtering: {zero_sales} ({zero_sales / len(df) * 100:.2f}%)\")\n",
    "    print(f\"Unique items: {item_count}\")\n",
    "    print(f\"Item count status: {'PASS' if 10000 <= item_count <= 15000 else 'FAIL'}\")\n",
    "    print(f\"Filtering time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = filter_sparse_products(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a4fb749a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns before lag creation: ['shop_id', 'item_id', 'date_block_num', 'month', 'year', 'item_category_id', 'item_cnt_day_winsor', 'returns', 'item_price', 'date', 'holiday', 'is_holiday']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag feature creation time: 0.89 seconds\n"
     ]
    }
   ],
   "source": [
    "def create_lag_features(df):\n",
    "    \"\"\"Create lag features for 1–3 months as per paper.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Verify required columns\n",
    "    required_cols = ['shop_id', 'item_id', 'date_block_num', 'item_cnt_day_winsor']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    print(f\"Columns before lag creation: {df.columns}\")\n",
    "    \n",
    "    # Sort by shop_id, item_id, date_block_num\n",
    "    df = df.sort(['shop_id', 'item_id', 'date_block_num'])\n",
    "    \n",
    "    # Create lag features for item_cnt_day_winsor only\n",
    "    for lag in [1, 2, 3]:\n",
    "        df = df.with_columns(\n",
    "            pl.col('item_cnt_day_winsor').shift(lag).over(['shop_id', 'item_id']).alias(f'lag_sales_{lag}')\n",
    "        )\n",
    "    \n",
    "    print(f\"Lag feature creation time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = create_lag_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0db1808f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values before lag imputation:\n",
      "   item_cnt_day_winsor  returns  item_price  lag_sales_1  lag_sales_2  lag_sales_3\n",
      "0                    0        0           0       105513       211026       316539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_cnt_day_winsor after imputation:\n",
      "shape: (9, 2)\n",
      "┌────────────┬────────────┐\n",
      "│ statistic  ┆ value      │\n",
      "│ ---        ┆ ---        │\n",
      "│ str        ┆ f64        │\n",
      "╞════════════╪════════════╡\n",
      "│ count      ┆ 3.481929e6 │\n",
      "│ null_count ┆ 0.0        │\n",
      "│ mean       ┆ 1.037785   │\n",
      "│ std        ┆ 5.071925   │\n",
      "│ min        ┆ -4.0916e-7 │\n",
      "│ 25%        ┆ 0.083333   │\n",
      "│ 50%        ┆ 0.416667   │\n",
      "│ 75%        ┆ 1.0        │\n",
      "│ max        ┆ 1000.0     │\n",
      "└────────────┴────────────┘\n",
      "Imputation time: 12.42 seconds\n",
      "\n",
      "Missing values after lag imputation:\n",
      "   item_cnt_day_winsor  returns  item_price  lag_sales_1  lag_sales_2  lag_sales_3\n",
      "0                    0        0           0            0            0            0\n"
     ]
    }
   ],
   "source": [
    "# Update numerical_cols to include only sales lags\n",
    "numerical_cols = ['item_cnt_day_winsor', 'returns', 'item_price', 'lag_sales_1', 'lag_sales_2', 'lag_sales_3']\n",
    "print(\"\\nMissing values before lag imputation:\")\n",
    "print(df.select(numerical_cols).null_count().to_pandas().to_string())\n",
    "\n",
    "df = apply_imputation(df, numerical_cols)\n",
    "\n",
    "print(\"\\nMissing values after lag imputation:\")\n",
    "print(df.select(numerical_cols).null_count().to_pandas().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1f64821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-save sparsity: 563610 (16.19%)\n",
      "Pre-save processed CV: 4.89\n",
      "Columns before saving: ['shop_id', 'item_id', 'date_block_num', 'month', 'year', 'item_category_id', 'item_cnt_day_winsor', 'returns', 'item_price', 'date', 'holiday', 'is_holiday', 'lag_sales_1', 'lag_sales_2', 'lag_sales_3']\n",
      "\n",
      "Null values in numerical_cols before scaling (Polars):\n",
      "   item_cnt_day_winsor  returns  item_price  lag_sales_1  lag_sales_2  lag_sales_3\n",
      "0                    0        0           0            0            0            0\n",
      "Rows with null item_cnt_day_winsor: 0\n",
      "Rows with null returns: 0\n",
      "Rows with null item_price: 0\n",
      "Rows with null lag_sales_1: 0\n",
      "Rows with null lag_sales_2: 0\n",
      "Rows with null lag_sales_3: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values in numerical_cols before scaling (Pandas):\n",
      "item_cnt_day_winsor    0\n",
      "returns                0\n",
      "item_price             0\n",
      "lag_sales_1            0\n",
      "lag_sales_2            0\n",
      "lag_sales_3            0\n",
      "Rows with null item_cnt_day_winsor: 0\n",
      "Rows with null returns: 0\n",
      "Rows with null item_price: 0\n",
      "Rows with null lag_sales_1: 0\n",
      "Rows with null lag_sales_2: 0\n",
      "Rows with null lag_sales_3: 0\n",
      "Saved unscaled data\n",
      "Scaler training data shape: (3165390, 6)\n",
      "Scaling and saving time: 2.84 seconds\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "def scale_and_save_data(df, numerical_cols):\n",
    "    \"\"\"Apply robust scaling and save datasets.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Pre-save diagnostics\n",
    "    zero_sales = df.filter(pl.col('item_cnt_day_winsor') == 0).height\n",
    "    print(f\"Pre-save sparsity: {zero_sales} ({zero_sales / len(df) * 100:.2f}%)\")\n",
    "    processed_cv = df['item_cnt_day_winsor'].std() / df['item_cnt_day_winsor'].mean()\n",
    "    print(f\"Pre-save processed CV: {processed_cv:.2f}\")\n",
    "    print(f\"Columns before saving: {df.columns}\")\n",
    "    \n",
    "    # Check for unexpected columns\n",
    "    expected_cols = ['date_block_num', 'shop_id', 'item_id', 'item_category_id', 'month', 'year',\n",
    "                     'item_cnt_day_winsor', 'returns', 'item_price', 'is_holiday', 'date',\n",
    "                     'lag_sales_1', 'lag_sales_2', 'lag_sales_3', 'holiday']\n",
    "    unexpected_cols = [col for col in df.columns if col not in expected_cols]\n",
    "    if unexpected_cols:\n",
    "        print(f\"Warning: Unexpected columns before saving: {unexpected_cols}\")\n",
    "    \n",
    "    # Pre-scaling null check (Polars)\n",
    "    print(\"\\nNull values in numerical_cols before scaling (Polars):\")\n",
    "    null_counts_polars = df.select(numerical_cols).null_count().to_pandas()\n",
    "    print(null_counts_polars.to_string())\n",
    "    for col in numerical_cols:\n",
    "        null_rows = df.filter(pl.col(col).is_null()).height\n",
    "        print(f\"Rows with null {col}: {null_rows}\")\n",
    "    \n",
    "    monthly_sales = df.to_pandas()\n",
    "    del df\n",
    "    \n",
    "    # Pre-scaling null check (Pandas)\n",
    "    print(\"\\nNull values in numerical_cols before scaling (Pandas):\")\n",
    "    null_counts_pandas = monthly_sales[numerical_cols].isna().sum()\n",
    "    print(null_counts_pandas.to_string())\n",
    "    for col in numerical_cols:\n",
    "        null_rows = monthly_sales[monthly_sales[col].isna()].shape[0]\n",
    "        print(f\"Rows with null {col}: {null_rows}\")\n",
    "    \n",
    "    save_dir = '/workspace/XAI-2/XAI/processed_data'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    monthly_sales.to_parquet(os.path.join(save_dir, 'monthly_sales_unscaled.parquet'))\n",
    "    print(\"Saved unscaled data\")\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    train_data = monthly_sales[monthly_sales['date_block_num'] < 30][numerical_cols]\n",
    "    print(f\"Scaler training data shape: {train_data.shape}\")\n",
    "    \n",
    "    scaler.fit(train_data)\n",
    "    monthly_sales[numerical_cols] = scaler.transform(monthly_sales[numerical_cols])\n",
    "    \n",
    "    if monthly_sales[numerical_cols].isna().any().any():\n",
    "        print(\"\\nNull values in numerical_cols after scaling:\")\n",
    "        null_counts_after = monthly_sales[numerical_cols].isna().sum()\n",
    "        print(null_counts_after.to_string())\n",
    "        for col in numerical_cols:\n",
    "            null_rows = monthly_sales[monthly_sales[col].isna()].shape[0]\n",
    "            print(f\"Rows with null {col}: {null_rows}\")\n",
    "        raise ValueError(\"NaNs introduced during scaling\")\n",
    "    \n",
    "    with open(os.path.join(save_dir, 'scaler.pkl'), 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    dtypes = {col: 'float32' for col in numerical_cols}\n",
    "    dtypes.update({\n",
    "        'shop_id': 'int32', 'item_id': 'int32', 'item_category_id': 'int32',\n",
    "        'date_block_num': 'int16', 'month': 'int32', 'year': 'int32', 'is_holiday': 'int8'\n",
    "    })\n",
    "    monthly_sales = monthly_sales.astype(dtypes, errors='ignore')\n",
    "    \n",
    "    print(f\"Scaling and saving time: {time.time() - start_time:.2f} seconds\")\n",
    "    return monthly_sales\n",
    "\n",
    "numerical_cols = ['item_cnt_day_winsor', 'returns', 'item_price', 'lag_sales_1', 'lag_sales_2', 'lag_sales_3']\n",
    "monthly_sales = scale_and_save_data(df, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3ec1ffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns: ['date_block_num', 'shop_id', 'item_id', 'item_category_id', 'month', 'year', 'item_price', 'returns', 'is_holiday', 'lag_sales_1', 'lag_sales_2', 'lag_sales_3']\n",
      "X_train: (3270903, 12), y_train: (3270903, 1)\n",
      "X_val: (105513, 12), y_val: (105513, 1)\n",
      "X_test: (105513, 12), y_test: (105513, 1)\n",
      "Unique shops in train: 52, items: 9870\n",
      "Split and save time: 4.06 seconds\n"
     ]
    }
   ],
   "source": [
    "def split_and_save_sets(df):\n",
    "    \"\"\"Split data into train/val/test per paper (months 0–30, 31, 32) and save X and y.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_df = df[df['date_block_num'] <= 30]\n",
    "    val_df = df[df['date_block_num'] == 31]\n",
    "    test_df = df[df['date_block_num'] == 32]\n",
    "    \n",
    "    target_col = 'item_cnt_day_winsor'\n",
    "    exclude_cols = [target_col, 'holiday', 'date']\n",
    "    feature_cols = [\n",
    "        'date_block_num', 'shop_id', 'item_id', 'item_category_id', 'month', 'year',\n",
    "        'item_price', 'returns', 'is_holiday', 'lag_sales_1', 'lag_sales_2', 'lag_sales_3'\n",
    "    ]\n",
    "    \n",
    "    # Debug unexpected columns\n",
    "    unexpected_cols = [col for col in df.columns if col not in feature_cols + exclude_cols]\n",
    "    if unexpected_cols:\n",
    "        print(f\"Warning: Unexpected columns in dataset: {unexpected_cols}\")\n",
    "    \n",
    "    print(f\"Feature columns: {feature_cols}\")\n",
    "    \n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[[target_col]]\n",
    "    X_val = val_df[feature_cols]\n",
    "    y_val = val_df[[target_col]]\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df[[target_col]]\n",
    "    \n",
    "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "    print(f\"Unique shops in train: {X_train['shop_id'].nunique()}, items: {X_train['item_id'].nunique()}\")\n",
    "    \n",
    "    output_dir = '/workspace/XAI-2/XAI/processed_data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    X_train.to_parquet(os.path.join(output_dir, 'X_train_processed.parquet'))\n",
    "    y_train.to_parquet(os.path.join(output_dir, 'y_train_processed.parquet'))\n",
    "    X_val.to_parquet(os.path.join(output_dir, 'X_val_processed.parquet'))\n",
    "    y_val.to_parquet(os.path.join(output_dir, 'y_val_processed.parquet'))\n",
    "    X_test.to_parquet(os.path.join(output_dir, 'X_test_processed.parquet'))\n",
    "    y_test.to_parquet(os.path.join(output_dir, 'y_test_processed.parquet'))\n",
    "    \n",
    "    df.to_parquet(os.path.join(output_dir, 'processed_sales.parquet'))\n",
    "    \n",
    "    print(f\"Split and save time: {time.time() - start_time:.2f} seconds\")\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_and_save_sets(monthly_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2ff8dcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 18:36:15 - INFO - Using device: cuda\n",
      "2025-05-12 18:36:15 - INFO - Starting program\n",
      "2025-05-12 18:36:15 - INFO - Loading datasets...\n",
      "2025-05-12 18:36:15 - INFO - Loading dataset from /workspace/XAI-2/XAI/processed_data/X_train_processed.parquet\n",
      "2025-05-12 18:36:15 - INFO - Loaded Parquet files in 0.49s\n",
      "2025-05-12 18:36:15 - ERROR - Error creating datasets: item_cnt_day_winsor\n",
      "\n",
      "Resolved plan until failure:\n",
      "\n",
      "\t---> FAILED HERE RESOLVING 'sink' <---\n",
      "DF [\"date_block_num\", \"shop_id\", \"item_id\", \"item_category_id\", ...]; PROJECT */13 COLUMNS\n",
      "2025-05-12 18:36:15 - ERROR - Program failed: item_cnt_day_winsor\n",
      "\n",
      "Resolved plan until failure:\n",
      "\n",
      "\t---> FAILED HERE RESOLVING 'sink' <---\n",
      "DF [\"date_block_num\", \"shop_id\", \"item_id\", \"item_category_id\", ...]; PROJECT */13 COLUMNS\n",
      "2025-05-12 18:36:15 - ERROR - Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2858/2081807326.py\", line 667, in <module>\n",
      "    main()\n",
      "  File \"/tmp/ipykernel_2858/2081807326.py\", line 551, in main\n",
      "    train_dataset = SalesDataset(\n",
      "  File \"/tmp/ipykernel_2858/2081807326.py\", line 74, in __init__\n",
      "    numerical_data = self.X.select(self.numerical_cols).to_numpy()\n",
      "  File \"/workspace/XAI-2/XAI/nvenv/lib/python3.10/site-packages/polars/dataframe/frame.py\", line 9657, in select\n",
      "    return self.lazy().select(*exprs, **named_exprs).collect(_eager=True)\n",
      "  File \"/workspace/XAI-2/XAI/nvenv/lib/python3.10/site-packages/polars/_utils/deprecation.py\", line 93, in wrapper\n",
      "    return function(*args, **kwargs)\n",
      "  File \"/workspace/XAI-2/XAI/nvenv/lib/python3.10/site-packages/polars/lazyframe/frame.py\", line 2224, in collect\n",
      "    return wrap_df(ldf.collect(engine, callback))\n",
      "polars.exceptions.ColumnNotFoundError: item_cnt_day_winsor\n",
      "\n",
      "Resolved plan until failure:\n",
      "\n",
      "\t---> FAILED HERE RESOLVING 'sink' <---\n",
      "DF [\"date_block_num\", \"shop_id\", \"item_id\", \"item_category_id\", ...]; PROJECT */13 COLUMNS\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import logging\n",
    "import pkg_resources\n",
    "\n",
    "# Check Polars version\n",
    "required_polars_version = \"0.20.0\"\n",
    "current_polars_version = pkg_resources.get_distribution(\"polars\").version\n",
    "if pkg_resources.parse_version(current_polars_version) < pkg_resources.parse_version(required_polars_version):\n",
    "    raise ImportError(\n",
    "        f\"Polars version {current_polars_version} is outdated. Please upgrade to {required_polars_version} or higher \"\n",
    "        \"using 'pip install --upgrade polars'\"\n",
    "    )\n",
    "\n",
    "# Set file descriptor limit\n",
    "os.system('ulimit -n 4096')\n",
    "\n",
    "# Minimal logging with DEBUG level\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# GPU Configuration\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "class SalesDataset(Dataset):\n",
    "    def __init__(self, X_file, y_file, sequence_length=12, num_shops=54, num_items=22170, num_categories=84):\n",
    "        logger.info(f\"Loading dataset from {X_file}\")\n",
    "        start = time.time()\n",
    "        \n",
    "        # Load data\n",
    "        self.X = pl.read_parquet(X_file)\n",
    "        self.y = pl.read_parquet(y_file).select(['item_cnt_day_winsor']).to_numpy().flatten().astype(np.float32)\n",
    "        logger.info(f\"Loaded Parquet files in {time.time() - start:.2f}s\")\n",
    "        \n",
    "        if len(self.X) != len(self.y):\n",
    "            raise ValueError(f\"Mismatch between X ({len(self.X)}) and y ({len(self.y)}) rows\")\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_shops = num_shops\n",
    "        self.num_items = num_items\n",
    "        self.num_categories = num_categories\n",
    "        \n",
    "        self.numerical_cols = [\n",
    "            'item_cnt_day_winsor', 'returns', 'item_price',\n",
    "            'lag_sales_1', 'lag_sales_2', 'lag_sales_3'\n",
    "        ]\n",
    "        self.categorical_cols = ['shop_id', 'item_id', 'item_category_id']\n",
    "        \n",
    "        # Validate and normalize numerical data\n",
    "        numerical_data = self.X.select(self.numerical_cols).to_numpy()\n",
    "        if np.isnan(numerical_data).any() or np.isnan(self.y).any():\n",
    "            raise ValueError(\"NaN values in X or y\")\n",
    "        if np.isinf(numerical_data).any() or np.isinf(self.y).any():\n",
    "            raise ValueError(\"Infinite values in X or y\")\n",
    "        \n",
    "        # Clip and normalize numerical data\n",
    "        numerical_data = np.clip(numerical_data, -1e5, 1e5)\n",
    "        mean = numerical_data.mean(axis=0, keepdims=True)\n",
    "        std = numerical_data.std(axis=0, keepdims=True) + 1e-6\n",
    "        numerical_data = (numerical_data - mean) / std\n",
    "        \n",
    "        # Clip and normalize target\n",
    "        self.y = np.clip(self.y, -1e5, 1e5)\n",
    "        self.y_mean = self.y.mean()\n",
    "        self.y_std = self.y.std() + 1e-6\n",
    "        self.y = (self.y - self.y_mean) / self.y_std\n",
    "        \n",
    "        # Clip categorical indices\n",
    "        self.X = self.X.with_columns([\n",
    "            pl.col('shop_id').clip(upper_bound=num_shops - 1),\n",
    "            pl.col('item_id').clip(upper_bound=num_items - 1),\n",
    "            pl.col('item_category_id').clip(upper_bound=num_categories - 1)\n",
    "        ])\n",
    "        \n",
    "        # Preload data\n",
    "        self.numerical = numerical_data.astype(np.float32)\n",
    "        self.shop_ids = self.X['shop_id'].to_numpy().astype(np.int64)\n",
    "        self.item_ids = self.X['item_id'].to_numpy().astype(np.int64)\n",
    "        self.category_ids = self.X['item_category_id'].to_numpy().astype(np.int64)\n",
    "        self.date_block_num = self.X['date_block_num'].to_numpy().astype(np.int32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.sequence_length + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            start_idx = idx\n",
    "            end_idx = idx + self.sequence_length\n",
    "            if end_idx > len(self.X):\n",
    "                raise IndexError(\"Index out of range\")\n",
    "            \n",
    "            numerical = torch.tensor(self.numerical[start_idx:end_idx], dtype=torch.float32)\n",
    "            shop_ids = torch.tensor(self.shop_ids[start_idx:end_idx], dtype=torch.int64)\n",
    "            item_ids = torch.tensor(self.item_ids[start_idx:end_idx], dtype=torch.int64)\n",
    "            category_ids = torch.tensor(self.category_ids[start_idx:end_idx], dtype=torch.int64)\n",
    "            date_block_num = torch.tensor(self.date_block_num[start_idx:end_idx], dtype=torch.int32)\n",
    "            target = torch.tensor(self.y[end_idx - 1], dtype=torch.float32)\n",
    "            \n",
    "            if torch.isnan(numerical).any() or torch.isnan(target).any():\n",
    "                raise ValueError(f\"NaN detected at index {idx}\")\n",
    "            \n",
    "            identifiers = torch.tensor([\n",
    "                int(self.shop_ids[end_idx - 1]), \n",
    "                int(self.item_ids[end_idx - 1]), \n",
    "                int(self.date_block_num[end_idx - 1])\n",
    "            ], dtype=torch.int32)\n",
    "            \n",
    "            return {\n",
    "                'numerical': numerical,\n",
    "                'shop_ids': shop_ids,\n",
    "                'item_ids': item_ids,\n",
    "                'category_ids': category_ids,\n",
    "                'target': target,\n",
    "                'date_block_num': date_block_num[-1],\n",
    "                'identifiers': identifiers\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in __getitem__ at index {idx}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "class FeatureAttention(nn.Module):\n",
    "    def __init__(self, feature_dim, attention_dim=64):\n",
    "        super(FeatureAttention, self).__init__()\n",
    "        self.query = nn.Linear(feature_dim, attention_dim)\n",
    "        self.key = nn.Linear(feature_dim, attention_dim)\n",
    "        self.value = nn.Linear(feature_dim, feature_dim)\n",
    "        self.scale = 1 / (attention_dim ** 0.5)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.norm = nn.LayerNorm(feature_dim)\n",
    "        nn.init.xavier_uniform_(self.query.weight)\n",
    "        nn.init.xavier_uniform_(self.key.weight)\n",
    "        nn.init.xavier_uniform_(self.value.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        scores = torch.bmm(query, key.transpose(1, 2)) * self.scale\n",
    "        weights = self.softmax(scores)\n",
    "        output = torch.bmm(weights, value)\n",
    "        output = self.norm(output)\n",
    "        return output, weights\n",
    "\n",
    "class HALSTM(nn.Module):\n",
    "    def __init__(self, num_shops=54, num_items=22170, num_categories=84, embed_dim=16, numerical_dim=6, \n",
    "                 hidden_dim=128, num_layers=2, num_heads=4, dropout=0.4, forecast_horizon=1):\n",
    "        super(HALSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        \n",
    "        self.shop_embed = nn.Embedding(num_shops, embed_dim)\n",
    "        self.item_embed = nn.Embedding(num_items, embed_dim)\n",
    "        self.category_embed = nn.Embedding(num_categories, embed_dim)\n",
    "        nn.init.normal_(self.shop_embed.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.item_embed.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.category_embed.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        self.input_dim = embed_dim * 3 + numerical_dim\n",
    "        self.feature_attention = FeatureAttention(self.input_dim)\n",
    "        self.lstm = nn.LSTM(self.input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.lstm_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.mha = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.mha_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.gate = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc_shared = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_horizons = nn.ModuleList([nn.Linear(hidden_dim, 1) for _ in range(forecast_horizon)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = self._create_positional_encoding(max_seq_len=100, d_model=hidden_dim)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _create_positional_encoding(self, max_seq_len, d_model):\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.to(device)\n",
    "    \n",
    "    def forward(self, numerical, shop_ids, item_ids, category_ids, trace_mode=False):\n",
    "        batch_size, seq_len, _ = numerical.size()\n",
    "        shop_embed = self.shop_embed(shop_ids)\n",
    "        item_embed = self.item_embed(item_ids)\n",
    "        category_embed = self.category_embed(category_ids)\n",
    "        \n",
    "        x = torch.cat([numerical, shop_embed, item_embed, category_embed], dim=-1).contiguous()\n",
    "        x, feature_weights = self.feature_attention(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device).contiguous()\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device).contiguous()\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        lstm_out = self.lstm_norm(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        lstm_out = lstm_out + self.positional_encoding[:seq_len, :].unsqueeze(0).to(lstm_out.device)\n",
    "        \n",
    "        mha_out, mha_weights = self.mha(lstm_out, lstm_out, lstm_out)\n",
    "        mha_out = self.mha_norm(mha_out)\n",
    "        mha_out = self.dropout(mha_out)\n",
    "        \n",
    "        combined = torch.cat([lstm_out[:, -1, :], mha_out[:, -1, :]], dim=-1)\n",
    "        gate = self.sigmoid(self.gate(combined))\n",
    "        fused = gate * lstm_out[:, -1, :] + (1 - gate) * mha_out[:, -1, :]\n",
    "        \n",
    "        shared = self.relu(self.fc_shared(fused))\n",
    "        outputs = torch.cat([fc(shared).unsqueeze(1) for fc in self.fc_horizons], dim=1)\n",
    "        outputs = outputs.squeeze(-1)\n",
    "        \n",
    "        if trace_mode:\n",
    "            return outputs\n",
    "        return outputs, {\n",
    "            'feature_weights': feature_weights,\n",
    "            'mha_weights': mha_weights,\n",
    "            'fused_output': fused,\n",
    "            'gate_weights': gate\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    if not batch:\n",
    "        logger.warning(\"Empty batch received\")\n",
    "        return {}\n",
    "    \n",
    "    numerical = torch.stack([item['numerical'] for item in batch])\n",
    "    shop_ids = torch.stack([item['shop_ids'] for item in batch])\n",
    "    item_ids = torch.stack([item['item_ids'] for item in batch])\n",
    "    category_ids = torch.stack([item['category_ids'] for item in batch])\n",
    "    target = torch.stack([item['target'] for item in batch])\n",
    "    date_block_num = torch.stack([item['date_block_num'] for item in batch])\n",
    "    identifiers = torch.stack([item['identifiers'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'numerical': numerical.contiguous(),\n",
    "        'shop_ids': shop_ids.contiguous(),\n",
    "        'item_ids': item_ids.contiguous(),\n",
    "        'category_ids': category_ids.contiguous(),\n",
    "        'target': target.contiguous(),\n",
    "        'date_block_num': date_block_num.contiguous(),\n",
    "        'identifiers': identifiers.contiguous()\n",
    "    }\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, lr=0.0005, accum_steps=2):\n",
    "    logger.info(\"Starting training\")\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "    l1_lambda = 1e-5\n",
    "    att_lambda = 1e-5\n",
    "    temp_lambda = 1e-5\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=lr, epochs=num_epochs, steps_per_epoch=len(train_loader)//accum_steps, pct_start=0.1\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    output_dir = Path('/workspace/XAI-2/XAI/processed_data')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    patience = 5  # Increased for stability\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, batch in progress:\n",
    "            try:\n",
    "                numerical = batch['numerical'].to(device, non_blocking=True)\n",
    "                shop_ids = batch['shop_ids'].to(device, non_blocking=True)\n",
    "                item_ids = batch['item_ids'].to(device, non_blocking=True)\n",
    "                category_ids = batch['category_ids'].to(device, non_blocking=True)\n",
    "                target = batch['target'].to(device, non_blocking=True)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output, attention_dict = model(numerical, shop_ids, item_ids, category_ids)\n",
    "                    mse_loss = criterion(output[:, -1], target) / accum_steps\n",
    "                    \n",
    "                    att_loss = attention_dict['mha_weights'].abs().sum()\n",
    "                    temp_loss = torch.zeros(1, device=device)\n",
    "                    if output.shape[1] > 1:\n",
    "                        temp_loss = (output[:, 1:] - output[:, :-1]).abs().sum()\n",
    "                    l1_loss = sum(p.abs().sum() for p in model.parameters() if p.requires_grad)\n",
    "                    \n",
    "                    loss = mse_loss + l1_lambda * l1_loss + att_lambda * att_loss + temp_lambda * temp_loss\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                if (batch_idx + 1) % accum_steps == 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                    logger.debug(f\"Batch {batch_idx}: Calling optimizer.step()\")\n",
    "                    scaler.step(optimizer)\n",
    "                    logger.debug(f\"Batch {batch_idx}: Calling scheduler.step()\")\n",
    "                    scheduler.step()\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                train_loss += mse_loss.item() * accum_steps\n",
    "                \n",
    "                if batch_idx % 100 == 0:\n",
    "                    try:\n",
    "                        import pynvml\n",
    "                        logger.info(f\"Batch {batch_idx}, GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB, \"\n",
    "                                    f\"Utilization: {torch.cuda.utilization()}%\")\n",
    "                    except (ImportError, Exception) as e:\n",
    "                        logger.warning(f\"GPU monitoring unavailable: {str(e)}\")\n",
    "                \n",
    "                progress.set_postfix({\"batch_loss\": f\"{mse_loss.item() * accum_steps:.6f}\"})\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "                try:\n",
    "                    numerical = batch['numerical'].to(device, non_blocking=True)\n",
    "                    shop_ids = batch['shop_ids'].to(device, non_blocking=True)\n",
    "                    item_ids = batch['item_ids'].to(device, non_blocking=True)\n",
    "                    category_ids = batch['category_ids'].to(device, non_blocking=True)\n",
    "                    target = batch['target'].to(device, non_blocking=True)\n",
    "                    \n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        output, _ = model(numerical, shop_ids, item_ids, category_ids)\n",
    "                        loss = criterion(output[:, -1], target)\n",
    "                    \n",
    "                    if torch.isnan(loss):\n",
    "                        logger.error(\"NaN detected in validation loss\")\n",
    "                        raise ValueError(\"Validation loss is NaN\")\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in validation: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), output_dir / 'best_ha_lstm.pth')\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            \n",
    "        if early_stop_counter >= patience:\n",
    "            logger.info(f\"Early stopping after {epoch+1} epochs\")\n",
    "            break\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    logger.info(f\"Training done. Best val loss: {best_val_loss:.6f}\")\n",
    "    model.load_state_dict(torch.load(output_dir / 'best_ha_lstm.pth'))\n",
    "    return model\n",
    "\n",
    "def predict(model, test_loader, dataset):\n",
    "    logger.info(\"Predicting\")\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    identifiers = []\n",
    "    fused_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            try:\n",
    "                numerical = batch['numerical'].to(device, non_blocking=True)\n",
    "                shop_ids = batch['shop_ids'].to(device, non_blocking=True)\n",
    "                item_ids = batch['item_ids'].to(device, non_blocking=True)\n",
    "                category_ids = batch['category_ids'].to(device, non_blocking=True)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output, attention_dict = model(numerical, shop_ids, item_ids, category_ids)\n",
    "                    preds = output.cpu().numpy()\n",
    "                    preds = preds * dataset.y_std + dataset.y_mean\n",
    "                    fused = attention_dict['fused_output'].cpu().numpy()\n",
    "                \n",
    "                predictions.append(preds)\n",
    "                identifiers.append(batch['identifiers'].cpu().numpy())\n",
    "                fused_outputs.append(fused)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in prediction: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    if not predictions or not identifiers or not fused_outputs:\n",
    "        logger.error(\"No predictions, identifiers, or fused outputs collected\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    identifiers = np.concatenate(identifiers, axis=0)\n",
    "    fused_outputs = np.concatenate(fused_outputs, axis=0)\n",
    "    \n",
    "    pred_df = pd.DataFrame({\n",
    "        'shop_id': identifiers[:, 0],\n",
    "        'item_id': identifiers[:, 1],\n",
    "        'date_block_num': identifiers[:, 2]\n",
    "    })\n",
    "    for h in range(predictions.shape[1]):\n",
    "        pred_df[f'forecast_h{h+1}'] = predictions[:, h]\n",
    "    \n",
    "    fused_df = pd.DataFrame(fused_outputs, columns=[f'fused_dim_{i}' for i in range(fused_outputs.shape[1])])\n",
    "    fused_df[['shop_id', 'item_id', 'date_block_num']] = identifiers\n",
    "    \n",
    "    output_dir = Path('/workspace/XAI-2/XAI/results')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    pred_df.to_csv(output_dir / 'predictions.csv', index=False)\n",
    "    fused_df.to_csv(output_dir / 'fused_outputs.csv', index=False)\n",
    "    logger.info(f\"Predictions saved to {output_dir / 'predictions.csv'}\")\n",
    "    logger.info(f\"Fused outputs saved to {output_dir / 'fused_outputs.csv'}\")\n",
    "    \n",
    "    return pred_df\n",
    "\n",
    "def visualize_results(pred_df, true_df=None, output_dir='/workspace/XAI-2/XAI/results'):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    logger.info(f\"Saving plots to {output_dir}\")\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    if pred_df.empty:\n",
    "        logger.error(\"Empty prediction dataframe\")\n",
    "        return None\n",
    "    \n",
    "    forecast_cols = [col for col in pred_df.columns if 'forecast' in col]\n",
    "    if not forecast_cols:\n",
    "        logger.error(\"No forecast columns found\")\n",
    "        return None\n",
    "        \n",
    "    for h in range(1, len(forecast_cols) + 1):\n",
    "        sns.kdeplot(pred_df[f'forecast_h{h}'], label=f'Horizon {h}')\n",
    "    \n",
    "    plt.title('Prediction Distribution')\n",
    "    plt.xlabel('Predicted Sales')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'prediction_distribution.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    if true_df is not None and not true_df.empty:\n",
    "        try:\n",
    "            merged_df = pred_df.merge(true_df, on=['shop_id', 'item_id', 'date_block_num'], how='inner')\n",
    "            if not merged_df.empty:\n",
    "                merged_df['error'] = merged_df['forecast_h1'] - merged_df['item_cnt_day_winsor']\n",
    "                \n",
    "                plt.figure(figsize=(12, 8))\n",
    "                sns.histplot(merged_df['error'], kde=True, bins=50)\n",
    "                plt.title('Error Distribution')\n",
    "                plt.xlabel('Prediction Error')\n",
    "                plt.ylabel('Count')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(output_dir / 'error_distribution.png', dpi=300)\n",
    "                plt.close()\n",
    "                \n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.scatter(merged_df['item_cnt_day_winsor'], merged_df['forecast_h1'], alpha=0.5)\n",
    "                plt.plot([0, merged_df['item_cnt_day_winsor'].max()], \n",
    "                         [0, merged_df['item_cnt_day_winsor'].max()], 'r--')\n",
    "                plt.title('Predicted vs Actual')\n",
    "                plt.xlabel('Actual Sales')\n",
    "                plt.ylabel('Predicted Sales')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(output_dir / 'predicted_vs_actual.png', dpi=300)\n",
    "                plt.close()\n",
    "                \n",
    "                mae = merged_df['error'].abs().mean()\n",
    "                rmse = (merged_df['error'] ** 2).mean() ** 0.5\n",
    "                mape = (merged_df['error'].abs() / merged_df['item_cnt_day_winsor'].abs().replace(0, 1e-6)).mean() * 100\n",
    "                logger.info(f\"MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.2f}%\")\n",
    "                \n",
    "                return mae, rmse, mape\n",
    "            else:\n",
    "                logger.warning(\"Merged dataframe is empty\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in visualization: {str(e)}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    data_dir = Path('/workspace/XAI-2/XAI/processed_data')\n",
    "    batch_size = 8192\n",
    "    num_workers = 0\n",
    "    prefetch_factor = None\n",
    "    num_shops = 54\n",
    "    num_items = 22170\n",
    "    num_categories = 84\n",
    "    sequence_length = 12\n",
    "    num_epochs = 50\n",
    "    lr = 0.0005\n",
    "    accum_steps = 2\n",
    "    \n",
    "    logger.info(\"Loading datasets...\")\n",
    "    required_files = [\n",
    "        'X_train_processed.parquet', 'y_train_processed.parquet',\n",
    "        'X_val_processed.parquet', 'y_val_processed.parquet',\n",
    "        'X_test_processed.parquet', 'y_test_processed.parquet'\n",
    "    ]\n",
    "    \n",
    "    missing_files = [f for f in required_files if not (data_dir / f).exists()]\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(f\"Missing files: {', '.join(missing_files)}\")\n",
    "    \n",
    "    try:\n",
    "        train_dataset = SalesDataset(\n",
    "            data_dir / 'X_train_processed.parquet',\n",
    "            data_dir / 'y_train_processed.parquet',\n",
    "            sequence_length=sequence_length,\n",
    "            num_shops=num_shops,\n",
    "            num_items=num_items,\n",
    "            num_categories=num_categories\n",
    "        )\n",
    "        val_dataset = SalesDataset(\n",
    "            data_dir / 'X_val_processed.parquet',\n",
    "            data_dir / 'y_val_processed.parquet',\n",
    "            sequence_length=sequence_length,\n",
    "            num_shops=num_shops,\n",
    "            num_items=num_items,\n",
    "            num_categories=num_categories\n",
    "        )\n",
    "        test_dataset = SalesDataset(\n",
    "            data_dir / 'X_test_processed.parquet',\n",
    "            data_dir / 'y_test_processed.parquet',\n",
    "            sequence_length=sequence_length,\n",
    "            num_shops=num_shops,\n",
    "            num_items=num_items,\n",
    "            num_categories=num_categories\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Train dataset size: {len(train_dataset)}\")\n",
    "        logger.info(f\"Val dataset size: {len(val_dataset)}\")\n",
    "        logger.info(f\"Test dataset size: {len(test_dataset)}\")\n",
    "        \n",
    "        # Verify dataset sizes against paper's expectations\n",
    "        expected_sizes = {'train': 2700000, 'val': 100000, 'test': 100000}\n",
    "        for split, size, expected in [('train', len(train_dataset), expected_sizes['train']),\n",
    "                                      ('val', len(val_dataset), expected_sizes['val']),\n",
    "                                      ('test', len(test_dataset), expected_sizes['test'])]:\n",
    "            if abs(size - expected) / expected > 0.1:\n",
    "                logger.warning(f\"{split.capitalize()} dataset size {size} deviates significantly from expected {expected}. \"\n",
    "                               \"Verify preprocessing and split_dataset output.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating datasets: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "            persistent_workers=False,\n",
    "            collate_fn=collate_fn,\n",
    "            timeout=0\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "            persistent_workers=False,\n",
    "            collate_fn=collate_fn,\n",
    "            timeout=0\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "            persistent_workers=False,\n",
    "            collate_fn=collate_fn,\n",
    "            timeout=0\n",
    "        )\n",
    "        \n",
    "        logger.info(\"DataLoaders created successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating DataLoaders: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        model = HALSTM(\n",
    "            num_shops=num_shops,\n",
    "            num_items=num_items,\n",
    "            num_categories=num_categories,\n",
    "            embed_dim=16,\n",
    "            numerical_dim=6,\n",
    "            hidden_dim=128,\n",
    "            num_layers=2,\n",
    "            num_heads=4,\n",
    "            dropout=0.4,\n",
    "            forecast_horizon=1\n",
    "        ).to(device)\n",
    "        \n",
    "        logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "        \n",
    "        model = train_model(model, train_loader, val_loader, num_epochs=num_epochs, lr=lr, accum_steps=accum_steps)\n",
    "        \n",
    "        predictions = predict(model, test_loader, test_dataset)\n",
    "        \n",
    "        y_test = pl.read_parquet(data_dir / 'y_test_processed.parquet')\n",
    "        x_test_identifiers = pl.read_parquet(data_dir / 'X_test_processed.parquet').select(['shop_id', 'item_id', 'date_block_num'])\n",
    "        true_df = pl.concat([x_test_identifiers, y_test], how='horizontal').to_pandas()\n",
    "        \n",
    "        visualize_results(predictions, true_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logger.info(\"Starting program\")\n",
    "        main()\n",
    "        logger.info(\"Program completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Program failed: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
