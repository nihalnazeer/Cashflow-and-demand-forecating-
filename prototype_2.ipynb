{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa50130",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from uuid import uuid4\n",
    "import warnings\n",
    "import subprocess\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enhanced PyTorch and CUDA diagnostics\n",
    "def check_cuda_environment():\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available. This notebook requires a GPU.\")\n",
    "    \n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    \n",
    "    # Test CUDA operation\n",
    "    try:\n",
    "        test_tensor = torch.tensor([1.0, 2.0, 3.0], device='cuda')\n",
    "        test_result = test_tensor + 1\n",
    "        print(f\"CUDA test operation successful: {test_result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"CUDA test operation failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Check NVIDIA driver and CUDA toolkit\n",
    "    try:\n",
    "        nvidia_smi = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        print(\"NVIDIA-SMI output:\")\n",
    "        print(nvidia_smi.stdout)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run nvidia-smi: {e}\")\n",
    "    \n",
    "    # Check GPU memory\n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 2**30:.2f} GiB\")\n",
    "    print(f\"Allocated GPU memory: {torch.cuda.memory_allocated(0) / 2**30:.2f} GiB\")\n",
    "    print(f\"Reserved GPU memory: {torch.cuda.memory_reserved(0) / 2**30:.2f} GiB\")\n",
    "\n",
    "try:\n",
    "    check_cuda_environment()\n",
    "except Exception as e:\n",
    "    print(f\"Error with PyTorch or CUDA setup: {e}\")\n",
    "    print(\"Try reinstalling PyTorch: pip install torch==2.7.0 --index-url https://download.pytorch.org/whl/cu124\")\n",
    "    raise\n",
    "\n",
    "# Environment diagnostics\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"PATH: {os.environ.get('PATH')}\")\n",
    "print(f\"Available disk space: {shutil.disk_usage('/').free / (2**30):.2f} GiB\")\n",
    "\n",
    "# Check for module shadowing\n",
    "if os.path.exists('/workspace/XAI/torch.py') or os.path.exists('/workspace/XAI/torch.pyc'):\n",
    "    print(\"Warning: Found 'torch.py' or 'torch.pyc' in /workspace/XAI. Please rename or remove it.\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration (GPU only)\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared GPU memory cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd0e159",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"/workspace/data\", file_name=\"merged_data.csv\"):\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    alt_path = \"/workspace/XAI-1/Predict Future Sales/merged_data.csv\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        if os.path.exists(alt_path):\n",
    "            file_path = alt_path\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File not found at {file_path} or {alt_path}\")\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"Loaded data from {file_path}\")\n",
    "        print(f\"Dataset shape: {data.shape}\")\n",
    "        print(f\"Columns: {list(data.columns)}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load {file_path}: {e}\")\n",
    "    \n",
    "    # Verify expected columns\n",
    "    expected_columns = ['date', 'shop_id', 'item_id', 'item_name', 'item_cnt_day', 'item_price', 'item_category_id', 'shop_name', 'item_category_name', 'date_block_num']\n",
    "    missing_cols = [col for col in expected_columns if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing expected columns: {missing_cols}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b8b230",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    handlers=[\n",
    "        logging.FileHandler('/workspace/processed_data/preprocess.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_memory_usage():\n",
    "    \"\"\"Log current memory usage in GB.\"\"\"\n",
    "    mem = psutil.Process().memory_info().rss / (1024 ** 3)\n",
    "    logger.info(f\"Memory usage: {mem:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e62e432",
   "metadata": {},
   "source": [
    "def load_data(data_dir=\"/workspace/data\", file_name=\"merged_data.csv\"):\n",
    "    \"\"\"Load CSV data with fallback path.\"\"\"\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    alt_path = \"/workspace/XAI-1/Predict Future Sales/merged_data.csv\"\n",
    "    path = file_path if os.path.exists(file_path) else alt_path\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found at {path}\")\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    logger.info(f\"Loaded data from {path}, shape: {df.shape}\")\n",
    "    logger.info(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    expected_cols = ['date', 'shop_id', 'item_id', 'item_name', 'item_cnt_day', \n",
    "                     'item_price', 'item_category_id', 'shop_name', 'item_category_name', 'date_block_num']\n",
    "    missing_cols = [col for col in expected_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        logger.warning(f\"Missing columns: {missing_cols}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a2e1f7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_russian_holidays():\n",
    "    \"\"\"Return DataFrame of Russian holidays and shopping events (2013â€“2015).\"\"\"\n",
    "    holidays = [\n",
    "        (\"2013-01-01\", \"New Year\"), (\"2014-01-01\", \"New Year\"), (\"2015-01-01\", \"New Year\"),\n",
    "        (\"2013-02-23\", \"Defender Day\"), (\"2014-02-23\", \"Defender Day\"), (\"2015-02-23\", \"Defender Day\"),\n",
    "        (\"2013-03-08\", \"Women's Day\"), (\"2014-03-08\", \"Women's Day\"), (\"2015-03-08\", \"Women's Day\"),\n",
    "        (\"2013-06-12\", \"Russia Day\"), (\"2014-06-12\", \"Russia Day\"), (\"2015-06-12\", \"Russia Day\"),\n",
    "        (\"2013-11-29\", \"Black Friday\"), (\"2014-11-28\", \"Black Friday\"), (\"2015-11-27\", \"Black Friday\")\n",
    "    ]\n",
    "    holiday_df = pl.DataFrame({\n",
    "        \"date\": [datetime.strptime(date, \"%Y-%m-%d\") for date, _ in holidays],\n",
    "        \"holiday\": [name for _, name in holidays]\n",
    "    }).with_columns([\n",
    "        pl.col(\"date\").dt.month().cast(pl.Int32).alias(\"month\"),\n",
    "        pl.col(\"date\").dt.year().cast(pl.Int32).alias(\"year\")\n",
    "    ])\n",
    "    return holiday_df\n",
    "\n",
    "# Create holiday DataFrame\n",
    "holiday_df = get_russian_holidays()\n",
    "logger.info(f\"Holiday DataFrame created with {len(holiday_df)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0edb2f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def initial_preprocessing(df):\n",
    "    \"\"\"Perform initial data cleaning and filtering.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert to Polars\n",
    "    df = pl.from_pandas(df)\n",
    "    logger.info(f\"Initial dataset size: {len(df)}\")\n",
    "    logger.info(f\"Unique shops: {df['shop_id'].n_unique()}, items: {df['item_id'].n_unique()}\")\n",
    "    logger.info(f\"Date block range: {df['date_block_num'].min()}â€“{df['date_block_num'].max()}\")\n",
    "    log_memory_usage()\n",
    "    \n",
    "    # Parse dates\n",
    "    df = df.with_columns(\n",
    "        pl.col('date').str.strptime(pl.Date, \"%Y-%m-%d\")\n",
    "    ).with_columns([\n",
    "        pl.col('date').dt.month().cast(pl.Int32).alias('month'),\n",
    "        pl.col('date').dt.year().cast(pl.Int32).alias('year')\n",
    "    ])\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Filter to date_block_num <= 32\n",
    "    df = df.filter(pl.col('date_block_num') <= 32)\n",
    "    logger.info(f\"Dataset size after date filter: {len(df)}\")\n",
    "    \n",
    "    # Note: Top-54 shop filtering is skipped, assuming all shops are used (paper reports 54 shops)\n",
    "    logger.info(f\"Unique shops after processing: {df['shop_id'].n_unique()}\")\n",
    "    \n",
    "    # Optimize dtypes\n",
    "    df = df.with_columns([\n",
    "        pl.col('date_block_num').cast(pl.Int16),\n",
    "        pl.col('shop_id').cast(pl.Int32),\n",
    "        pl.col('item_id').cast(pl.Int32),\n",
    "        pl.col('item_category_id').cast(pl.Int32),\n",
    "        pl.col('item_cnt_day').cast(pl.Float32),\n",
    "        pl.col('item_price').cast(pl.Float32),\n",
    "        pl.col('month').cast(pl.Int32),\n",
    "        pl.col('year').cast(pl.Int32)\n",
    "    ])\n",
    "    \n",
    "    logger.info(f\"Initial preprocessing time: {time.time() - start_time:.2f} seconds\")\n",
    "    log_memory_usage()\n",
    "    return df\n",
    "\n",
    "# Apply initial preprocessing\n",
    "df = initial_preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f95c9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def handle_outliers_and_returns(df):\n",
    "    \"\"\"Apply rolling Winsorization and handle negative sales.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Rolling Winsorization (30-day window)\n",
    "    df = df.sort(['shop_id', 'item_id', 'date']).with_columns(\n",
    "        rolling_quantile=pl.col('item_cnt_day').rolling_quantile(\n",
    "            quantile=0.99, window_size=30, min_periods=1\n",
    "        ).over(['shop_id', 'item_id'])\n",
    "    ).with_columns(\n",
    "        item_cnt_day_winsor=pl.col('item_cnt_day').clip(None, pl.col('rolling_quantile'))\n",
    "    )\n",
    "    outlier_count = df.filter(pl.col('item_cnt_day') > pl.col('rolling_quantile')).height\n",
    "    logger.info(f\"Outliers capped: {outlier_count} ({outlier_count / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    # Handle negative sales\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col('item_cnt_day_winsor') < 0)\n",
    "          .then(pl.col('item_cnt_day_winsor').abs())\n",
    "          .otherwise(0)\n",
    "          .alias('returns'),\n",
    "        pl.col('item_cnt_day_winsor').clip(lower_bound=0).alias('item_cnt_day_winsor')\n",
    "    ])\n",
    "    logger.info(f\"Negative sales after processing: {df.filter(pl.col('item_cnt_day_winsor') < 0).height}\")\n",
    "    \n",
    "    logger.info(f\"Outlier and returns handling time: {time.time() - start_time:.2f} seconds\")\n",
    "    log_memory_usage()\n",
    "    return df\n",
    "\n",
    "# Apply outlier and returns handling\n",
    "df = handle_outliers_and_returns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b03f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def aggregate_to_monthly(df):\n",
    "    \"\"\"Aggregate data to monthly level.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = df.group_by(['date_block_num', 'shop_id', 'item_id', 'item_category_id', 'month', 'year']).agg([\n",
    "        pl.col('item_cnt_day_winsor').sum().alias('item_cnt_day_winsor'),\n",
    "        pl.col('returns').sum().alias('returns'),\n",
    "        pl.col('item_price').mean().alias('item_price')\n",
    "    ])\n",
    "    \n",
    "    logger.info(f\"Dataset size after aggregation: {len(df)}\")\n",
    "    logger.info(f\"Unique shops: {df['shop_id'].n_unique()}, items: {df['item_id'].n_unique()}\")\n",
    "    logger.info(f\"Aggregation time: {time.time() - start_time:.2f} seconds\")\n",
    "    log_memory_usage()\n",
    "    return df\n",
    "\n",
    "# Aggregate to monthly\n",
    "df = aggregate_to_monthly(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9234efe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_full_grid(df):\n",
    "    \"\"\"Create full shop-item-month grid.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    shops = df['shop_id'].unique().to_list()\n",
    "    items = df['item_id'].unique().to_list()\n",
    "    date_blocks = list(range(33))  # 0â€“32\n",
    "    \n",
    "    grid = pl.DataFrame({'shop_id': shops}).join(\n",
    "        pl.DataFrame({'item_id': items}), how='cross'\n",
    "    ).join(\n",
    "        pl.DataFrame({'date_block_num': date_blocks}), how='cross'\n",
    "    ).with_columns([\n",
    "        pl.col('shop_id').cast(pl.Int32),\n",
    "        pl.col('item_id').cast(pl.Int32),\n",
    "        pl.col('date_block_num').cast(pl.Int16),\n",
    "        ((pl.col('date_block_num') % 12) + 1).cast(pl.Int32).alias('month'),\n",
    "        ((pl.col('date_block_num') // 12) + 2013).cast(pl.Int32).alias('year')\n",
    "    ])\n",
    "    \n",
    "    # Merge with aggregated data\n",
    "    df = grid.join(\n",
    "        df, on=['shop_id', 'item_id', 'date_block_num', 'month', 'year'], how='left'\n",
    "    ).with_columns([\n",
    "        pl.col('item_cnt_day_winsor').fill_null(0),\n",
    "        pl.col('returns').fill_null(0),\n",
    "        pl.col('item_price').fill_null(pl.col('item_price').mean().over('item_id')).fill_null(0),\n",
    "        pl.col('item_category_id').fill_null(pl.col('item_category_id').first().over('item_id')).fill_null(0)\n",
    "    ]).with_columns(\n",
    "        pl.datetime(pl.col('year'), pl.col('month'), 1).alias('date')\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Grid size: {len(grid)}, after merge: {len(df)}\")\n",
    "    logger.info(f\"Unique shops: {df['shop_id'].n_unique()}, items: {df['item_id'].n_unique()}\")\n",
    "    logger.info(f\"Grid creation time: {time.time() - start_time:.2f} seconds\")\n",
    "    log_memory_usage()\n",
    "    return df\n",
    "\n",
    "# Create full grid\n",
    "df = create_full_grid(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ec0f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def seasonal_imputation(df, col):\n",
    "    \"\"\"Apply seasonality-aware imputation to a column.\"\"\"\n",
    "    df = df.with_columns(\n",
    "        pl.col(col).interpolate().over(['shop_id', 'item_id']).alias(f'{col}_interp')\n",
    "    ).with_columns(\n",
    "        seasonal_value=pl.col(col).shift(12).over(['shop_id', 'item_id', 'month']),\n",
    "        ma_value=pl.col(col).rolling_mean(window_size=12, min_periods=1).over(['shop_id', 'item_id'])\n",
    "    ).with_columns(\n",
    "        pl.when(pl.col(f'{col}_interp').is_null() & pl.col('seasonal_value').is_not_null())\n",
    "          .then(pl.col('seasonal_value'))\n",
    "          .when(pl.col(f'{col}_interp').is_null())\n",
    "          .then(pl.col('ma_value'))\n",
    "          .otherwise(pl.col(f'{col}_interp'))\n",
    "          .alias(col)\n",
    "    ).drop([f'{col}_interp', 'seasonal_value', 'ma_value'])\n",
    "    return df\n",
    "\n",
    "def apply_imputation(df, cols):\n",
    "    \"\"\"Impute missing values for specified columns.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for col in cols:\n",
    "        df = seasonal_imputation(df, col)\n",
    "    df = df.with_columns([pl.col(col).fill_null(0) for col in cols])\n",
    "    \n",
    "    logger.info(f\"Imputation time: {time.time() - start_time:.2f} seconds\")\n",
    "    log_memory_usage()\n",
    "    return df\n",
    "\n",
    "# Apply imputation\n",
    "numerical_cols = ['item_cnt_day_winsor', 'returns', 'item_price']\n",
    "df = apply_imputation(df, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d8b12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def add_holiday_features(df, holiday_df):\n",
    "    \"\"\"Add holiday features to the dataset.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = df.join(\n",
    "        holiday_df.select(['year', 'month', 'holiday']),\n",
    "        on=['year', 'month'], how='left'\n",
    "    ).with_columns(\n",
    "        is_holiday=pl.col('holiday').is_not_null().cast(pl.Int8),\n",
    "        holiday=pl.col('holiday').fill_null('None')\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Holiday feature addition time: {time.time() - start_time:.2f} seconds\")\n",
    "    log_memory_usage()\n",
    "    return df\n",
    "\n",
    "# Add holiday features\n",
    "df = add_holiday_features(df, holiday_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d6be2d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def filter_sparse_products(df):\n",
    "    \"\"\"Exclude shop-item pairs with >30% missing data.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    shop_item_missing = df.group_by(['shop_id', 'item_id']).agg(\n",
    "        missing_ratio=pl.col('item_cnt_day_winsor').eq(0).mean()\n",
    "    )\n",
    "    valid_shop_items = shop_item_missing.filter(pl.col('missing_ratio') <= 0.3).select(['shop_id', 'item_id'])\n",
    "    initial_size = len(df)\n",
    "    df = df.join(valid_shop_items, on=['shop_id', 'item_id'], how='inner')\n",
    "    \n",
    "    logger.info(f\"Records dropped due to >30% missing: {initial_size - len(df)}\")\n",
    "    logger.info(f\"Records after filtering: {len(df)}, shop-item pairs: {len(valid_shop_items)}\")\n",
    "    logger.info(f\"Filtering time: {time.time() - start_time:.2f} seconds\")\n",
    "    log_memory_usage()\n",
    "    return df\n",
    "\n",
    "# Filter sparse products\n",
    "df = filter_sparse_products(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cba67a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_lag_features(df):\n",
    "    \"\"\"Create lag features for 1â€“3 months.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = df.sort(['shop_id', 'item_id', 'date'])\n",
    "    for lag in [1, 2, 3]:\n",
    "        df = df.with_columns([\n",
    "            pl.col('item_cnt_day_winsor').shift(lag).over(['shop_id', 'item_id']).alias(f'lag_sales_{lag}'),\n",
    "            pl.col('returns').shift(lag).over(['shop_id', 'item_id']).alias(f'lag_returns_{lag}'),\n",
    "            pl.col('item_price').shift(lag).over(['shop_id', 'item_id']).alias(f'lag_price_{lag}')\n",
    "        ])\n",
    "    \n",
    "    logger.info(f\"Lag feature creation time: {time.time() - start_time:.2f} seconds\")\n",
    "    log_memory_usage()\n",
    "    return df\n",
    "\n",
    "# Create lag features\n",
    "df = create_lag_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d7f87",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Impute lag features\n",
    "numerical_cols += ['lag_sales_1', 'lag_sales_2', 'lag_sales_3',\n",
    "                   'lag_returns_1', 'lag_returns_2', 'lag_returns_3',\n",
    "                   'lag_price_1', 'lag_price_2', 'lag_price_3']\n",
    "logger.info(\"\\nMissing values before lag imputation:\")\n",
    "logger.info(df.select(numerical_cols).null_count().to_pandas().to_string())\n",
    "\n",
    "df = apply_imputation(df, numerical_cols)\n",
    "\n",
    "logger.info(\"\\nMissing values after lag imputation:\")\n",
    "logger.info(df.select(numerical_cols).null_count().to_pandas().to_string())\n",
    "\n",
    "# Note: Ensure generate_embeddings.py creates entity embeddings for shop_id, item_id, item_category_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a9d0f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def scale_and_save_data(df, numerical_cols):\n",
    "    \"\"\"Apply robust scaling and save datasets.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert to pandas\n",
    "    monthly_sales = df.to_pandas()\n",
    "    del df  # Free memory\n",
    "    log_memory_usage()\n",
    "    \n",
    "    # Save unscaled data\n",
    "    monthly_sales.to_parquet('/workspace/processed_data/monthly_sales_unscaled.parquet')\n",
    "    logger.info(\"Saved unscaled data\")\n",
    "    \n",
    "    # Robust scaling\n",
    "    scaler = RobustScaler()\n",
    "    train_data = monthly_sales[monthly_sales['date_block_num'] < 30][numerical_cols]\n",
    "    logger.info(f\"Scaler training data shape: {train_data.shape}\")\n",
    "    \n",
    "    scaler.fit(train_data)\n",
    "    monthly_sales[numerical_cols] = scaler.transform(monthly_sales[numerical_cols])\n",
    "    \n",
    "    if monthly_sales[numerical_cols].isna().any().any():\n",
    "        raise ValueError(\"NaNs introduced during scaling\")\n",
    "    \n",
    "    with open('/workspace/processed_data/scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    # Optimize dtypes\n",
    "    dtypes = {col: 'float32' for col in numerical_cols}\n",
    "    dtypes.update({\n",
    "        'shop_id': 'int32', 'item_id': 'int32', 'item_category_id': 'int32',\n",
    "        'date_block_num': 'int16', 'month': 'int32', 'year': 'int32', 'is_holiday': 'int8'\n",
    "    })\n",
    "    monthly_sales = monthly_sales.astype(dtypes, errors='ignore')\n",
    "    \n",
    "    logger.info(f\"Scaling and saving time: {time.time() - start_time:.2f} seconds\")\n",
    "    log_memory_usage()\n",
    "    return monthly_sales\n",
    "\n",
    "# Scale and save data\n",
    "monthly_sales = scale_and_save_data(df, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec0f64",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def split_and_save_sets(df):\n",
    "    \"\"\"Split data into train/val/test per paper (months 0â€“30, 31, 32).\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_df = df[df['date_block_num'] <= 30]  # Inclusive\n",
    "    val_df = df[df['date_block_num'] == 31]\n",
    "    test_df = df[df['date_block_num'] == 32]\n",
    "    \n",
    "    logger.info(f\"Train: {train_df.shape}, Val: {val_df.shape}, Test: {test_df.shape}\")\n",
    "    \n",
    "    output_dir = '/workspace/processed_data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    train_df.to_parquet(os.path.join(output_dir, 'X_train_processed.parquet'))\n",
    "    val_df.to_parquet(os.path.join(output_dir, 'X_val_processed.parquet'))\n",
    "    test_df.to_parquet(os.path.join(output_dir, 'X_test_processed.parquet'))\n",
    "    df.to_parquet('/workspace/raw_data/processed_sales.parquet')\n",
    "    \n",
    "    logger.info(f\"Split and save time: {time.time() - start_time:.2f} seconds\")\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Split and save datasets\n",
    "train_df, val_df, test_df = split_and_save_sets(monthly_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a225900",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def save_metadata(df, numerical_cols):\n",
    "    \"\"\"Save feature and date index as JSON.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    feature_index = {col: i for i, col in enumerate(numerical_cols)}\n",
    "    date_index = df[['date']].reset_index().rename(columns={'index': 'row_index'}).to_dict(orient='records')\n",
    "    \n",
    "    metadata = {\n",
    "        \"feature_index\": feature_index,\n",
    "        \"date_index\": date_index\n",
    "    }\n",
    "    \n",
    "    with open('/workspace/processed_data/metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    logger.info(\"Saved metadata to /workspace/processed_data/metadata.json\")\n",
    "    logger.info(f\"Metadata save time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Save metadata\n",
    "save_metadata(monthly_sales, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b888c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def validate_statistics(raw_df, processed_df):\n",
    "    \"\"\"Compute coefficient of variation for raw and processed data.\"\"\"\n",
    "    raw_cv = raw_df['item_cnt_day'].std() / raw_df['item_cnt_day'].mean()\n",
    "    processed_cv = processed_df['item_cnt_day_winsor'].std() / processed_df['item_cnt_day_winsor'].mean()\n",
    "    logger.info(f\"Raw coefficient of variation: {raw_cv:.2f}\")\n",
    "    logger.info(f\"Processed coefficient of variation: {processed_cv:.2f}\")\n",
    "    if abs(raw_cv - 2.8) > 0.1 or abs(processed_cv - 1.9) > 0.1:\n",
    "        logger.warning(\"Coefficient of variation deviates from paper's reported values (raw: 2.8, processed: 1.9)\")\n",
    "\n",
    "# Validate statistics\n",
    "validate_statistics(data, monthly_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d894aa7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def validate_final_dataset(df):\n",
    "    \"\"\"Validate final dataset and log statistics.\"\"\"\n",
    "    logger.info(f\"Final dataset size: {len(df)}\")\n",
    "    logger.info(f\"Unique shops: {df['shop_id'].nunique()}, items: {df['item_id'].nunique()}\")\n",
    "    logger.info(f\"Date block range: {df['date_block_num'].min()}â€“{df['date_block_num'].max()}\")\n",
    "    \n",
    "    expected_size = 2935849  # Raw record count from paper\n",
    "    if abs(len(df) - expected_size) / expected_size > 0.1:\n",
    "        logger.warning(f\"Dataset size {len(df)} deviates from expected {expected_size}\")\n",
    "\n",
    "# Validate final dataset\n",
    "validate_final_dataset(monthly_sales)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
