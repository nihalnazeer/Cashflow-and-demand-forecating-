{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa50130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU device: NVIDIA RTX A6000\n",
      "GPU count: 1\n",
      "Current device: 0\n",
      "CUDA test operation successful: tensor([2., 3., 4.], device='cuda:0')\n",
      "NVIDIA-SMI output:\n",
      "Mon May 12 04:37:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:D5:00.0 Off |                    0 |\n",
      "| 30%   33C    P2             76W /  300W |     315MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "Total GPU memory: 44.45 GiB\n",
      "Allocated GPU memory: 0.00 GiB\n",
      "Reserved GPU memory: 0.00 GiB\n",
      "Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n",
      "Python executable: /workspace/XAI-2/nvenv/bin/python\n",
      "PATH: /workspace/XAI-2/nvenv/bin:/root/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/bin/remote-cli:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "Available disk space: 16.54 GiB\n",
      "Using device: cuda\n",
      "Cleared GPU memory cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import warnings\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def check_cuda_environment():\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available. This notebook requires a GPU.\")\n",
    "    \n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    \n",
    "    try:\n",
    "        test_tensor = torch.tensor([1.0, 2.0, 3.0], device='cuda')\n",
    "        test_result = test_tensor + 1\n",
    "        print(f\"CUDA test operation successful: {test_result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"CUDA test operation failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        nvidia_smi = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        print(\"NVIDIA-SMI output:\")\n",
    "        print(nvidia_smi.stdout)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run nvidia-smi: {e}\")\n",
    "    \n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 2**30:.2f} GiB\")\n",
    "    print(f\"Allocated GPU memory: {torch.cuda.memory_allocated(0) / 2**30:.2f} GiB\")\n",
    "    print(f\"Reserved GPU memory: {torch.cuda.memory_reserved(0) / 2**30:.2f} GiB\")\n",
    "\n",
    "try:\n",
    "    check_cuda_environment()\n",
    "except Exception as e:\n",
    "    print(f\"Error with PyTorch or CUDA setup: {e}\")\n",
    "    print(\"Try reinstalling PyTorch: pip install torch==2.7.0 --index-url https://download.pytorch.org/whl/cu124\")\n",
    "    raise\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"PATH: {os.environ.get('PATH')}\")\n",
    "print(f\"Available disk space: {shutil.disk_usage('/').free / (2**30):.2f} GiB\")\n",
    "\n",
    "if os.path.exists('/workspace/XAI/torch.py') or os.path.exists('/workspace/XAI/torch.pyc'):\n",
    "    print(\"Warning: Found 'torch.py' or 'torch.pyc' in /workspace/XAI. Please rename or remove it.\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared GPU memory cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd0e159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from /workspace/XAI-2/Predict Future Sales/merged_data.csv\n",
      "Dataset shape: (2935849, 10)\n",
      "Columns: ['date', 'date_block_num', 'shop_id', 'item_id', 'item_price', 'item_cnt_day', 'item_name', 'item_category_id', 'item_category_name', 'shop_name']\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_dir=\"/workspace/data\", file_name=\"merged_data.csv\"):\n",
    "    \"\"\"Load CSV data with fallback path.\"\"\"\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    alt_path = \"/workspace/XAI-2/Predict Future Sales/merged_data.csv\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        if os.path.exists(alt_path):\n",
    "            file_path = alt_path\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File not found at {file_path} or {alt_path}\")\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"Loaded data from {file_path}\")\n",
    "        print(f\"Dataset shape: {data.shape}\")\n",
    "        print(f\"Columns: {list(data.columns)}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load {file_path}: {e}\")\n",
    "    \n",
    "    expected_columns = ['date', 'shop_id', 'item_id', 'item_name', 'item_cnt_day', \n",
    "                        'item_price', 'item_category_id', 'shop_name', 'item_category_name', 'date_block_num']\n",
    "    missing_cols = [col for col in expected_columns if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing expected columns: {missing_cols}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83a2e1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holiday DataFrame created with 15 entries\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from datetime import datetime\n",
    "\n",
    "def get_russian_holidays():\n",
    "    \"\"\"Return DataFrame of Russian holidays and shopping events (2013–2015).\"\"\"\n",
    "    holidays = [\n",
    "        (\"2013-01-01\", \"New Year\"), (\"2014-01-01\", \"New Year\"), (\"2015-01-01\", \"New Year\"),\n",
    "        (\"2013-02-23\", \"Defender Day\"), (\"2014-02-23\", \"Defender Day\"), (\"2015-02-23\", \"Defender Day\"),\n",
    "        (\"2013-03-08\", \"Women's Day\"), (\"2014-03-08\", \"Women's Day\"), (\"2015-03-08\", \"Women's Day\"),\n",
    "        (\"2013-06-12\", \"Russia Day\"), (\"2014-06-12\", \"Russia Day\"), (\"2015-06-12\", \"Russia Day\"),\n",
    "        (\"2013-11-29\", \"Black Friday\"), (\"2014-11-28\", \"Black Friday\"), (\"2015-11-27\", \"Black Friday\")\n",
    "    ]\n",
    "    holiday_df = pl.DataFrame({\n",
    "        \"date\": [datetime.strptime(date, \"%Y-%m-%d\") for date, _ in holidays],\n",
    "        \"holiday\": [name for _, name in holidays]\n",
    "    }).with_columns([\n",
    "        pl.col(\"date\").dt.month().cast(pl.Int32).alias(\"month\"),\n",
    "        pl.col(\"date\").dt.year().cast(pl.Int32).alias(\"year\")\n",
    "    ])\n",
    "    return holiday_df\n",
    "\n",
    "holiday_df = get_russian_holidays()\n",
    "print(f\"Holiday DataFrame created with {len(holiday_df)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0edb2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset size: 2935849\n",
      "Unique shops: 60, items: 21807\n",
      "Date block range: 0–33\n",
      "Dataset size after date filter: 2882335\n",
      "Dataset size after shop filter: 2868195\n",
      "Unique shops after processing: 54\n",
      "Initial preprocessing time: 2.48 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import polars as pl\n",
    "\n",
    "def initial_preprocessing(df):\n",
    "    \"\"\"Perform initial data cleaning and filtering, including shop selection.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert to Polars\n",
    "    df = pl.from_pandas(df)\n",
    "    print(f\"Initial dataset size: {len(df)}\")\n",
    "    print(f\"Unique shops: {df['shop_id'].n_unique()}, items: {df['item_id'].n_unique()}\")\n",
    "    print(f\"Date block range: {df['date_block_num'].min()}–{df['date_block_num'].max()}\")\n",
    "    \n",
    "    # Parse dates\n",
    "    df = df.with_columns(\n",
    "        pl.col('date').str.strptime(pl.Date, \"%d.%m.%Y\")\n",
    "    ).with_columns([\n",
    "        pl.col('date').dt.month().cast(pl.Int32).alias('month'),\n",
    "        pl.col('date').dt.year().cast(pl.Int32).alias('year')\n",
    "    ])\n",
    "    \n",
    "    # Filter to date_block_num <= 32\n",
    "    df = df.filter(pl.col('date_block_num') <= 32)\n",
    "    print(f\"Dataset size after date filter: {len(df)}\")\n",
    "    \n",
    "    # Filter to top 54 shops by total sales volume\n",
    "    shop_sales = df.group_by('shop_id').agg(\n",
    "        total_sales=pl.col('item_cnt_day').sum()\n",
    "    ).sort('total_sales', descending=True).head(54)\n",
    "    valid_shops = shop_sales['shop_id'].to_list()\n",
    "    df = df.filter(pl.col('shop_id').is_in(valid_shops))\n",
    "    print(f\"Dataset size after shop filter: {len(df)}\")\n",
    "    print(f\"Unique shops after processing: {df['shop_id'].n_unique()}\")\n",
    "    \n",
    "    # Optimize dtypes\n",
    "    df = df.with_columns([\n",
    "        pl.col('date_block_num').cast(pl.Int16),\n",
    "        pl.col('shop_id').cast(pl.Int32),\n",
    "        pl.col('item_id').cast(pl.Int32),\n",
    "        pl.col('item_category_id').cast(pl.Int32),\n",
    "        pl.col('item_cnt_day').cast(pl.Float32),\n",
    "        pl.col('item_price').cast(pl.Float32),\n",
    "        pl.col('month').cast(pl.Int32),\n",
    "        pl.col('year').cast(pl.Int32)\n",
    "    ])\n",
    "    \n",
    "    print(f\"Initial preprocessing time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = initial_preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61623c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_cnt_day distribution before processing:\n",
      "shape: (9, 2)\n",
      "┌────────────┬────────────┐\n",
      "│ statistic  ┆ value      │\n",
      "│ ---        ┆ ---        │\n",
      "│ str        ┆ f64        │\n",
      "╞════════════╪════════════╡\n",
      "│ count      ┆ 2.868195e6 │\n",
      "│ null_count ┆ 0.0        │\n",
      "│ mean       ┆ 1.240954   │\n",
      "│ std        ┆ 2.28652    │\n",
      "│ min        ┆ -22.0      │\n",
      "│ 25%        ┆ 1.0        │\n",
      "│ 50%        ┆ 1.0        │\n",
      "│ 75%        ┆ 1.0        │\n",
      "│ max        ┆ 1000.0     │\n",
      "└────────────┴────────────┘\n",
      "Rows with item_cnt_day > 100: 135\n",
      "Rows with item_cnt_day > 500: 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers capped: 9314 (0.32%)\n",
      "Rolling quantile stats:\n",
      "shape: (9, 2)\n",
      "┌────────────┬────────────┐\n",
      "│ statistic  ┆ value      │\n",
      "│ ---        ┆ ---        │\n",
      "│ str        ┆ f64        │\n",
      "╞════════════╪════════════╡\n",
      "│ count      ┆ 2.868195e6 │\n",
      "│ null_count ┆ 0.0        │\n",
      "│ mean       ┆ 2.361271   │\n",
      "│ std        ┆ 6.042789   │\n",
      "│ min        ┆ -5.0       │\n",
      "│ 25%        ┆ 1.0        │\n",
      "│ 50%        ┆ 1.0        │\n",
      "│ 75%        ┆ 2.0        │\n",
      "│ max        ┆ 1000.0     │\n",
      "└────────────┴────────────┘\n",
      "Max item_cnt_day_winsor: 1000.0\n",
      "Negative sales after processing: 0\n",
      "Negative sales and outlier handling time: 1.00 seconds\n"
     ]
    }
   ],
   "source": [
    "def handle_negative_sales_and_outliers(df):\n",
    "    \"\"\"Apply rolling Winsorization and handle negative sales at daily level.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Diagnostics: Inspect item_cnt_day distribution\n",
    "    print(\"item_cnt_day distribution before processing:\")\n",
    "    print(df['item_cnt_day'].describe())\n",
    "    print(f\"Rows with item_cnt_day > 100: {df.filter(pl.col('item_cnt_day') > 100).height}\")\n",
    "    print(f\"Rows with item_cnt_day > 500: {df.filter(pl.col('item_cnt_day') > 500).height}\")\n",
    "    \n",
    "    # Sort for rolling operations\n",
    "    df = df.sort(['shop_id', 'item_id', 'date'])\n",
    "    \n",
    "    # Rolling Winsorization (30-day window, 95th percentile) with static cap\n",
    "    df = df.with_columns(\n",
    "        rolling_quantile=pl.col('item_cnt_day').rolling_quantile(\n",
    "            quantile=0.95, window_size=30, min_periods=1\n",
    "        ).over(['shop_id', 'item_id'])\n",
    "    ).with_columns(\n",
    "        item_cnt_day_winsor=pl.col('item_cnt_day').clip(None, pl.min_horizontal(pl.col('rolling_quantile'), 1000))\n",
    "    )\n",
    "    outlier_count = df.filter(pl.col('item_cnt_day') > pl.col('rolling_quantile')).height\n",
    "    print(f\"Outliers capped: {outlier_count} ({outlier_count / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    # Diagnostics: Inspect rolling quantile and winsorized values\n",
    "    print(\"Rolling quantile stats:\")\n",
    "    print(df['rolling_quantile'].describe())\n",
    "    print(f\"Max item_cnt_day_winsor: {df['item_cnt_day_winsor'].max()}\")\n",
    "    \n",
    "    # Handle negative sales\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col('item_cnt_day_winsor') < 0)\n",
    "          .then(pl.col('item_cnt_day_winsor').abs())\n",
    "          .otherwise(0)\n",
    "          .alias('returns'),\n",
    "        pl.col('item_cnt_day_winsor').clip(lower_bound=0).alias('item_cnt_day_winsor')\n",
    "    ])\n",
    "    print(f\"Negative sales after processing: {df.filter(pl.col('item_cnt_day_winsor') < 0).height}\")\n",
    "    \n",
    "    # Drop temporary column\n",
    "    df = df.drop('rolling_quantile')\n",
    "    \n",
    "    print(f\"Negative sales and outlier handling time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = handle_negative_sales_and_outliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c5f95c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_cnt_day_winsor distribution after aggregation:\n",
      "shape: (9, 2)\n",
      "┌────────────┬───────────┐\n",
      "│ statistic  ┆ value     │\n",
      "│ ---        ┆ ---       │\n",
      "│ str        ┆ f64       │\n",
      "╞════════════╪═══════════╡\n",
      "│ count      ┆ 1.56857e6 │\n",
      "│ null_count ┆ 0.0       │\n",
      "│ mean       ┆ 2.254784  │\n",
      "│ std        ┆ 8.010804  │\n",
      "│ min        ┆ 0.0       │\n",
      "│ 25%        ┆ 1.0       │\n",
      "│ 50%        ┆ 1.0       │\n",
      "│ 75%        ┆ 2.0       │\n",
      "│ max        ┆ 1274.0    │\n",
      "└────────────┴───────────┘\n",
      "Dataset size after aggregation: 1568570\n",
      "Unique shops: 54, items: 21309\n",
      "Aggregation time: 0.26 seconds\n"
     ]
    }
   ],
   "source": [
    "def aggregate_to_monthly(df):\n",
    "    \"\"\"Aggregate data to monthly level.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Aggregate to monthly\n",
    "    df = df.group_by(['date_block_num', 'shop_id', 'item_id', 'item_category_id', 'month', 'year']).agg([\n",
    "        pl.col('item_cnt_day_winsor').sum().alias('item_cnt_day_winsor'),\n",
    "        pl.col('returns').sum().alias('returns'),\n",
    "        pl.col('item_price').mean().alias('item_price')\n",
    "    ])\n",
    "    \n",
    "    # Diagnostics: Inspect aggregated item_cnt_day_winsor\n",
    "    print(\"item_cnt_day_winsor distribution after aggregation:\")\n",
    "    print(df['item_cnt_day_winsor'].describe())\n",
    "    \n",
    "    print(f\"Dataset size after aggregation: {len(df)}\")\n",
    "    print(f\"Unique shops: {df['shop_id'].n_unique()}, items: {df['item_id'].n_unique()}\")\n",
    "    print(f\"Aggregation time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = aggregate_to_monthly(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9234efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: 37972638, after merge: 37972638\n",
      "Unique shops: 54, items: 21309\n",
      "Grid creation time: 5.13 seconds\n"
     ]
    }
   ],
   "source": [
    "def create_full_grid(df):\n",
    "    \"\"\"Create full shop-item-month grid.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    shops = df['shop_id'].unique().to_list()\n",
    "    items = df['item_id'].unique().to_list()\n",
    "    date_blocks = list(range(33))  # 0–32\n",
    "    \n",
    "    grid = pl.DataFrame({'shop_id': shops}).join(\n",
    "        pl.DataFrame({'item_id': items}), how='cross'\n",
    "    ).join(\n",
    "        pl.DataFrame({'date_block_num': date_blocks}), how='cross'\n",
    "    ).with_columns([\n",
    "        pl.col('shop_id').cast(pl.Int32),\n",
    "        pl.col('item_id').cast(pl.Int32),\n",
    "        pl.col('date_block_num').cast(pl.Int16),\n",
    "        ((pl.col('date_block_num') % 12) + 1).cast(pl.Int32).alias('month'),\n",
    "        ((pl.col('date_block_num') // 12) + 2013).cast(pl.Int32).alias('year')\n",
    "    ])\n",
    "    \n",
    "    # Merge with aggregated data\n",
    "    df = grid.join(\n",
    "        df, on=['shop_id', 'item_id', 'date_block_num', 'month', 'year'], how='left'\n",
    "    ).with_columns([\n",
    "        pl.col('item_cnt_day_winsor').fill_null(0),\n",
    "        pl.col('returns').fill_null(0),\n",
    "        pl.col('item_price').fill_null(pl.col('item_price').mean().over('item_id')).fill_null(0),\n",
    "        pl.col('item_category_id').fill_null(pl.col('item_category_id').first().over('item_id')).fill_null(0)\n",
    "    ]).with_columns(\n",
    "        pl.datetime(pl.col('year'), pl.col('month'), 1).alias('date')\n",
    "    )\n",
    "    \n",
    "    print(f\"Grid size: {len(grid)}, after merge: {len(df)}\")\n",
    "    print(f\"Unique shops: {df['shop_id'].n_unique()}, items: {df['item_id'].n_unique()}\")\n",
    "    print(f\"Grid creation time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "# Create full grid\n",
    "df = create_full_grid(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f99ec0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation time: 61.02 seconds\n"
     ]
    }
   ],
   "source": [
    "def seasonal_imputation(df, col):\n",
    "    \"\"\"Apply seasonality-aware imputation to a column.\"\"\"\n",
    "    df = df.with_columns(\n",
    "        pl.col(col).interpolate().over(['shop_id', 'item_id']).alias(f'{col}_interp')\n",
    "    ).with_columns(\n",
    "        seasonal_value=pl.col(col).shift(12).over(['shop_id', 'item_id', 'month']),\n",
    "        ma_value=pl.col(col).rolling_mean(window_size=12, min_periods=1).over(['shop_id', 'item_id'])\n",
    "    ).with_columns(\n",
    "        pl.when(pl.col(f'{col}_interp').is_null() & pl.col('seasonal_value').is_not_null())\n",
    "          .then(pl.col('seasonal_value'))\n",
    "          .when(pl.col(f'{col}_interp').is_null())\n",
    "          .then(pl.col('ma_value'))\n",
    "          .otherwise(pl.col(f'{col}_interp'))\n",
    "          .alias(col)\n",
    "    ).drop([f'{col}_interp', 'seasonal_value', 'ma_value'])\n",
    "    return df\n",
    "\n",
    "def apply_imputation(df, cols):\n",
    "    \"\"\"Impute missing values for specified columns.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for col in cols:\n",
    "        df = seasonal_imputation(df, col)\n",
    "    df = df.with_columns([pl.col(col).fill_null(0) for col in cols])\n",
    "    \n",
    "    print(f\"Imputation time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "# Apply imputation\n",
    "numerical_cols = ['item_cnt_day_winsor', 'returns', 'item_price']\n",
    "df = apply_imputation(df, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "938d8b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holiday feature addition time: 0.60 seconds\n"
     ]
    }
   ],
   "source": [
    "def add_holiday_features(df, holiday_df):\n",
    "    \"\"\"Add holiday features to the dataset.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = df.join(\n",
    "        holiday_df.select(['year', 'month', 'holiday']),\n",
    "        on=['year', 'month'], how='left'\n",
    "    ).with_columns(\n",
    "        is_holiday=pl.col('holiday').is_not_null().cast(pl.Int8),\n",
    "        holiday=pl.col('holiday').fill_null('None')\n",
    "    )\n",
    "    \n",
    "    print(f\"Holiday feature addition time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "# Add holiday features\n",
    "df = add_holiday_features(df, holiday_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d6be2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records dropped due to >30% missing: 37883835\n",
      "Records after filtering: 88803, shop-item pairs: 2691\n",
      "Filtering time: 0.47 seconds\n"
     ]
    }
   ],
   "source": [
    "def filter_sparse_products(df):\n",
    "    \"\"\"Exclude shop-item pairs with >30% missing data.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    shop_item_missing = df.group_by(['shop_id', 'item_id']).agg(\n",
    "        missing_ratio=pl.col('item_cnt_day_winsor').eq(0).mean()\n",
    "    )\n",
    "    valid_shop_items = shop_item_missing.filter(pl.col('missing_ratio') <= 0.3).select(['shop_id', 'item_id'])\n",
    "    initial_size = len(df)\n",
    "    df = df.join(valid_shop_items, on=['shop_id', 'item_id'], how='inner')\n",
    "    \n",
    "    print(f\"Records dropped due to >30% missing: {initial_size - len(df)}\")\n",
    "    print(f\"Records after filtering: {len(df)}, shop-item pairs: {len(valid_shop_items)}\")\n",
    "    print(f\"Filtering time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "# Filter sparse products\n",
    "df = filter_sparse_products(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10cba67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag feature creation time: 0.09 seconds\n"
     ]
    }
   ],
   "source": [
    "def create_lag_features(df):\n",
    "    \"\"\"Create lag features for 1–3 months.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = df.sort(['shop_id', 'item_id', 'date'])\n",
    "    for lag in [1, 2, 3]:\n",
    "        df = df.with_columns([\n",
    "            pl.col('item_cnt_day_winsor').shift(lag).over(['shop_id', 'item_id']).alias(f'lag_sales_{lag}'),\n",
    "            pl.col('returns').shift(lag).over(['shop_id', 'item_id']).alias(f'lag_returns_{lag}'),\n",
    "            pl.col('item_price').shift(lag).over(['shop_id', 'item_id']).alias(f'lag_price_{lag}')\n",
    "        ])\n",
    "    \n",
    "    print(f\"Lag feature creation time: {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "# Create lag features\n",
    "df = create_lag_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "459d7f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values before lag imputation:\n",
      "   item_cnt_day_winsor  returns  item_price  lag_sales_1  lag_sales_2  lag_sales_3  lag_returns_1  lag_returns_2  lag_returns_3  lag_price_1  lag_price_2  lag_price_3\n",
      "0                    0        0           0         2691         5382         8073           2691           5382           8073         2691         5382         8073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation time: 1.03 seconds\n",
      "\n",
      "Missing values after lag imputation:\n",
      "   item_cnt_day_winsor  returns  item_price  lag_sales_1  lag_sales_2  lag_sales_3  lag_returns_1  lag_returns_2  lag_returns_3  lag_price_1  lag_price_2  lag_price_3\n",
      "0                    0        0           0            0            0            0              0              0              0            0            0            0\n"
     ]
    }
   ],
   "source": [
    "# Impute lag features\n",
    "numerical_cols += ['lag_sales_1', 'lag_sales_2', 'lag_sales_3',\n",
    "                   'lag_returns_1', 'lag_returns_2', 'lag_returns_3',\n",
    "                   'lag_price_1', 'lag_price_2', 'lag_price_3']\n",
    "print(\"\\nMissing values before lag imputation:\")\n",
    "print(df.select(numerical_cols).null_count().to_pandas().to_string())\n",
    "\n",
    "df = apply_imputation(df, numerical_cols)\n",
    "\n",
    "print(\"\\nMissing values after lag imputation:\")\n",
    "print(df.select(numerical_cols).null_count().to_pandas().to_string())\n",
    "\n",
    "# Note: Ensure generate_embeddings.py creates entity embeddings for shop_id, item_id, item_category_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "592a9d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved unscaled data\n",
      "Scaler training data shape: (80730, 12)\n",
      "Scaling and saving time: 0.28 seconds\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def scale_and_save_data(df, numerical_cols):\n",
    "    \"\"\"Apply robust scaling and save datasets.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert to pandas\n",
    "    monthly_sales = df.to_pandas()\n",
    "    del df  # Free memory\n",
    "    \n",
    "    # Save unscaled data\n",
    "    monthly_sales.to_parquet('/workspace/XAI-2/processed_data/monthly_sales_unscaled.parquet')\n",
    "    print(\"Saved unscaled data\")\n",
    "    \n",
    "    # Robust scaling\n",
    "    scaler = RobustScaler()\n",
    "    train_data = monthly_sales[monthly_sales['date_block_num'] < 30][numerical_cols]\n",
    "    print(f\"Scaler training data shape: {train_data.shape}\")\n",
    "    \n",
    "    scaler.fit(train_data)\n",
    "    monthly_sales[numerical_cols] = scaler.transform(monthly_sales[numerical_cols])\n",
    "    \n",
    "    if monthly_sales[numerical_cols].isna().any().any():\n",
    "        raise ValueError(\"NaNs introduced during scaling\")\n",
    "    \n",
    "    with open('/workspace/XAI-2/processed_data/scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    # Optimize dtypes\n",
    "    dtypes = {col: 'float32' for col in numerical_cols}\n",
    "    dtypes.update({\n",
    "        'shop_id': 'int32', 'item_id': 'int32', 'item_category_id': 'int32',\n",
    "        'date_block_num': 'int16', 'month': 'int32', 'year': 'int32', 'is_holiday': 'int8'\n",
    "    })\n",
    "    monthly_sales = monthly_sales.astype(dtypes, errors='ignore')\n",
    "    \n",
    "    print(f\"Scaling and saving time: {time.time() - start_time:.2f} seconds\")\n",
    "    return monthly_sales\n",
    "\n",
    "# Scale and save data\n",
    "monthly_sales = scale_and_save_data(df, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec0f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (83421, 19), y_train: (83421, 1)\n",
      "X_val: (2691, 19), y_val: (2691, 1)\n",
      "X_test: (2691, 19), y_test: (2691, 1)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/workspace/XAI-2/raw_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_train, y_train, X_val, y_val, X_test, y_test\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Split and save datasets\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m X_train, y_train, X_val, y_val, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43msplit_and_save_sets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonthly_sales\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 39\u001b[0m, in \u001b[0;36msplit_and_save_sets\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     36\u001b[0m y_test\u001b[38;5;241m.\u001b[39mto_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test_processed.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Save full dataset\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/workspace/XAI-2/raw_data/processed_sales.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplit and save time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train, y_train, X_val, y_val, X_test, y_test\n",
      "File \u001b[0;32m/workspace/XAI-2/nvenv/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/XAI-2/nvenv/lib/python3.10/site-packages/pandas/core/frame.py:3113\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3033\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   3034\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3109\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   3110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 3113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3122\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/XAI-2/nvenv/lib/python3.10/site-packages/pandas/io/parquet.py:480\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    478\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 480\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m/workspace/XAI-2/nvenv/lib/python3.10/site-packages/pandas/io/parquet.py:198\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     merged_metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexisting_metadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdf_metadata}\n\u001b[1;32m    196\u001b[0m     table \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mreplace_schema_metadata(merged_metadata)\n\u001b[0;32m--> 198\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(path_or_handle, io\u001b[38;5;241m.\u001b[39mBufferedWriter)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(path_or_handle, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle\u001b[38;5;241m.\u001b[39mname, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m))\n\u001b[1;32m    209\u001b[0m ):\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mbytes\u001b[39m):\n",
      "File \u001b[0;32m/workspace/XAI-2/nvenv/lib/python3.10/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/workspace/XAI-2/nvenv/lib/python3.10/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/XAI-2/nvenv/lib/python3.10/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/workspace/XAI-2/raw_data'"
     ]
    }
   ],
   "source": [
    "def split_and_save_sets(df):\n",
    "    \"\"\"Split data into train/val/test per paper (months 0–30, 31, 32) and save X and y.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define splits\n",
    "    train_df = df[df['date_block_num'] <= 30]  # Inclusive\n",
    "    val_df = df[df['date_block_num'] == 31]\n",
    "    test_df = df[df['date_block_num'] == 32]\n",
    "    \n",
    "    # Extract features (X) and target (y)\n",
    "    target_col = 'item_cnt_day_winsor'\n",
    "    exclude_cols = [target_col, 'holiday']  # Exclude target and non-numeric holiday\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[[target_col]]\n",
    "    X_val = val_df[feature_cols]\n",
    "    y_val = val_df[[target_col]]\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df[[target_col]]\n",
    "    \n",
    "    # Print shapes\n",
    "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "    \n",
    "    # Save splits\n",
    "    output_dir = '/workspace/XAI-2/processed_data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    X_train.to_parquet(os.path.join(output_dir, 'X_train_processed.parquet'))\n",
    "    y_train.to_parquet(os.path.join(output_dir, 'y_train_processed.parquet'))\n",
    "    X_val.to_parquet(os.path.join(output_dir, 'X_val_processed.parquet'))\n",
    "    y_val.to_parquet(os.path.join(output_dir, 'y_val_processed.parquet'))\n",
    "    X_test.to_parquet(os.path.join(output_dir, 'X_test_processed.parquet'))\n",
    "    y_test.to_parquet(os.path.join(output_dir, 'y_test_processed.parquet'))\n",
    "    \n",
    "    # Save full dataset\n",
    "    df.to_parquet('/workspace/XAI-2/processed_data/')\n",
    "    \n",
    "    print(f\"Split and save time: {time.time() - start_time:.2f} seconds\")\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Split and save datasets\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_and_save_sets(monthly_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a225900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata(df, numerical_cols):\n",
    "    \"\"\"Save feature and date index as JSON.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    feature_index = {col: i for i, col in enumerate(numerical_cols)}\n",
    "    date_index = df[['date']].reset_index().rename(columns={'index': 'row_index'}).to_dict(orient='records')\n",
    "    \n",
    "    metadata = {\n",
    "        \"feature_index\": feature_index,\n",
    "        \"date_index\": date_index\n",
    "    }\n",
    "    \n",
    "    with open('/workspace/processed_data/metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    logger.info(\"Saved metadata to /workspace/processed_data/metadata.json\")\n",
    "    logger.info(f\"Metadata save time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Save metadata\n",
    "save_metadata(monthly_sales, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b888c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_statistics(raw_df, processed_df):\n",
    "    \"\"\"Compute coefficient of variation for raw and processed data.\"\"\"\n",
    "    raw_cv = raw_df['item_cnt_day'].std() / raw_df['item_cnt_day'].mean()\n",
    "    processed_cv = processed_df['item_cnt_day_winsor'].std() / processed_df['item_cnt_day_winsor'].mean()\n",
    "    logger.info(f\"Raw coefficient of variation: {raw_cv:.2f}\")\n",
    "    logger.info(f\"Processed coefficient of variation: {processed_cv:.2f}\")\n",
    "    if abs(raw_cv - 2.8) > 0.1 or abs(processed_cv - 1.9) > 0.1:\n",
    "        logger.warning(\"Coefficient of variation deviates from paper's reported values (raw: 2.8, processed: 1.9)\")\n",
    "\n",
    "# Validate statistics\n",
    "validate_statistics(data, monthly_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d894aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_final_dataset(df):\n",
    "    \"\"\"Validate final dataset and log statistics.\"\"\"\n",
    "    logger.info(f\"Final dataset size: {len(df)}\")\n",
    "    logger.info(f\"Unique shops: {df['shop_id'].nunique()}, items: {df['item_id'].nunique()}\")\n",
    "    logger.info(f\"Date block range: {df['date_block_num'].min()}–{df['date_block_num'].max()}\")\n",
    "    \n",
    "    expected_size = 2935849  # Raw record count from paper\n",
    "    if abs(len(df) - expected_size) / expected_size > 0.1:\n",
    "        logger.warning(f\"Dataset size {len(df)} deviates from expected {expected_size}\")\n",
    "\n",
    "# Validate final dataset\n",
    "validate_final_dataset(monthly_sales)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
