{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19295381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five rows of the data:\n",
      "         date  date_block_num  shop_id  item_id  item_price  item_cnt_day  \\\n",
      "0  02.01.2013               0       59    22154      999.00           1.0   \n",
      "1  03.01.2013               0       25     2552      899.00           1.0   \n",
      "2  05.01.2013               0       25     2552      899.00          -1.0   \n",
      "3  06.01.2013               0       25     2554     1709.05           1.0   \n",
      "4  15.01.2013               0       25     2555     1099.00           1.0   \n",
      "\n",
      "                                  item_name  item_category_id  \\\n",
      "0                    ANNOUNCEMENT 2012 (BD)                37   \n",
      "1  DEEP PURPLE  The House Of Blue Light  LP                58   \n",
      "2  DEEP PURPLE  The House Of Blue Light  LP                58   \n",
      "3  DEEP PURPLE  Who Do You Think We Are  LP                58   \n",
      "4            DEEP PURPE 30 Very Best Of 2CD                56   \n",
      "\n",
      "                 item_category_name            shop_name  \n",
      "0                 Cinema - Blue-Ray  Jaroslavl TC Altair  \n",
      "1                     Music - Vinyl    Moscow ARK Atrium  \n",
      "2                     Music - Vinyl    Moscow ARK Atrium  \n",
      "3                     Music - Vinyl    Moscow ARK Atrium  \n",
      "4  Music - a CD of brand production    Moscow ARK Atrium  \n",
      "\n",
      "Data Shape: \n",
      "(2935849, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_path = pd.read_csv(\"/Users/mohammednihal/XAI-1/Predict Future Sales/merged_data.csv\")\n",
    "df = data_path\n",
    "\n",
    "print(\"First five rows of the data:\")\n",
    "print(df.head())\n",
    "print(\"\\nData Shape: \")\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ce598356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                   1034\n",
       "date_block_num           34\n",
       "shop_id                  60\n",
       "item_id               21807\n",
       "item_price            19993\n",
       "item_cnt_day            198\n",
       "item_name             21233\n",
       "item_category_id         84\n",
       "item_category_name       83\n",
       "shop_name                60\n",
       "dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "637eed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset with 2,935,849 records\n",
      "Initial unique counts:\n",
      "item_id: 21807\n",
      "item_name: 21233\n",
      "item_name nulls before imputation: 84\n",
      "item_id with 'Unknown' item_name: 0\n",
      "Unique item_name per item_id:\n",
      "item_name\n",
      "1    21806\n",
      "0        1\n",
      "Name: count, dtype: int64\n",
      "Initial null counts:\n",
      "Column 'date': 0 nulls (0.00%)\n",
      "Column 'date_block_num': 0 nulls (0.00%)\n",
      "Column 'shop_id': 0 nulls (0.00%)\n",
      "Column 'item_id': 0 nulls (0.00%)\n",
      "Column 'item_price': 0 nulls (0.00%)\n",
      "Column 'item_cnt_day': 0 nulls (0.00%)\n",
      "Column 'item_name': 84 nulls (0.00%)\n",
      "Column 'item_category_id': 0 nulls (0.00%)\n",
      "Column 'item_category_name': 0 nulls (0.00%)\n",
      "Column 'shop_name': 0 nulls (0.00%)\n",
      "Created Return column. Total Returns: 7,541.0\n",
      "Converted date column to datetime format\n",
      "Imputed 84 missing item names with 'Unknown'\n",
      "Selected 54 shops after removing ['36', '11', '20', '8', '9', '40']\n",
      "Removed 14,017 records from shop selection\n",
      "Winsorization applied at 99th percentile for item_cnt_day after 3σ rolling-window detection\n",
      "Total records: 2,921,832\n",
      "Outliers (above 3σ): 2,072 (0.07%)\n",
      "Values clipped (>99th percentile): 48,150 (1.65%)\n",
      "Final unique counts:\n",
      "item_id: 21600\n",
      "item_name: 21027\n"
     ]
    }
   ],
   "source": [
    "def data_preprocessing(df):\n",
    "    expected_columns = ['date', 'date_block_num', 'shop_id', 'item_id', 'item_price', 'item_cnt_day',\n",
    "                       'item_name', 'item_category_id', 'item_category_name', 'shop_name']\n",
    "    if not all(col in df.columns for col in expected_columns):\n",
    "        missing = set(expected_columns) - set(df.columns)\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "    \n",
    "    print(f\"Preprocessing dataset with {len(df):,} records\")\n",
    "    \n",
    "    cleaned_df = df.copy()\n",
    "    # Initial checks\n",
    "    print(\"Initial unique counts:\")\n",
    "    print(f\"item_id: {cleaned_df['item_id'].nunique()}\")\n",
    "    print(f\"item_name: {cleaned_df['item_name'].nunique()}\")\n",
    "    print(\"item_name nulls before imputation:\", cleaned_df['item_name'].isna().sum())\n",
    "    print(\"item_id with 'Unknown' item_name:\", (cleaned_df['item_name'] == 'Unknown').sum())\n",
    "    print(\"Unique item_name per item_id:\")\n",
    "    name_counts = cleaned_df.groupby('item_id')['item_name'].nunique()\n",
    "    print(name_counts.value_counts())\n",
    "    if (name_counts > 1).any():\n",
    "        print(\"item_id with multiple item_name:\", name_counts[name_counts > 1].index.tolist())\n",
    "\n",
    "    cleaned_df['item_price'] = cleaned_df['item_price'].astype(np.float32)\n",
    "    cleaned_df['item_cnt_day'] = cleaned_df['item_cnt_day'].astype(np.float32)\n",
    "    \n",
    "    print(\"Initial null counts:\")\n",
    "    for col in cleaned_df.columns:\n",
    "        nulls = cleaned_df[col].isna().sum()\n",
    "        print(f\"Column '{col}': {nulls:,} nulls ({nulls/len(cleaned_df)*100:.2f}%)\")\n",
    "    \n",
    "    cleaned_df['Return'] = cleaned_df['item_cnt_day'].where(cleaned_df['item_cnt_day'] < 0, 0).abs().astype(np.float32)\n",
    "    cleaned_df['item_cnt_day'] = cleaned_df['item_cnt_day'].clip(lower=0)\n",
    "    print(f\"Created Return column. Total Returns: {cleaned_df['Return'].sum():,}\")\n",
    "    \n",
    "    cleaned_df['date'] = pd.to_datetime(cleaned_df['date'], format='%d.%m.%Y')\n",
    "    print(\"Converted date column to datetime format\")\n",
    "    \n",
    "    # Handle item_name\n",
    "    if 'item_name' in cleaned_df.columns and cleaned_df['item_name'].isna().any():\n",
    "        item_name_nulls = cleaned_df['item_name'].isna().sum()\n",
    "        cleaned_df['item_name'] = cleaned_df['item_name'].fillna('Unknown')\n",
    "        print(f\"Imputed {item_name_nulls:,} missing item names with 'Unknown'\")\n",
    "    \n",
    "    # Ensure string types\n",
    "    cleaned_df['shop_id'] = cleaned_df['shop_id'].astype(str)\n",
    "    cleaned_df['item_id'] = cleaned_df['item_id'].astype(str)\n",
    "    cleaned_df['item_name'] = cleaned_df['item_name'].astype(str)\n",
    "    \n",
    "    # Fix multiple item_name per item_id\n",
    "    name_counts = cleaned_df.groupby('item_id')['item_name'].nunique()\n",
    "    if (name_counts > 1).any():\n",
    "        print(f\"Warning: {name_counts[name_counts > 1].count()} item_id(s) have multiple item_name values. Taking most frequent.\")\n",
    "        most_frequent = cleaned_df.groupby('item_id')['item_name'].agg(lambda x: x.mode()[0]).reset_index()\n",
    "        cleaned_df = cleaned_df.drop(columns='item_name').merge(most_frequent, on='item_id', how='left')\n",
    "    \n",
    "    shop_stats = cleaned_df.groupby('shop_id').size().reset_index(name='count')\n",
    "    shops_to_remove = shop_stats.nsmallest(6, 'count')['shop_id'].tolist()\n",
    "    remove_records = cleaned_df[cleaned_df['shop_id'].isin(shops_to_remove)].shape[0]\n",
    "    cleaned_df = cleaned_df[~cleaned_df['shop_id'].isin(shops_to_remove)]\n",
    "    print(f\"Selected {cleaned_df['shop_id'].nunique()} shops after removing {shops_to_remove}\")\n",
    "    print(f\"Removed {remove_records:,} records from shop selection\")\n",
    "    \n",
    "    cleaned_df = cleaned_df.sort_values(['shop_id', 'item_id', 'date'])\n",
    "    \n",
    "    def winsorize_with_rolling_stats(group):\n",
    "        group = group.set_index('date').sort_index()\n",
    "        rolling_mean = group['item_cnt_day'].rolling(window='30D', min_periods=1).mean()\n",
    "        rolling_std = group['item_cnt_day'].rolling(window='30D', min_periods=1).std()\n",
    "        upper_3sigma = rolling_mean + 3 * rolling_std\n",
    "        \n",
    "        outliers = group['item_cnt_day'] > upper_3sigma\n",
    "        outlier_count = outliers.sum()\n",
    "        \n",
    "        winsor_limit = group['item_cnt_day'].quantile(0.99)\n",
    "        clipped = group['item_cnt_day'].clip(upper=winsor_limit)\n",
    "        clipped_count = (group['item_cnt_day'] > winsor_limit).sum()\n",
    "        \n",
    "        return clipped.reset_index(drop=True), outlier_count, clipped_count\n",
    "        \n",
    "    total_records = len(cleaned_df)\n",
    "    item_cnt_day_winsorized = []\n",
    "    total_outlier = 0\n",
    "    total_clipped = 0\n",
    "    \n",
    "    for (shop_id, item_id), group in cleaned_df.groupby(['shop_id', 'item_id']):\n",
    "        clipped_series, outliers, clipped = winsorize_with_rolling_stats(group.copy())\n",
    "        item_cnt_day_winsorized.extend(clipped_series)\n",
    "        total_outlier += outliers\n",
    "        total_clipped += clipped\n",
    "        \n",
    "    cleaned_df['item_cnt_day'] = item_cnt_day_winsorized\n",
    "    \n",
    "    outlier_percentage = (total_outlier / total_records * 100) if total_records > 0 else 0\n",
    "    clipped_percentage = (total_clipped / total_records * 100) if total_records > 0 else 0\n",
    "\n",
    "    print(\"Winsorization applied at 99th percentile for item_cnt_day after 3σ rolling-window detection\")\n",
    "    print(f\"Total records: {total_records:,}\")\n",
    "    print(f\"Outliers (above 3σ): {total_outlier:,} ({outlier_percentage:.2f}%)\")\n",
    "    print(f\"Values clipped (>99th percentile): {total_clipped:,} ({clipped_percentage:.2f}%)\")\n",
    "    \n",
    "    # Final checks\n",
    "    print(\"Final unique counts:\")\n",
    "    print(f\"item_id: {cleaned_df['item_id'].nunique()}\")\n",
    "    print(f\"item_name: {cleaned_df['item_name'].nunique()}\")\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "cleaned_data = data_preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e5d13ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21027"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data['item_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7fdbfc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created base grid with 13,529,586 shop-item-month combinations\n",
      "Missing after fix:\n",
      " date                  0\n",
      "item_name             0\n",
      "item_category_id      0\n",
      "item_category_name    0\n",
      "shop_name             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_sales_grid(cleaned_df, min_months_item=3, min_months_shop=5):\n",
    "    cleaned_df_copy = cleaned_df.copy()\n",
    "\n",
    "    active_items = (\n",
    "        cleaned_df_copy.groupby('item_id')['date_block_num'].nunique()\n",
    "        .loc[lambda x: x >= min_months_item].index\n",
    "    )\n",
    "    active_shops = (\n",
    "        cleaned_df_copy.groupby('shop_id')['date_block_num'].nunique()\n",
    "        .loc[lambda x: x >= min_months_shop].index\n",
    "    )\n",
    "    filtered_df = cleaned_df_copy[\n",
    "        cleaned_df_copy['item_id'].isin(active_items) &\n",
    "        cleaned_df_copy['shop_id'].isin(active_shops)\n",
    "    ]\n",
    "    \n",
    "    # Valid shop-item pairs\n",
    "    valid_pairs = filtered_df[['shop_id', 'item_id']].drop_duplicates()\n",
    "    months = sorted(filtered_df['date_block_num'].unique())\n",
    "    full_grid = []\n",
    "    for month in months:\n",
    "        temp = valid_pairs.copy()\n",
    "        temp['date_block_num'] = month\n",
    "        full_grid.append(temp)\n",
    "    full_grid = pd.concat(full_grid, ignore_index=True)\n",
    "\n",
    "    # Monthly aggregation\n",
    "    monthly_data = filtered_df.groupby(\n",
    "        ['shop_id', 'item_id', 'date_block_num']\n",
    "    ).agg({\n",
    "        'item_cnt_day': 'sum',\n",
    "        'item_price': 'mean',\n",
    "        'Return': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    full_grid = full_grid.merge(monthly_data, on=['shop_id', 'item_id', 'date_block_num'], how='left')\n",
    "    full_grid['item_cnt_day'] = full_grid['item_cnt_day'].fillna(0)\n",
    "    full_grid['Return'] = full_grid['Return'].fillna(0)\n",
    "    full_grid['item_price'] = full_grid['item_price'].fillna(cleaned_df_copy['item_price'].median())\n",
    "\n",
    "    full_grid.rename(columns={'item_cnt_day': 'item_cnt_month'}, inplace=True)\n",
    "    full_grid = full_grid.sort_values(['shop_id', 'item_id', 'date_block_num'])\n",
    "\n",
    "    # Add representative date for each date_block_num\n",
    "    # Assume 'date' is a datetime column; take the first date for each date_block_num\n",
    "    date_mapping = filtered_df.groupby('date_block_num')['date'].min().reset_index()\n",
    "    full_grid = full_grid.merge(date_mapping, on='date_block_num', how='left')\n",
    "\n",
    "    print(f\"Created base grid with {len(full_grid):,} shop-item-month combinations\")\n",
    "    return full_grid, filtered_df\n",
    "\n",
    "# Recreate grid and get filtered data\n",
    "sales_grid, filtered_df = create_sales_grid(cleaned_data)\n",
    "\n",
    "# Merge item-related metadata\n",
    "items_df = cleaned_data[['item_id', 'item_name', 'item_category_id']].drop_duplicates(subset=['item_id'])\n",
    "sales_grid = sales_grid.merge(items_df, on='item_id', how='left')\n",
    "\n",
    "# Merge item category name\n",
    "item_categories_df = cleaned_data[['item_category_id', 'item_category_name']].drop_duplicates(subset=['item_category_id'])\n",
    "sales_grid = sales_grid.merge(item_categories_df, on='item_category_id', how='left')\n",
    "\n",
    "# Merge shop name\n",
    "shops_df = cleaned_data[['shop_id', 'shop_name']].drop_duplicates(subset=['shop_id'])\n",
    "sales_grid = sales_grid.merge(shops_df, on='shop_id', how='left')\n",
    "\n",
    "# Check missing values\n",
    "print(\"Missing after fix:\\n\", sales_grid[['date', 'item_name', 'item_category_id', 'item_category_name', 'shop_name']].isna().sum())\n",
    "\n",
    "# Rename to merged_df for consistency\n",
    "merged_df = sales_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "175639bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shop_id                  51\n",
       "item_id               17313\n",
       "date_block_num           34\n",
       "item_cnt_month         6872\n",
       "item_price            39712\n",
       "Return                   12\n",
       "date                     34\n",
       "item_name             16837\n",
       "item_category_id         74\n",
       "item_category_name       73\n",
       "shop_name                51\n",
       "dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cc486a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aim for 58,823 to 85,294 shop-item pairs\n",
      "\n",
      "Filtered Data Shape: (2449938, 11)\n",
      "Unique pairs: 72,057\n",
      "Avg months per pair: 34.0\n",
      "Data types:\n",
      " shop_id       int64\n",
      "item_id       int64\n",
      "item_name    object\n",
      "dtype: object\n",
      "Sample data:\n",
      "          date  shop_id  item_id                       item_name  \\\n",
      "68 2013-01-01        2       31  007: SKIPHALL COORDINATES (BD)   \n",
      "69 2013-02-01        2       31  007: SKIPHALL COORDINATES (BD)   \n",
      "70 2013-03-01        2       31  007: SKIPHALL COORDINATES (BD)   \n",
      "71 2013-04-01        2       31  007: SKIPHALL COORDINATES (BD)   \n",
      "72 2013-05-01        2       31  007: SKIPHALL COORDINATES (BD)   \n",
      "\n",
      "    item_cnt_month  \n",
      "68             0.0  \n",
      "69             4.0  \n",
      "70             1.0  \n",
      "71             1.0  \n",
      "72             0.0  \n",
      "Category distribution:\n",
      " item_category_id\n",
      "40    645354\n",
      "55    354348\n",
      "37    276658\n",
      "30    114342\n",
      "19    110704\n",
      "23     83232\n",
      "38     57936\n",
      "63     53754\n",
      "41     51272\n",
      "72     50728\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "min_rows = 2_000_000\n",
    "max_rows = 2_900_000\n",
    "\n",
    "# Calculate average months per pair\n",
    "avg_months_per_pair = merged_df.groupby(['shop_id', 'item_id']).size().mean()\n",
    "min_pairs = int(min_rows / avg_months_per_pair)\n",
    "max_pairs = int(max_rows / avg_months_per_pair)\n",
    "print(f\"Aim for {min_pairs:,} to {max_pairs:,} shop-item pairs\")\n",
    "\n",
    "# Score pairs with balanced criteria\n",
    "pair_scores = (\n",
    "    merged_df.groupby(['shop_id', 'item_id'])\n",
    "    .agg(\n",
    "        total_sales=('item_cnt_month', 'sum'),  # Total sales volume\n",
    "        sales_activity=('item_cnt_month', lambda x: (x > 0).sum()),  # Months with non-zero sales\n",
    "        recency=('date_block_num', 'max'),  # Most recent month\n",
    "        category=('item_category_id', 'first')  # For stratification\n",
    "    )\n",
    "    .assign(\n",
    "        score=lambda x: (\n",
    "            0.4 * (x['total_sales'] / (x['total_sales'].max() + 1)) +  # Normalize sales\n",
    "            0.4 * (x['sales_activity'] / (x['sales_activity'].max() + 1)) +  # Normalize activity\n",
    "            0.2 * ((x['recency'] - merged_df['date_block_num'].min()) /\n",
    "                   (merged_df['date_block_num'].max() - merged_df['date_block_num'].min() + 1))  # Normalize recency\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Stratified sampling: Select top pairs per item_category_id to ensure diversity\n",
    "def stratified_sample(df, n_pairs, group_col='category'):\n",
    "    # Target pairs per category, proportional to category size\n",
    "    category_counts = df[group_col].value_counts()\n",
    "    total_pairs = min(max_pairs, max(min_pairs, n_pairs))\n",
    "    category_proportions = category_counts / category_counts.sum()\n",
    "    category_pairs = (category_proportions * total_pairs).round().astype(int)\n",
    "    \n",
    "    selected_pairs = []\n",
    "    for cat, n in category_pairs.items():\n",
    "        cat_pairs = df[df[group_col] == cat].nlargest(n, 'score').index\n",
    "        selected_pairs.extend(cat_pairs)\n",
    "    \n",
    "    # If under target, add more from top scores\n",
    "    if len(selected_pairs) < min_pairs:\n",
    "        remaining = min_pairs - len(selected_pairs)\n",
    "        extra_pairs = df[~df.index.isin(selected_pairs)].nlargest(remaining, 'score').index\n",
    "        selected_pairs.extend(extra_pairs)\n",
    "    # If over target, trim\n",
    "    selected_pairs = selected_pairs[:max_pairs]\n",
    "    return selected_pairs\n",
    "\n",
    "# Select pairs (aim for middle of range, e.g., ~2.45M rows)\n",
    "target_pairs = int((min_pairs + max_pairs) / 2)\n",
    "selected_pairs = stratified_sample(pair_scores, target_pairs, group_col='category')\n",
    "\n",
    "# Apply filter\n",
    "filtered_data = merged_df[\n",
    "    merged_df.set_index(['shop_id', 'item_id']).index.isin(selected_pairs)\n",
    "]\n",
    "\n",
    "# 4. Verify\n",
    "print(f\"\\nFiltered Data Shape: {filtered_data.shape}\")\n",
    "print(f\"Unique pairs: {filtered_data[['shop_id', 'item_id']].drop_duplicates().shape[0]:,}\")\n",
    "print(f\"Avg months per pair: {filtered_data.groupby(['shop_id', 'item_id']).size().mean():.1f}\")\n",
    "print(\"Data types:\\n\", filtered_data[['shop_id', 'item_id', 'item_name']].dtypes)\n",
    "print(\"Sample data:\\n\", filtered_data[['date', 'shop_id', 'item_id', 'item_name', 'item_cnt_month']].head())\n",
    "print(\"Category distribution:\\n\", filtered_data['item_category_id'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ca3d2fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shop_id                  51\n",
       "item_id                8908\n",
       "date_block_num           34\n",
       "item_cnt_month         4973\n",
       "item_price            27526\n",
       "Return                    9\n",
       "date                     34\n",
       "item_name              8686\n",
       "item_category_id         71\n",
       "item_category_name       70\n",
       "shop_name                51\n",
       "dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b0cbfa1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8686"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data['item_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "88413831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineering on filtered_data with 2,449,938 rows\n",
      "Created lag features: item_cnt_month_lag_1, item_cnt_month_lag_2, item_cnt_month_lag_3\n",
      "Imputed 72,057 NaN values in 'item_cnt_month_lag_1' with mean value 0.8498\n",
      "Imputed 144,114 NaN values in 'item_cnt_month_lag_2' with mean value 0.8620\n",
      "Imputed 216,171 NaN values in 'item_cnt_month_lag_3' with mean value 0.8762\n",
      "\n",
      "Missing values after imputation:\n",
      "item_cnt_month_lag_1    0\n",
      "item_cnt_month_lag_2    0\n",
      "item_cnt_month_lag_3    0\n",
      "dtype: int64\n",
      "\n",
      "Sample of engineered DataFrame with lag features:\n",
      "    shop_id  item_id  date_block_num  item_cnt_month  item_cnt_month_lag_1  \\\n",
      "68        2       31               0             0.0              0.849805   \n",
      "69        2       31               1             4.0              0.000000   \n",
      "70        2       31               2             1.0              4.000000   \n",
      "71        2       31               3             1.0              1.000000   \n",
      "72        2       31               4             0.0              1.000000   \n",
      "73        2       31               5             0.0              0.000000   \n",
      "74        2       31               6             0.0              0.000000   \n",
      "75        2       31               7             0.0              0.000000   \n",
      "76        2       31               8             0.0              0.000000   \n",
      "77        2       31               9             0.0              0.000000   \n",
      "\n",
      "    item_cnt_month_lag_2  item_cnt_month_lag_3  \n",
      "68               0.86198              0.876203  \n",
      "69               0.86198              0.876203  \n",
      "70               0.00000              0.876203  \n",
      "71               4.00000              0.000000  \n",
      "72               1.00000              4.000000  \n",
      "73               1.00000              1.000000  \n",
      "74               0.00000              1.000000  \n",
      "75               0.00000              0.000000  \n",
      "76               0.00000              0.000000  \n",
      "77               0.00000              0.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_lag_features(df, group_cols, value_cols, lags=[1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Create lag features for specified value columns grouped by given columns.\n",
    "    Lag features will have NaN for initial periods where lag does not exist.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(group_cols + ['date_block_num']).copy()\n",
    "    for col in value_cols:\n",
    "        for lag in lags:\n",
    "            lag_col = f\"{col}_lag_{lag}\"\n",
    "            # Shift values by lag within group\n",
    "            df[lag_col] = df.groupby(group_cols)[col].shift(lag)\n",
    "    return df\n",
    "\n",
    "def impute_nans_with_median_then_mean(df, cols):\n",
    "    \"\"\"\n",
    "    Impute NaNs in specified columns with median.\n",
    "    If median is zero, then try mean.\n",
    "    If mean is also zero, fill with 0.\n",
    "    Returns the DataFrame and dictionary of imputation values.\n",
    "    \"\"\"\n",
    "    imputation_values = {}\n",
    "    for col in cols:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            median_val = df[col].median()\n",
    "            if median_val != 0:\n",
    "                fill_val = median_val\n",
    "                method = 'median'\n",
    "            else:\n",
    "                mean_val = df[col].mean()\n",
    "                if mean_val != 0:\n",
    "                    fill_val = mean_val\n",
    "                    method = 'mean'\n",
    "                else:\n",
    "                    fill_val = 0\n",
    "                    method = 'zero'\n",
    "            df[col] = df[col].fillna(fill_val).astype(np.float32)\n",
    "            imputation_values[col] = (fill_val, method)\n",
    "            print(f\"Imputed {nan_count:,} NaN values in '{col}' with {method} value {fill_val:.4f}\")\n",
    "        else:\n",
    "            imputation_values[col] = (None, None)\n",
    "    return df, imputation_values\n",
    "\n",
    "# Assuming filtered_data is your input DataFrame\n",
    "# filtered_data = pd.read_csv('your_data.csv') or defined elsewhere\n",
    "\n",
    "working_df = filtered_data.copy()\n",
    "\n",
    "print(f\"Starting feature engineering on filtered_data with {len(working_df):,} rows\")\n",
    "\n",
    "group_cols = ['shop_id', 'item_id']\n",
    "value_cols = ['item_cnt_month']  # Only lag this column\n",
    "lags = [1, 2, 3]\n",
    "\n",
    "engineered_df = create_lag_features(working_df, group_cols, value_cols, lags)\n",
    "\n",
    "lag_features = [f\"{col}_lag_{lag}\" for col in value_cols for lag in lags]\n",
    "print(f\"Created lag features: {', '.join(lag_features)}\")\n",
    "\n",
    "engineered_df, imputation_values = impute_nans_with_median_then_mean(engineered_df, lag_features)\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(engineered_df[lag_features].isna().sum())\n",
    "\n",
    "print(\"\\nSample of engineered DataFrame with lag features:\")\n",
    "print(engineered_df[['shop_id', 'item_id', 'date_block_num', 'item_cnt_month'] + lag_features].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3ea283fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled dataframe sample:\n",
      "    shop_id  item_id  date_block_num  item_cnt_month  item_price  Return  \\\n",
      "68        2       31               0             0.0         0.0     0.0   \n",
      "69        2       31               1             4.0       300.0     0.0   \n",
      "70        2       31               2             1.0       299.5     0.0   \n",
      "71        2       31               3             1.0       300.0     0.0   \n",
      "72        2       31               4             0.0         0.0     0.0   \n",
      "\n",
      "         date  item_name  item_category_id  item_category_name  shop_name  \\\n",
      "68 2013-01-01         72                37                  15          5   \n",
      "69 2013-02-01         72                37                  15          5   \n",
      "70 2013-03-01         72                37                  15          5   \n",
      "71 2013-04-01         72                37                  15          5   \n",
      "72 2013-05-01         72                37                  15          5   \n",
      "\n",
      "    item_cnt_month_lag_1  item_cnt_month_lag_2  item_cnt_month_lag_3  year  \n",
      "68              0.849805               0.86198              0.876203  2013  \n",
      "69              0.000000               0.86198              0.876203  2013  \n",
      "70              4.000000               0.00000              0.876203  2013  \n",
      "71              1.000000               4.00000              0.000000  2013  \n",
      "72              1.000000               1.00000              4.000000  2013  \n",
      "\n",
      "Scaled dataframe dtypes:\n",
      "shop_id                          int64\n",
      "item_id                          int64\n",
      "date_block_num                   int64\n",
      "item_cnt_month                 float32\n",
      "item_price                     float32\n",
      "Return                         float32\n",
      "date                    datetime64[ns]\n",
      "item_name                        int32\n",
      "item_category_id                 int32\n",
      "item_category_name               int32\n",
      "shop_name                        int32\n",
      "item_cnt_month_lag_1           float32\n",
      "item_cnt_month_lag_2           float32\n",
      "item_cnt_month_lag_3           float32\n",
      "year                             int16\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Copy engineered_df\n",
    "scaled_df = engineered_df.copy()\n",
    "\n",
    "# Step 2: Encode categorical features as category codes (for entity embeddings later)\n",
    "categorical_cols = ['item_name', 'item_category_name', 'shop_name']\n",
    "for col in categorical_cols:\n",
    "    scaled_df[col] = scaled_df[col].astype('category').cat.codes.astype('int32')\n",
    "\n",
    "# Keep item_category_id as int32 (assuming numeric categorical ID)\n",
    "scaled_df['item_category_id'] = scaled_df['item_category_id'].astype('int32')\n",
    "\n",
    "# Step 3: Identify numerical columns for scaling\n",
    "numerical_cols = [\n",
    "    'item_cnt_month', 'item_price', 'Return',\n",
    "    'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3'\n",
    "]\n",
    "\n",
    "# Step 4: Apply RobustScaler to numerical columns\n",
    "scaler = RobustScaler()\n",
    "scaled_values = scaler.fit_transform(scaled_df[numerical_cols])\n",
    "scaled_df[numerical_cols] = scaled_values.astype(np.float32)\n",
    "\n",
    "# Step 5: Extract year from date (date column remains unchanged)\n",
    "scaled_df['year'] = scaled_df['date'].dt.year.astype('int16')\n",
    "\n",
    "# Step 6: Save date lookup for post-processing\n",
    "dates_for_lookup = scaled_df[['shop_id', 'item_id', 'date_block_num', 'date']].copy()\n",
    "dates_for_lookup.to_csv(\"date_lookup.csv\", index=False)\n",
    "\n",
    "# Step 7: Confirm\n",
    "print(\"Scaled dataframe sample:\")\n",
    "print(scaled_df.head())\n",
    "print(\"\\nScaled dataframe dtypes:\")\n",
    "print(scaled_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b4842c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2449938, 15)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "758c9c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1441140 sequences with encoder length 10 and decoder length 5\n",
      "X shape: (1441140, 10, 6), y shape: (1441140, 5, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_encoder_decoder_sequences(df, group_cols, feature_cols, encoder_seq_len=10, decoder_seq_len=5):\n",
    "    \n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    # Sort the dataframe to ensure correct time order\n",
    "    df = df.sort_values(group_cols + ['date_block_num']).reset_index(drop=True)\n",
    "    \n",
    "    # Group by shop_id and item_id\n",
    "    grouped = df.groupby(group_cols)\n",
    "    \n",
    "    for _, group in grouped:\n",
    "        group = group.reset_index(drop=True)\n",
    "        total_time_steps = len(group)\n",
    "        \n",
    "        # Number of sequences we can create from this group\n",
    "        max_start_idx = total_time_steps - encoder_seq_len - decoder_seq_len + 1\n",
    "        \n",
    "        for start_idx in range(max_start_idx):\n",
    "            encoder_seq = group.loc[start_idx : start_idx + encoder_seq_len - 1, feature_cols].values\n",
    "            decoder_seq = group.loc[start_idx + encoder_seq_len : start_idx + encoder_seq_len + decoder_seq_len - 1, feature_cols].values\n",
    "            \n",
    "            X_list.append(encoder_seq)\n",
    "            y_list.append(decoder_seq)\n",
    "    \n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    print(f\"Created {len(X)} sequences with encoder length {encoder_seq_len} and decoder length {decoder_seq_len}\")\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "group_cols = ['shop_id', 'item_id']\n",
    "feature_cols = ['item_cnt_month', 'item_price', 'Return', 'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3']\n",
    "encoder_seq_len = 10  # past 10 months\n",
    "decoder_seq_len = 5   # predict next 5 months\n",
    "\n",
    "X, y = create_encoder_decoder_sequences(scaled_df, group_cols, feature_cols, encoder_seq_len, decoder_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4688812e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 1313865 sequences\n",
      "Validation set: 84885 sequences\n",
      "Test set: 42390 sequences\n",
      "Saved all datasets as parquet files.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Your temporal split function\n",
    "def temporal_train_val_test_split(X, y, sequence_start_dates, val_months=2, test_months=1, total_months=34):\n",
    "    val_start = total_months - val_months - test_months\n",
    "    test_start = total_months - test_months\n",
    "    \n",
    "    sequence_start_dates = np.array(sequence_start_dates)\n",
    "    \n",
    "    train_mask = sequence_start_dates < val_start\n",
    "    val_mask = (sequence_start_dates >= val_start) & (sequence_start_dates < test_start)\n",
    "    test_mask = sequence_start_dates >= test_start\n",
    "    \n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "    \n",
    "    print(f\"Train set: {X_train.shape[0]} sequences\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} sequences\")\n",
    "    print(f\"Test set: {X_test.shape[0]} sequences\")\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "sequence_start_dates = np.random.randint(0, 34, size=X.shape[0])\n",
    "\n",
    "# Step 1: Split data temporally\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = temporal_train_val_test_split(\n",
    "    X, y, sequence_start_dates, val_months=2, test_months=1, total_months=34\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def flatten_sequences(arr, prefix):\n",
    "    n_samples, n_timesteps, n_features = arr.shape\n",
    "    # Reshape to (samples, timesteps*features)\n",
    "    arr_flat = arr.reshape(n_samples, n_timesteps * n_features)\n",
    "    # Create columns names: feat_0_t0, feat_1_t0, ..., feat_5_t9 (assuming 6 features, 10 timesteps)\n",
    "    cols = []\n",
    "    for t in range(n_timesteps):\n",
    "        for f in range(n_features):\n",
    "            cols.append(f\"{prefix}_t{t}_f{f}\")\n",
    "    df = pd.DataFrame(arr_flat, columns=cols)\n",
    "    return df\n",
    "\n",
    "# Flatten\n",
    "X_train_df = flatten_sequences(X_train, \"X\")\n",
    "y_train_df = flatten_sequences(y_train, \"y\")\n",
    "X_val_df = flatten_sequences(X_val, \"X\")\n",
    "y_val_df = flatten_sequences(y_val, \"y\")\n",
    "X_test_df = flatten_sequences(X_test, \"X\")\n",
    "y_test_df = flatten_sequences(y_test, \"y\")\n",
    "\n",
    "# Step 3: Save each split as separate parquet files\n",
    "X_train_df.to_parquet(\"X_train.parquet\", index=False)\n",
    "y_train_df.to_parquet(\"y_train.parquet\", index=False)\n",
    "\n",
    "X_val_df.to_parquet(\"X_val.parquet\", index=False)\n",
    "y_val_df.to_parquet(\"y_val.parquet\", index=False)\n",
    "\n",
    "X_test_df.to_parquet(\"X_test.parquet\", index=False)\n",
    "y_test_df.to_parquet(\"y_test.parquet\", index=False)\n",
    "\n",
    "print(\"Saved all datasets as parquet files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0eac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "class SalesDataset(Dataset):\n",
    "    def __init__(self, X_file, y_file, target_col='target', sequence_length=12, num_shops=51, num_items=8000, num_categories=74):\n",
    "        self.X = pl.read_parquet(X_file)\n",
    "        self.y = pl.read_parquet(y_file).select([target_col]).to_numpy().flatten().astype(np.float32)\n",
    "        self.dates = self.X['date'].to_numpy()  # For interpretability\n",
    "        if len(self.X) != len(self.y):\n",
    "            raise ValueError(\"X and y length mismatch\")\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.numerical_cols = ['date_block_num', 'returns', 'lag_1', 'lag_2', 'lag_3', 'shop_mean_lag_1', 'category_mean_lag_1']\n",
    "        self.categorical_cols = ['shop_id_encoded', 'item_id_encoded', 'item_category_id_encoded']\n",
    "        \n",
    "        # Normalize numerical data\n",
    "        numerical_data = self.X.select(self.numerical_cols).to_numpy().astype(np.float32)\n",
    "        numerical_data = np.clip(numerical_data, -1e5, 1e5)\n",
    "        mean = numerical_data.mean(axis=0, keepdims=True)\n",
    "        std = numerical_data.std(axis=0, keepdims=True) + 1e-6\n",
    "        self.numerical = (numerical_data - mean) / std\n",
    "        \n",
    "        # Normalize target\n",
    "        self.y = np.clip(self.y, -1e5, 1e5)\n",
    "        self.y_mean = 0.5224\n",
    "        self.y_std = self.y.std() + 1e-6\n",
    "        self.y = (self.y - self.y_mean) / self.y_std\n",
    "        \n",
    "        # Categorical data\n",
    "        self.shop_ids = self.X['shop_id_encoded'].to_numpy().astype(np.int64).clip(0, num_shops - 1)\n",
    "        self.item_ids = self.X['item_id_encoded'].to_numpy().astype(np.int64).clip(0, num_items - 1)\n",
    "        self.category_ids = self.X['item_category_id_encoded'].to_numpy().astype(np.int64).clip(0, num_categories - 1)\n",
    "        self.date_block_num = self.X['date_block_num'].to_numpy().astype(np.int32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.sequence_length + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx\n",
    "        end_idx = idx + self.sequence_length\n",
    "        numerical = torch.tensor(self.numerical[start_idx:end_idx], dtype=torch.float32)\n",
    "        shop_ids = torch.tensor(self.shop_ids[start_idx:end_idx], dtype=torch.int64)\n",
    "        item_ids = torch.tensor(self.item_ids[start_idx:end_idx], dtype=torch.int64)\n",
    "        category_ids = torch.tensor(self.category_ids[start_idx:end_idx], dtype=torch.int64)\n",
    "        target = torch.tensor(self.y[end_idx - 1], dtype=torch.float32)\n",
    "        identifiers = torch.tensor([self.shop_ids[end_idx - 1], self.item_ids[end_idx - 1], self.date_block_num[end_idx - 1]], dtype=torch.int32)\n",
    "        dates = self.dates[start_idx:end_idx]\n",
    "        return {\n",
    "            'numerical': numerical, 'shop_ids': shop_ids, 'item_ids': item_ids, 'category_ids': category_ids,\n",
    "            'target': target, 'identifiers': identifiers, 'dates': dates\n",
    "        }\n",
    "\n",
    "class HALSTM(nn.Module):\n",
    "    def __init__(self, num_shops=51, num_items=8000, num_categories=74, embed_dim=16, numerical_dim=7,\n",
    "                 hidden_dim=128, num_layers=2, num_heads=4, dropout=0.35):\n",
    "        super(HALSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.shop_embed = nn.Embedding(num_shops, embed_dim)\n",
    "        self.item_embed = nn.Embedding(num_items, embed_dim)\n",
    "        self.category_embed = nn.Embedding(num_categories, embed_dim)\n",
    "        nn.init.normal_(self.shop_embed.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.item_embed.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.category_embed.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        self.input_dim = numerical_dim + embed_dim * 3\n",
    "        self.lstm = nn.LSTM(self.input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.lstm_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.mha = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.mha_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.gate = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc_shared = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.positional_encoding = torch.zeros(100, hidden_dim).to(device)\n",
    "        position = torch.arange(0, 100, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / hidden_dim))\n",
    "        self.positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "    def forward(self, numerical, shop_ids, item_ids, category_ids):\n",
    "        batch_size, seq_len, _ = numerical.size()\n",
    "        shop_embed = self.shop_embed(shop_ids)\n",
    "        item_embed = self.item_embed(item_ids)\n",
    "        category_embed = self.category_embed(category_ids)\n",
    "        \n",
    "        x = torch.cat([numerical, shop_embed, item_embed, category_embed], dim=-1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        lstm_out = self.lstm_norm(lstm_out)\n",
    "        \n",
    "        lstm_out = lstm_out + self.positional_encoding[:seq_len, :].unsqueeze(0)\n",
    "        \n",
    "        mha_out, mha_weights = self.mha(lstm_out, lstm_out, lstm_out)\n",
    "        mha_out = self.mha_norm(mha_out)\n",
    "        \n",
    "        combined = torch.cat([lstm_out[:, -1, :], mha_out[:, -1, :]], dim=-1)\n",
    "        gate = self.sigmoid(self.gate(combined))\n",
    "        fused = gate * lstm_out[:, -1, :] + (1 - gate) * mha_out[:, -1, :]\n",
    "        \n",
    "        shared = self.relu(self.fc_shared(fused))\n",
    "        output = self.fc_out(shared)\n",
    "        \n",
    "        return output.squeeze(-1), {'mha_weights': mha_weights, 'gate_weights': gate}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    if not batch:\n",
    "        return {}\n",
    "    return {\n",
    "        'numerical': torch.stack([item['numerical'] for item in batch]),\n",
    "        'shop_ids': torch.stack([item['shop_ids'] for item in batch]),\n",
    "        'item_ids': torch.stack([item['item_ids'] for item in batch]),\n",
    "        'category_ids': torch.stack([item['category_ids'] for item in batch]),\n",
    "        'target': torch.stack([item['target'] for item in batch]),\n",
    "        'identifiers': torch.stack([item['identifiers'] for item in batch]),\n",
    "        'dates': [item['dates'] for item in batch]\n",
    "    }\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, lr=0.001, accum_steps=2):\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=num_epochs, steps_per_epoch=len(train_loader)//accum_steps)\n",
    "    \n",
    "    metrics = {'epoch': [], 'train_mse': [], 'train_rmse': [], 'val_mse': [], 'val_rmse': []}\n",
    "    best_val_loss = float('inf')\n",
    "    output_dir = Path('/workspace/results')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_mse = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "            numerical = batch['numerical'].to(device)\n",
    "            shop_ids = batch['shop_ids'].to(device)\n",
    "            item_ids = batch['item_ids'].to(device)\n",
    "            category_ids = batch['category_ids'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                output, _ = model(numerical, shop_ids, item_ids, category_ids)\n",
    "                mse_loss = criterion(output, target) / accum_steps\n",
    "                loss = mse_loss\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            if (batch_idx + 1) % accum_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                scaler.step(optimizer)\n",
    "                scheduler.step()\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += mse_loss.item() * accum_steps\n",
    "            train_mse += mse_loss.item() * accum_steps\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_mse /= len(train_loader)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_mse = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                numerical = batch['numerical'].to(device)\n",
    "                shop_ids = batch['shop_ids'].to(device)\n",
    "                item_ids = batch['item_ids'].to(device)\n",
    "                category_ids = batch['category_ids'].to(device)\n",
    "                target = batch['target'].to(device)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output, _ = model(numerical, shop_ids, item_ids, category_ids)\n",
    "                    loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_mse += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_mse /= len(val_loader)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        \n",
    "        metrics['epoch'].append(epoch + 1)\n",
    "        metrics['train_mse'].append(train_mse)\n",
    "        metrics['train_rmse'].append(train_rmse)\n",
    "        metrics['val_mse'].append(val_mse)\n",
    "        metrics['val_rmse'].append(val_rmse)\n",
    "        print(f\"Epoch {epoch+1}, Train MSE: {train_mse:.4f}, RMSE: {train_rmse:.4f}, Val MSE: {val_mse:.4f}, RMSE: {val_rmse:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), output_dir / 'best_ha_lstm.pth')\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_df.to_csv(output_dir / 'training_metrics.csv', index=False)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(metrics['epoch'], metrics['val_rmse'], label='Val RMSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('Validation RMSE')\n",
    "    plt.legend()\n",
    "    plt.savefig(output_dir / 'rmse_plot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    model.load_state_dict(torch.load(output_dir / 'best_ha_lstm.pth'))\n",
    "    return model, metrics_df\n",
    "\n",
    "def predict(model, test_loader, dataset):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    interpret_outputs = []\n",
    "    modalities = ['numerical', 'shop_id', 'item_id', 'item_category_id']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            numerical = batch['numerical'].to(device)\n",
    "            shop_ids = batch['shop_ids'].to(device)\n",
    "            item_ids = batch['item_ids'].to(device)\n",
    "            category_ids = batch['category_ids'].to(device)\n",
    "            identifiers = batch['identifiers']\n",
    "            dates = batch['dates']\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                output, attn_dict = model(numerical, shop_ids, item_ids, category_ids)\n",
    "                preds = output.cpu().numpy() * dataset.y_std + dataset.y_mean\n",
    "            \n",
    "            mha_weights = attn_dict['mha_weights'][:, -1, :].cpu().numpy()\n",
    "            gate_weights = attn_dict['gate_weights'].cpu().numpy()\n",
    "            \n",
    "            for i in range(len(preds)):\n",
    "                predictions.append({\n",
    "                    'shop_id': identifiers[i][0].item(),\n",
    "                    'item_id': identifiers[i][1].item(),\n",
    "                    'date_block_num': identifiers[i][2].item(),\n",
    "                    'forecast_h1': preds[i]\n",
    "                })\n",
    "                interpret_outputs.append({\n",
    "                    'timestamp_reference': dates[i][-1].isoformat(),\n",
    "                    'forecasted_value': float(preds[i]),\n",
    "                    'fusion_weights': gate_weights[i].tolist(),\n",
    "                    'attention_weights': mha_weights[i].tolist(),\n",
    "                    'input_sequence_dates': [d.isoformat() for d in dates[i]],\n",
    "                    'modalities_used': modalities,\n",
    "                    'gating_decision_output': gate_weights[i].tolist()\n",
    "                })\n",
    "    \n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    interpret_pred_df = pd.DataFrame(interpret_outputs(pred_df))\n",
    "    \n",
    "    output_dir = Path('/workspace/results')\n",
    "    pred_df.to_csv(output_dir / 'predictions.csv', index=False)\n",
    "    interpret_df.to_csv(output_dir / 'interpretability_outputs.csv', index=False)\n",
    "    \n",
    "    return pred_df, interpret_df\n",
    "\n",
    "def visualize_results(pred_df):\n",
    "    output_dir = Path('/workspace/results')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.kdeplot(pred_df['forecast_h1'], label='Horizon 1')\n",
    "    plt.title('Prediction Distribution')\n",
    "    plt.xlabel('Predicted Sales')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.savefig(output_dir / 'prediction_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    data_dir = Path('/workspace/data')\n",
    "    train_X_path = data_dir / 'X_train.parquet'\n",
    "    train_y_path = data_dir / 'y_train.parquet'\n",
    "    val_X_path = data_dir / 'X_val.parquet'\n",
    "    val_y_path = data_dir / 'y_val.parquet'\n",
    "    test_X_path = data_dir / 'X_test.parquet'\n",
    "    test_y_path = data_dir / 'y_test.parquet'\n",
    "    \n",
    "    batch_size = 4096\n",
    "    num_workers = 8\n",
    "    num_epochs = 50\n",
    "    lr = 0.001\n",
    "    accum_steps = 2\n",
    "    \n",
    "    # Create dataset objects\n",
    "    train_dataset = SalesDataset(train_X_path, train_y_path, target_col='target')\n",
    "    val_dataset = SalesDataset(val_X_path, val_y_path, target_col='target')\n",
    "    test_dataset = SalesDataset(test_X_path, test_y_path, target_col='target')\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # Create model object\n",
    "    model = HALSTM().to(device)\n",
    "    \n",
    "    # Train model\n",
    "    model, metrics_df = train_model(model, train_loader, val_loader, num_epochs, lr, accum_steps)\n",
    "    \n",
    "    # Predict and generate interpretability outputs\n",
    "    pred_df, interpret_df = predict(model, test_loader, test_dataset)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_results(pred_df)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f70423f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
