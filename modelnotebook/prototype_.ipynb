{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01845c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five rows of the data:\n",
      "         date  date_block_num  shop_id  item_id  item_price  item_cnt_day  \\\n",
      "0  02.01.2013               0       59    22154      999.00           1.0   \n",
      "1  03.01.2013               0       25     2552      899.00           1.0   \n",
      "2  05.01.2013               0       25     2552      899.00          -1.0   \n",
      "3  06.01.2013               0       25     2554     1709.05           1.0   \n",
      "4  15.01.2013               0       25     2555     1099.00           1.0   \n",
      "\n",
      "                                  item_name  item_category_id  \\\n",
      "0                    ANNOUNCEMENT 2012 (BD)                37   \n",
      "1  DEEP PURPLE  The House Of Blue Light  LP                58   \n",
      "2  DEEP PURPLE  The House Of Blue Light  LP                58   \n",
      "3  DEEP PURPLE  Who Do You Think We Are  LP                58   \n",
      "4            DEEP PURPE 30 Very Best Of 2CD                56   \n",
      "\n",
      "                 item_category_name            shop_name  \n",
      "0                 Cinema - Blue-Ray  Jaroslavl TC Altair  \n",
      "1                     Music - Vinyl    Moscow ARK Atrium  \n",
      "2                     Music - Vinyl    Moscow ARK Atrium  \n",
      "3                     Music - Vinyl    Moscow ARK Atrium  \n",
      "4  Music - a CD of brand production    Moscow ARK Atrium  \n",
      "\n",
      "Data Shape: \n",
      "(2935849, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_path = pd.read_csv(\"/Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/Predict Future Sales/merged_data.csv\")\n",
    "df = data_path\n",
    "\n",
    "print(\"First five rows of the data:\")\n",
    "print(df.head())\n",
    "print(\"\\nData Shape: \")\n",
    "print(df.shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa42e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6537eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset with 2,935,849 records\n",
      "Initial unique counts:\n",
      "item_id: 21807\n",
      "item_name: 21233\n",
      "item_name nulls before imputation: 84\n",
      "item_id with 'Unknown' item_name: 0\n",
      "Unique item_name per item_id:\n",
      "item_name\n",
      "1    21806\n",
      "0        1\n",
      "Name: count, dtype: int64\n",
      "Initial null counts:\n",
      "Column 'date': 0 nulls (0.00%)\n",
      "Column 'date_block_num': 0 nulls (0.00%)\n",
      "Column 'shop_id': 0 nulls (0.00%)\n",
      "Column 'item_id': 0 nulls (0.00%)\n",
      "Column 'item_price': 0 nulls (0.00%)\n",
      "Column 'item_cnt_day': 0 nulls (0.00%)\n",
      "Column 'item_name': 84 nulls (0.00%)\n",
      "Column 'item_category_id': 0 nulls (0.00%)\n",
      "Column 'item_category_name': 0 nulls (0.00%)\n",
      "Column 'shop_name': 0 nulls (0.00%)\n",
      "Created Return column. Total Returns: 7,541.0\n",
      "Converted date column to datetime format\n",
      "Imputed 84 missing item names with 'Unknown'\n",
      "Selected 54 shops after removing ['36', '11', '20', '8', '9', '40']\n",
      "Removed 14,017 records from shop selection\n",
      "Winsorization applied at 99th percentile for item_cnt_day after 3σ rolling-window detection\n",
      "Total records: 2,921,832\n",
      "Outliers (above 3σ): 2,072 (0.07%)\n",
      "Values clipped (>99th percentile): 48,150 (1.65%)\n",
      "Final unique counts:\n",
      "item_id: 21600\n",
      "item_name: 21027\n"
     ]
    }
   ],
   "source": [
    "def data_preprocessing(df):\n",
    "    expected_columns = ['date', 'date_block_num', 'shop_id', 'item_id', 'item_price', 'item_cnt_day',\n",
    "                       'item_name', 'item_category_id', 'item_category_name', 'shop_name']\n",
    "    if not all(col in df.columns for col in expected_columns):\n",
    "        missing = set(expected_columns) - set(df.columns)\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "    \n",
    "    print(f\"Preprocessing dataset with {len(df):,} records\")\n",
    "    \n",
    "    cleaned_df = df.copy()\n",
    "    # Initial checks\n",
    "    print(\"Initial unique counts:\")\n",
    "    print(f\"item_id: {cleaned_df['item_id'].nunique()}\")\n",
    "    print(f\"item_name: {cleaned_df['item_name'].nunique()}\")\n",
    "    print(\"item_name nulls before imputation:\", cleaned_df['item_name'].isna().sum())\n",
    "    print(\"item_id with 'Unknown' item_name:\", (cleaned_df['item_name'] == 'Unknown').sum())\n",
    "    print(\"Unique item_name per item_id:\")\n",
    "    name_counts = cleaned_df.groupby('item_id')['item_name'].nunique()\n",
    "    print(name_counts.value_counts())\n",
    "    if (name_counts > 1).any():\n",
    "        print(\"item_id with multiple item_name:\", name_counts[name_counts > 1].index.tolist())\n",
    "\n",
    "    cleaned_df['item_price'] = cleaned_df['item_price'].astype(np.float32)\n",
    "    cleaned_df['item_cnt_day'] = cleaned_df['item_cnt_day'].astype(np.float32)\n",
    "    \n",
    "    print(\"Initial null counts:\")\n",
    "    for col in cleaned_df.columns:\n",
    "        nulls = cleaned_df[col].isna().sum()\n",
    "        print(f\"Column '{col}': {nulls:,} nulls ({nulls/len(cleaned_df)*100:.2f}%)\")\n",
    "    \n",
    "    cleaned_df['Return'] = cleaned_df['item_cnt_day'].where(cleaned_df['item_cnt_day'] < 0, 0).abs().astype(np.float32)\n",
    "    cleaned_df['item_cnt_day'] = cleaned_df['item_cnt_day'].clip(lower=0)\n",
    "    print(f\"Created Return column. Total Returns: {cleaned_df['Return'].sum():,}\")\n",
    "    \n",
    "    cleaned_df['date'] = pd.to_datetime(cleaned_df['date'], format='%d.%m.%Y')\n",
    "    print(\"Converted date column to datetime format\")\n",
    "    \n",
    "    # Handle item_name\n",
    "    if 'item_name' in cleaned_df.columns and cleaned_df['item_name'].isna().any():\n",
    "        item_name_nulls = cleaned_df['item_name'].isna().sum()\n",
    "        cleaned_df['item_name'] = cleaned_df['item_name'].fillna('Unknown')\n",
    "        print(f\"Imputed {item_name_nulls:,} missing item names with 'Unknown'\")\n",
    "    \n",
    "    # Ensure string types\n",
    "    cleaned_df['shop_id'] = cleaned_df['shop_id'].astype(str)\n",
    "    cleaned_df['item_id'] = cleaned_df['item_id'].astype(str)\n",
    "    cleaned_df['item_name'] = cleaned_df['item_name'].astype(str)\n",
    "    \n",
    "    # Fix multiple item_name per item_id\n",
    "    name_counts = cleaned_df.groupby('item_id')['item_name'].nunique()\n",
    "    if (name_counts > 1).any():\n",
    "        print(f\"Warning: {name_counts[name_counts > 1].count()} item_id(s) have multiple item_name values. Taking most frequent.\")\n",
    "        most_frequent = cleaned_df.groupby('item_id')['item_name'].agg(lambda x: x.mode()[0]).reset_index()\n",
    "        cleaned_df = cleaned_df.drop(columns='item_name').merge(most_frequent, on='item_id', how='left')\n",
    "    \n",
    "    shop_stats = cleaned_df.groupby('shop_id').size().reset_index(name='count')\n",
    "    shops_to_remove = shop_stats.nsmallest(6, 'count')['shop_id'].tolist()\n",
    "    remove_records = cleaned_df[cleaned_df['shop_id'].isin(shops_to_remove)].shape[0]\n",
    "    cleaned_df = cleaned_df[~cleaned_df['shop_id'].isin(shops_to_remove)]\n",
    "    print(f\"Selected {cleaned_df['shop_id'].nunique()} shops after removing {shops_to_remove}\")\n",
    "    print(f\"Removed {remove_records:,} records from shop selection\")\n",
    "    \n",
    "    cleaned_df = cleaned_df.sort_values(['shop_id', 'item_id', 'date'])\n",
    "    \n",
    "    def winsorize_with_rolling_stats(group):\n",
    "        group = group.set_index('date').sort_index()\n",
    "        rolling_mean = group['item_cnt_day'].rolling(window='30D', min_periods=1).mean()\n",
    "        rolling_std = group['item_cnt_day'].rolling(window='30D', min_periods=1).std()\n",
    "        upper_3sigma = rolling_mean + 3 * rolling_std\n",
    "        \n",
    "        outliers = group['item_cnt_day'] > upper_3sigma\n",
    "        outlier_count = outliers.sum()\n",
    "        \n",
    "        winsor_limit = group['item_cnt_day'].quantile(0.99)\n",
    "        clipped = group['item_cnt_day'].clip(upper=winsor_limit)\n",
    "        clipped_count = (group['item_cnt_day'] > winsor_limit).sum()\n",
    "        \n",
    "        return clipped.reset_index(drop=True), outlier_count, clipped_count\n",
    "        \n",
    "    total_records = len(cleaned_df)\n",
    "    item_cnt_day_winsorized = []\n",
    "    total_outlier = 0\n",
    "    total_clipped = 0\n",
    "    \n",
    "    for (shop_id, item_id), group in cleaned_df.groupby(['shop_id', 'item_id']):\n",
    "        clipped_series, outliers, clipped = winsorize_with_rolling_stats(group.copy())\n",
    "        item_cnt_day_winsorized.extend(clipped_series)\n",
    "        total_outlier += outliers\n",
    "        total_clipped += clipped\n",
    "        \n",
    "    cleaned_df['item_cnt_day'] = item_cnt_day_winsorized\n",
    "    \n",
    "    outlier_percentage = (total_outlier / total_records * 100) if total_records > 0 else 0\n",
    "    clipped_percentage = (total_clipped / total_records * 100) if total_records > 0 else 0\n",
    "\n",
    "    print(\"Winsorization applied at 99th percentile for item_cnt_day after 3σ rolling-window detection\")\n",
    "    print(f\"Total records: {total_records:,}\")\n",
    "    print(f\"Outliers (above 3σ): {total_outlier:,} ({outlier_percentage:.2f}%)\")\n",
    "    print(f\"Values clipped (>99th percentile): {total_clipped:,} ({clipped_percentage:.2f}%)\")\n",
    "    \n",
    "    # Final checks\n",
    "    print(\"Final unique counts:\")\n",
    "    print(f\"item_id: {cleaned_df['item_id'].nunique()}\")\n",
    "    print(f\"item_name: {cleaned_df['item_name'].nunique()}\")\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "cleaned_data = data_preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3737910b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'date_block_num',\n",
       " 'shop_id',\n",
       " 'item_id',\n",
       " 'item_price',\n",
       " 'item_cnt_day',\n",
       " 'item_name',\n",
       " 'item_category_id',\n",
       " 'item_category_name',\n",
       " 'shop_name',\n",
       " 'Return']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6528415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly data sample:\n",
      "   date_block_num shop_id item_id  item_category_id  item_cnt_month  \\\n",
      "0               0       0    1000                67             5.0   \n",
      "1               0       0    1001                67             2.0   \n",
      "2               0       0   10012                40             1.0   \n",
      "3               0       0    1002                67             2.0   \n",
      "4               0       0    1003                67             2.0   \n",
      "\n",
      "   item_price  Return  \n",
      "0        58.0     0.0  \n",
      "1        58.0     0.0  \n",
      "2        76.0     0.0  \n",
      "3        58.0     0.0  \n",
      "4        58.0     0.0  \n",
      "Monthly data shape: (1601409, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path # <-- 1. Import the Path object\n",
    "\n",
    "def aggregate_to_monthly(df_clean):\n",
    "    # Aggregate to monthly level\n",
    "    monthly_data = df_clean.groupby(['date_block_num', 'shop_id', 'item_id', 'item_category_id']).agg({\n",
    "        'item_cnt_day': 'sum',\n",
    "        'item_price': 'mean',\n",
    "        'Return': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    monthly_data.rename(columns={'item_cnt_day': 'item_cnt_month'}, inplace=True)\n",
    "    \n",
    "    # Clip item_cnt_month\n",
    "    monthly_data['item_cnt_month'] = monthly_data['item_cnt_month'].clip(0, 20).astype(np.float32)\n",
    "    \n",
    "    return monthly_data\n",
    "\n",
    "# Assuming df_clean is loaded\n",
    "monthly_data = aggregate_to_monthly(cleaned_data)\n",
    "\n",
    "# --- FIX STARTS HERE ---\n",
    "\n",
    "# 2. Define the output directory path\n",
    "output_dir = Path(\"lstm_data\")\n",
    "\n",
    "# 3. Create the directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 4. Save the file inside the now-guaranteed-to-exist directory\n",
    "monthly_data.to_parquet(output_dir / \"monthly_data.parquet\", index=False)\n",
    "\n",
    "# --- FIX ENDS HERE ---\n",
    "\n",
    "\n",
    "print(\"Monthly data sample:\")\n",
    "print(monthly_data.head())\n",
    "print(f\"Monthly data shape: {monthly_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cb616f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_name</th>\n",
       "      <th>item_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>3D Action Puzzle Zombie Cleaner</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10004</td>\n",
       "      <td>SM/F (region)</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>3D Action Puzzle \"Zomby\" Shahter</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10012</td>\n",
       "      <td>WOLT m/f (Region)</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002</td>\n",
       "      <td>3D Action Puzzle \"Technica\" Bomber</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  item_id                           item_name  item_price\n",
       "0    1000     3D Action Puzzle Zombie Cleaner        58.0\n",
       "1   10004                       SM/F (region)        64.0\n",
       "2    1001    3D Action Puzzle \"Zomby\" Shahter        58.0\n",
       "3   10012                   WOLT m/f (Region)        76.0\n",
       "4    1002  3D Action Puzzle \"Technica\" Bomber        58.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_df = cleaned_data[['item_id', 'item_name', 'item_price']].drop_duplicates().reset_index(drop=True)\n",
    "items_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05679faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea86d4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 14:44:59,107 - INFO - Creating and saving complete item metadata...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'items_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h7/zh8nh5wx1fb6smgs5s2whtkr0000gn/T/ipykernel_71243/4288468321.py\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 2. Link item names and category names together.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# This starts with your original items and categories data for a complete list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mitem_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_cats_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'item_category_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# 3. Get the average UN SCALED price for each item from your monthly aggregated data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'items_df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# --- SETUP (Assume these are already loaded in your script) ---\n",
    "# This setup is for demonstration. In your script, you would have already loaded these.\n",
    "# monthly_data = pd.read_parquet(\"lstm_data/monthly_data.parquet\")\n",
    "# items_df = pd.read_csv(\"raw_data/items.csv\")\n",
    "# item_cats_df = pd.read_csv(\"raw_data/item_categories.csv\")\n",
    "# logger = logging.getLogger(__name__)\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# --- START OF THE CODE TO ADD ---\n",
    "\n",
    "logger.info(\"Creating and saving complete item metadata...\")\n",
    "\n",
    "# 1. Define the absolute save directory as you specified.\n",
    "SAVE_DIR = Path(\"/Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/modelnotebook/training_data\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Link item names and category names together.\n",
    "# This starts with your original items and categories data for a complete list.\n",
    "item_metadata = pd.merge(items_df, item_cats_df, on='item_category_id', how='left')\n",
    "\n",
    "# 3. Get the average UN SCALED price for each item from your monthly aggregated data.\n",
    "# This ensures we capture the real-world price for revenue calculations.\n",
    "avg_price_per_item = monthly_data.groupby('item_id')['item_price'].mean().reset_index()\n",
    "\n",
    "# 4. Merge the average price into the metadata.\n",
    "# We use a left merge to ensure we keep all items, even if they had no sales.\n",
    "item_metadata = pd.merge(item_metadata, avg_price_per_item, on='item_id', how='left')\n",
    "\n",
    "# 5. Select and rename the final columns to match your requirements.\n",
    "# We fill any missing prices (for items that never sold) with 0.\n",
    "item_metadata = item_metadata[[\n",
    "    'item_id',\n",
    "    'item_name',\n",
    "    'item_category_id',\n",
    "    'item_category_name',\n",
    "    'item_price'\n",
    "]].fillna({'item_price': 0})\n",
    "\n",
    "# 6. Save the final, clean metadata file to the specified directory.\n",
    "try:\n",
    "    item_metadata.to_parquet(SAVE_DIR / 'item_metadata.parquet', index=False)\n",
    "    logger.info(f\"✓ Successfully saved item_metadata.parquet to: {SAVE_DIR.resolve()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"✗ Failed to save item_metadata.parquet: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- END OF THE CODE TO ADD ---\n",
    "\n",
    "# You can add a verification step to see the output.\n",
    "print(\"\\nSample of the created item_metadata.parquet:\")\n",
    "print(item_metadata.head())\n",
    "print(f\"\\nShape of item_metadata: {item_metadata.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a724ed50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date_block_num',\n",
       " 'shop_id',\n",
       " 'item_id',\n",
       " 'item_category_id',\n",
       " 'item_cnt_month',\n",
       " 'item_price',\n",
       " 'Return']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "monthly_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6988cbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineering on monthly_data with 1,601,409 rows\n",
      "Imputed 418,619 NaN values in 'item_cnt_month_lag_1' with median value 1.0000\n",
      "Imputed 694,941 NaN values in 'item_cnt_month_lag_2' with median value 1.0000\n",
      "Imputed 890,881 NaN values in 'item_cnt_month_lag_3' with median value 1.0000\n",
      "\n",
      "Item_price statistics in engineered_df:\n",
      "count    1.601409e+06\n",
      "mean     6.066872e+00\n",
      "std      9.899732e-01\n",
      "min      8.617770e-02\n",
      "25%      5.298317e+00\n",
      "50%      5.991465e+00\n",
      "75%      6.789816e+00\n",
      "max      8.517393e+00\n",
      "Name: item_price, dtype: float64\n",
      "\n",
      "Sample of engineered DataFrame:\n",
      "  shop_id item_id  date_block_num  item_category_id  item_cnt_month  Return  \\\n",
      "0       0    1000               0                67             5.0       0   \n",
      "1       0    1000               1                67             4.0       0   \n",
      "2       0   10004               1                40             1.0       0   \n",
      "3       0    1001               0                67             2.0       0   \n",
      "4       0   10012               0                40             1.0       0   \n",
      "5       0   10012               1                40             2.0       0   \n",
      "6       0    1002               0                67             2.0       0   \n",
      "7       0    1003               0                67             2.0       0   \n",
      "8       0   10033               1                55             1.0       0   \n",
      "9       0   10038               1                40             1.0       0   \n",
      "\n",
      "   item_price  item_cnt_month_lag_1  item_cnt_month_lag_2  \\\n",
      "0    4.077538                   1.0                   1.0   \n",
      "1    4.077538                   5.0                   1.0   \n",
      "2    4.174387                   1.0                   1.0   \n",
      "3    4.077538                   1.0                   1.0   \n",
      "4    4.343805                   1.0                   1.0   \n",
      "5    4.343805                   1.0                   1.0   \n",
      "6    4.077538                   1.0                   1.0   \n",
      "7    4.077538                   1.0                   1.0   \n",
      "8    4.709530                   1.0                   1.0   \n",
      "9    4.248495                   1.0                   1.0   \n",
      "\n",
      "   item_cnt_month_lag_3  item_cnt_month_mean_category  \\\n",
      "0                   1.0                      1.654111   \n",
      "1                   1.0                      1.664591   \n",
      "2                   1.0                      1.824594   \n",
      "3                   1.0                      1.654111   \n",
      "4                   1.0                      1.767606   \n",
      "5                   1.0                      1.824594   \n",
      "6                   1.0                      1.654111   \n",
      "7                   1.0                      1.654111   \n",
      "8                   1.0                      1.312519   \n",
      "9                   1.0                      1.824594   \n",
      "\n",
      "   item_cnt_month_mean_shop  item_cnt_month_mean_item  item_price  \n",
      "0                  2.263845                  2.058125    4.077538  \n",
      "1                  2.384922                  2.481667    4.077538  \n",
      "2                  2.384922                  1.076923    4.174387  \n",
      "3                  2.263845                  1.263158    4.077538  \n",
      "4                  2.263845                  1.071429    4.343805  \n",
      "5                  2.384922                  1.142857    4.343805  \n",
      "6                  2.263845                  1.832222    4.077538  \n",
      "7                  2.263845                  1.553125    4.077538  \n",
      "8                  2.384922                  1.000000    4.709530  \n",
      "9                  2.384922                  1.000000    4.248495  \n",
      "Engineered data shape: (1601409, 13)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_lag_features(df, group_cols, value_cols, lags=[1, 2, 3]):\n",
    "    df = df.sort_values(group_cols + ['date_block_num']).copy()\n",
    "    for col in value_cols:\n",
    "        for lag in lags:\n",
    "            lag_col = f\"{col}_lag_{lag}\"\n",
    "            df[lag_col] = df.groupby(group_cols)[col].shift(lag)\n",
    "    return df\n",
    "\n",
    "def impute_nans_with_median_then_mean(df, cols):\n",
    "    imputation_values = {}\n",
    "    for col in cols:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            median_val = df[col].median()\n",
    "            if median_val != 0:\n",
    "                fill_val = median_val\n",
    "                method = 'median'\n",
    "            else:\n",
    "                mean_val = df[col].mean()\n",
    "                if mean_val != 0:\n",
    "                    fill_val = mean_val\n",
    "                    method = 'mean'\n",
    "                else:\n",
    "                    fill_val = 0\n",
    "                    method = 'zero'\n",
    "            df[col] = df[col].fillna(fill_val).astype(np.float32)\n",
    "            imputation_values[col] = (fill_val, method)\n",
    "            print(f\"Imputed {nan_count:,} NaN values in '{col}' with {method} value {fill_val:.4f}\")\n",
    "        else:\n",
    "            imputation_values[col] = (None, None)\n",
    "    return df, imputation_values\n",
    "\n",
    "# ---- APPLY FEATURE ENGINEERING ----\n",
    "monthly_data = pd.read_parquet(\"lstm_data/monthly_data.parquet\")\n",
    "engineered_df = monthly_data.copy()\n",
    "\n",
    "print(f\"Starting feature engineering on monthly_data with {len(engineered_df):,} rows\")\n",
    "\n",
    "group_cols = ['shop_id', 'item_id']\n",
    "value_cols = ['item_cnt_month']\n",
    "lags = [1, 2, 3]\n",
    "\n",
    "# Clip item_cnt_month\n",
    "engineered_df['item_cnt_month'] = engineered_df['item_cnt_month'].clip(0, 20)\n",
    "\n",
    "# Make Return binary\n",
    "engineered_df['Return'] = (engineered_df['Return'] > 0).astype(np.int8)\n",
    "\n",
    "# Log-transform item_price with tighter clipping\n",
    "engineered_df['item_price'] = np.log1p(engineered_df['item_price'].clip(lower=0, upper=5000)).astype(np.float32)\n",
    "\n",
    "# Create lag features\n",
    "engineered_df = create_lag_features(engineered_df, group_cols, value_cols, lags)\n",
    "\n",
    "# Add mean encodings\n",
    "category_means = engineered_df.groupby(['item_category_id', 'date_block_num'])['item_cnt_month'].mean().reset_index()\n",
    "category_means.rename(columns={'item_cnt_month': 'item_cnt_month_mean_category'}, inplace=True)\n",
    "engineered_df = engineered_df.merge(category_means, on=['item_category_id', 'date_block_num'], how='left')\n",
    "\n",
    "shop_means = engineered_df.groupby(['shop_id', 'date_block_num'])['item_cnt_month'].mean().reset_index()\n",
    "shop_means.rename(columns={'item_cnt_month': 'item_cnt_month_mean_shop'}, inplace=True)\n",
    "engineered_df = engineered_df.merge(shop_means, on=['shop_id', 'date_block_num'], how='left')\n",
    "\n",
    "item_means = engineered_df.groupby(['item_id', 'date_block_num'])['item_cnt_month'].mean().reset_index()\n",
    "item_means.rename(columns={'item_cnt_month': 'item_cnt_month_mean_item'}, inplace=True)\n",
    "engineered_df = engineered_df.merge(item_means, on=['item_id', 'date_block_num'], how='left')\n",
    "\n",
    "# Impute missing values\n",
    "features_to_impute = [\n",
    "    'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3',\n",
    "    'item_cnt_month_mean_category', 'item_cnt_month_mean_shop', 'item_cnt_month_mean_item',\n",
    "    'item_price'\n",
    "]\n",
    "engineered_df, imputation_info = impute_nans_with_median_then_mean(engineered_df, features_to_impute)\n",
    "\n",
    "# Save\n",
    "engineered_df.to_parquet(\"lstm_data/engineered_df.parquet\", index=False)\n",
    "\n",
    "# Verify item_price\n",
    "print(\"\\nItem_price statistics in engineered_df:\")\n",
    "print(engineered_df['item_price'].describe())\n",
    "\n",
    "# Preview\n",
    "print(\"\\nSample of engineered DataFrame:\")\n",
    "print(engineered_df[['shop_id', 'item_id', 'date_block_num', 'item_category_id', 'item_cnt_month', 'Return', 'item_price'] + features_to_impute].head(10))\n",
    "print(f\"Engineered data shape: {engineered_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcddcb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date_block_num',\n",
       " 'shop_id',\n",
       " 'item_id',\n",
       " 'item_category_id',\n",
       " 'item_cnt_month',\n",
       " 'item_price',\n",
       " 'Return',\n",
       " 'item_cnt_month_lag_1',\n",
       " 'item_cnt_month_lag_2',\n",
       " 'item_cnt_month_lag_3',\n",
       " 'item_cnt_month_mean_category',\n",
       " 'item_cnt_month_mean_shop',\n",
       " 'item_cnt_month_mean_item']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engineered_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80b9b055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return value counts:\n",
      "Return\n",
      "0    1594181\n",
      "1       7228\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Date_block_num value counts (first 10):\n",
      "date_block_num\n",
      "0    62238\n",
      "1    59132\n",
      "2    63302\n",
      "3    54637\n",
      "4    53296\n",
      "5    56196\n",
      "6    58035\n",
      "7    58022\n",
      "8    51575\n",
      "9    50463\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Item_price statistics in scaled_df:\n",
      "count    1.601409e+06\n",
      "mean     1.507174e-01\n",
      "std      7.141147e-01\n",
      "min     -4.163441e+00\n",
      "25%     -4.036773e-01\n",
      "50%      9.632267e-02\n",
      "75%      6.722114e-01\n",
      "max      1.918395e+00\n",
      "Name: item_price, dtype: float64\n",
      "\n",
      "Sample of scaled_df after scaling and encoding:\n",
      "   shop_id  item_id  date_block_num  item_category_id  item_cnt_month  Return  \\\n",
      "0        0       32               0                40        1.531888       0   \n",
      "1        0       33               0                37        0.371638       0   \n",
      "2        0       35               0                40       -0.401862       0   \n",
      "3        0       43               0                40       -0.401862       0   \n",
      "4        0       51               0                57       -0.015112       0   \n",
      "5        0       61               0                43       -0.401862       0   \n",
      "6        0       75               0                40       -0.401862       0   \n",
      "7        0       88               0                40       -0.401862       0   \n",
      "8        0       95               0                40       -0.401862       0   \n",
      "9        0       96               0                40       -0.401862       0   \n",
      "\n",
      "   item_price  item_cnt_month_lag_1  item_cnt_month_lag_2  \\\n",
      "0   -0.328397             -0.375761             -0.344595   \n",
      "1   -0.004134             -0.375761             -0.344595   \n",
      "2   -0.248507             -0.375761             -0.344595   \n",
      "3   -0.328397             -0.375761             -0.344595   \n",
      "4   -0.717201             -0.375761             -0.344595   \n",
      "5   -0.418250             -0.375761             -0.344595   \n",
      "6   -1.092212             -0.375761             -0.344595   \n",
      "7   -1.092212             -0.375761             -0.344595   \n",
      "8   -0.425649             -0.375761             -0.344595   \n",
      "9   -1.150732             -0.375761             -0.344595   \n",
      "\n",
      "   item_cnt_month_lag_3  item_cnt_month_mean_shop  item_cnt_month_mean_item  \\\n",
      "0             -0.314134                  0.600264                  2.351771   \n",
      "1             -0.314134                  0.600264                  0.006324   \n",
      "2             -0.314134                  0.600264                  0.628654   \n",
      "3             -0.314134                  0.600264                 -0.549458   \n",
      "4             -0.314134                  0.600264                 -0.218961   \n",
      "5             -0.314134                  0.600264                 -0.549458   \n",
      "6             -0.314134                  0.600264                 -0.549458   \n",
      "7             -0.314134                  0.600264                 -0.549458   \n",
      "8             -0.314134                  0.600264                 -0.549458   \n",
      "9             -0.314134                  0.600264                 -0.398374   \n",
      "\n",
      "   item_cnt_month_mean_category  shop_id_mean_encode  item_id_mean_encode  \\\n",
      "0                     -0.267539             0.110591             0.271668   \n",
      "1                     -0.601106             0.110591            -0.194529   \n",
      "2                     -0.267539             0.110591             0.367717   \n",
      "3                     -0.267539             0.110591            -0.401862   \n",
      "4                     -0.728130             0.110591            -0.238736   \n",
      "5                     -0.862468             0.110591            -0.401862   \n",
      "6                     -0.267539             0.110591            -0.368947   \n",
      "7                     -0.267539             0.110591            -0.401862   \n",
      "8                     -0.267539             0.110591            -0.401862   \n",
      "9                     -0.267539             0.110591            -0.344566   \n",
      "\n",
      "   item_category_id_mean_encode  \n",
      "0                     -0.082734  \n",
      "1                     -0.209486  \n",
      "2                     -0.082734  \n",
      "3                     -0.082734  \n",
      "4                     -0.267926  \n",
      "5                     -0.312478  \n",
      "6                     -0.082734  \n",
      "7                     -0.082734  \n",
      "8                     -0.082734  \n",
      "9                     -0.082734  \n",
      "\n",
      "Data shape:\n",
      "scaled_df: (1601409, 16)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import joblib\n",
    "\n",
    "# ---- COPY AND PREP ----\n",
    "scaled_df = engineered_df.copy()\n",
    "\n",
    "# Ensure correct data types\n",
    "scaled_df['shop_id'] = scaled_df['shop_id'].astype('int16')\n",
    "scaled_df['item_id'] = scaled_df['item_id'].astype('int32')\n",
    "scaled_df['date_block_num'] = scaled_df['date_block_num'].astype('int8')\n",
    "scaled_df['item_category_id'] = scaled_df['item_category_id'].astype('int32')\n",
    "scaled_df['Return'] = scaled_df['Return'].astype('int8')\n",
    "\n",
    "# ---- DEFINE COLUMNS ----\n",
    "numerical_cols = [\n",
    "    'item_cnt_month',\n",
    "    'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3',\n",
    "    'item_cnt_month_mean_shop', 'item_cnt_month_mean_item', 'item_cnt_month_mean_category'\n",
    "]\n",
    "price_col = ['item_price']\n",
    "categorical_cols = ['shop_id', 'item_id', 'item_category_id']\n",
    "\n",
    "# Verify columns exist\n",
    "missing_numerical = [col for col in numerical_cols + price_col if col not in scaled_df.columns]\n",
    "missing_categorical = [col for col in categorical_cols if col not in scaled_df.columns]\n",
    "if missing_numerical:\n",
    "    print(f\"Warning: Missing numerical columns: {missing_numerical}\")\n",
    "if missing_categorical:\n",
    "    print(f\"Warning: Missing categorical columns: {missing_categorical}\")\n",
    "\n",
    "# ---- CHRONOLOGICAL SPLIT FOR SCALING/ENCODING ----\n",
    "train_df = scaled_df[scaled_df['date_block_num'] <= 26].copy()\n",
    "val_df = scaled_df[(scaled_df['date_block_num'] > 26) & (scaled_df['date_block_num'] <= 29)].copy()\n",
    "test_df = scaled_df[scaled_df['date_block_num'] > 29].copy()\n",
    "\n",
    "# ---- APPLY STANDARD SCALING TO NUMERICAL COLUMNS ----\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[numerical_cols])  # Fit only on training data\n",
    "train_df[numerical_cols] = scaler.transform(train_df[numerical_cols]).astype(np.float32)\n",
    "val_df[numerical_cols] = scaler.transform(val_df[numerical_cols]).astype(np.float32)\n",
    "test_df[numerical_cols] = scaler.transform(test_df[numerical_cols]).astype(np.float32)\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'lstm_data/scaler.joblib')\n",
    "\n",
    "# ---- APPLY ROBUST SCALING TO item_price ----\n",
    "price_scaler = RobustScaler()\n",
    "price_scaler.fit(train_df[price_col])  # Fit only on training data\n",
    "train_df['item_price'] = price_scaler.transform(train_df[price_col]).astype(np.float32)\n",
    "val_df['item_price'] = price_scaler.transform(val_df[price_col]).astype(np.float32)\n",
    "test_df['item_price'] = price_scaler.transform(test_df[price_col]).astype(np.float32)\n",
    "\n",
    "# Save price scaler\n",
    "joblib.dump(price_scaler, 'lstm_data/price_scaler.joblib')\n",
    "\n",
    "# Mean encoding for shop_id, item_id, item_category_id\n",
    "for col in categorical_cols:\n",
    "    mean_encoded = train_df.groupby(col)['item_cnt_month'].mean().to_dict()\n",
    "    train_df[f'{col}_mean_encode'] = train_df[col].map(mean_encoded).astype(np.float32)\n",
    "    val_df[f'{col}_mean_encode'] = val_df[col].map(mean_encoded).fillna(train_df['item_cnt_month'].mean()).astype(np.float32)\n",
    "    test_df[f'{col}_mean_encode'] = test_df[col].map(mean_encoded).fillna(train_df['item_cnt_month'].mean()).astype(np.float32)\n",
    "\n",
    "\n",
    "scaled_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "# ---- SORT BY date_block_num, shop_id, item_id ----\n",
    "scaled_df = scaled_df.sort_values(['date_block_num', 'shop_id', 'item_id']).reset_index(drop=True)\n",
    "\n",
    "# ---- SAVE OUTPUT ----\n",
    "scaled_df.to_parquet(\"lstm_data/scaled_df.parquet\", index=False)\n",
    "\n",
    "# ---- VERIFICATION ----\n",
    "print(\"Return value counts:\")\n",
    "print(scaled_df['Return'].value_counts())\n",
    "print(\"\\nDate_block_num value counts (first 10):\")\n",
    "print(scaled_df['date_block_num'].value_counts().sort_index().head(10))\n",
    "print(\"\\nItem_price statistics in scaled_df:\")\n",
    "print(scaled_df['item_price'].describe())\n",
    "\n",
    "# ---- PREVIEW OUTPUT ----\n",
    "print(\"\\nSample of scaled_df after scaling and encoding:\")\n",
    "preview_cols = ['shop_id', 'item_id', 'date_block_num', 'item_category_id', 'item_cnt_month', 'Return', 'item_price', 'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3', 'item_cnt_month_mean_shop', 'item_cnt_month_mean_item', 'item_cnt_month_mean_category', 'shop_id_mean_encode', 'item_id_mean_encode', 'item_category_id_mean_encode']\n",
    "print(scaled_df[preview_cols].head(10))\n",
    "\n",
    "print(\"\\nData shape:\")\n",
    "print(f\"scaled_df: {scaled_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37fcd4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date_block_num',\n",
       " 'shop_id',\n",
       " 'item_id',\n",
       " 'item_category_id',\n",
       " 'item_cnt_month',\n",
       " 'item_price',\n",
       " 'Return',\n",
       " 'item_cnt_month_lag_1',\n",
       " 'item_cnt_month_lag_2',\n",
       " 'item_cnt_month_lag_3',\n",
       " 'item_cnt_month_mean_category',\n",
       " 'item_cnt_month_mean_shop',\n",
       " 'item_cnt_month_mean_item',\n",
       " 'shop_id_mean_encode',\n",
       " 'item_id_mean_encode',\n",
       " 'item_category_id_mean_encode']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7d7f1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 19:07:39,387 - INFO - Original data shape: 1601409\n",
      "2025-09-13 19:07:39,390 - INFO - Performing feature engineering...\n",
      "2025-09-13 19:07:39,486 - INFO - Preparing data...\n",
      "2025-09-13 19:07:39,812 - INFO - Prepared data shape: 1,146,719, valid groups: 145,397\n",
      "2025-09-13 19:07:39,813 - INFO - Scaling target and feature variables...\n",
      "2025-09-13 19:07:39,961 - INFO - ✓ Saved target_scaler.pkl and feature_scaler.pkl\n",
      "2025-09-13 19:07:39,962 - INFO - Creating sequences...\n",
      "2025-09-13 19:08:32,341 - INFO - Created 710,528 sequences\n",
      "2025-09-13 19:08:33,128 - INFO - Splitting data...\n",
      "2025-09-13 19:08:33,134 - INFO - Train: 573,640, Val: 59,474, Test: 77,414\n",
      "2025-09-13 19:08:33,971 - INFO - Saved train sequences to /Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/modelnotebook/training_data/train_[X|y].parquet\n",
      "2025-09-13 19:08:34,027 - INFO - Saved val sequences to /Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/modelnotebook/training_data/val_[X|y].parquet\n",
      "2025-09-13 19:08:34,106 - INFO - Saved test sequences to /Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/modelnotebook/training_data/test_[X|y].parquet\n",
      "2025-09-13 19:08:35,008 - INFO - ✓ Saved test_X_with_ids.parquet\n",
      "2025-09-13 19:08:35,009 - INFO - Creating and saving item_metadata.parquet...\n",
      "2025-09-13 19:08:35,066 - INFO - ✓ Saved item_metadata.parquet\n",
      "2025-09-13 19:08:35,068 - INFO - \n",
      "=== Data Shapes ===\n",
      "2025-09-13 19:08:35,068 - INFO - Original: (1601409, 16)\n",
      "2025-09-13 19:08:35,069 - INFO - Prepared: (1146719, 16)\n",
      "2025-09-13 19:08:35,069 - INFO - Train: (573640, 3, 12), y: (573640,)\n",
      "2025-09-13 19:08:35,070 - INFO - Val: (59474, 3, 12), y: (59474,)\n",
      "2025-09-13 19:08:35,070 - INFO - Test: (77414, 3, 12), y: (77414,)\n",
      "2025-09-13 19:08:35,071 - INFO - \n",
      "Date ranges:\n",
      "2025-09-13 19:08:35,086 - INFO - Train: 3 to 26\n",
      "2025-09-13 19:08:35,105 - INFO - Val: 27 to 29\n",
      "2025-09-13 19:08:35,116 - INFO - Test: 30 to 33\n",
      "2025-09-13 19:08:35,116 - INFO - \n",
      "Saved feature columns:\n",
      "2025-09-13 19:08:35,117 - INFO - ['item_cnt_month_t0', 'item_price_t0', 'Return_t0', 'item_cnt_month_lag_1_t0', 'item_cnt_month_lag_2_t0', 'item_cnt_month_lag_3_t0', 'item_cnt_month_mean_category_t0', 'item_cnt_month_mean_shop_t0', 'item_cnt_month_mean_item_t0', 'shop_id_mean_encode_t0', 'item_id_mean_encode_t0', 'item_category_id_mean_encode_t0', 'item_cnt_month_t1', 'item_price_t1', 'Return_t1', 'item_cnt_month_lag_1_t1', 'item_cnt_month_lag_2_t1', 'item_cnt_month_lag_3_t1', 'item_cnt_month_mean_category_t1', 'item_cnt_month_mean_shop_t1', 'item_cnt_month_mean_item_t1', 'shop_id_mean_encode_t1', 'item_id_mean_encode_t1', 'item_category_id_mean_encode_t1', 'item_cnt_month_t2', 'item_price_t2', 'Return_t2', 'item_cnt_month_lag_1_t2', 'item_cnt_month_lag_2_t2', 'item_cnt_month_lag_3_t2', 'item_cnt_month_mean_category_t2', 'item_cnt_month_mean_shop_t2', 'item_cnt_month_mean_item_t2', 'shop_id_mean_encode_t2', 'item_id_mean_encode_t2', 'item_category_id_mean_encode_t2']\n",
      "2025-09-13 19:08:35,118 - INFO - ✓ Verified: train_X.parquet and train_y.parquet exist\n",
      "2025-09-13 19:08:35,119 - INFO - ✓ Verified: val_X.parquet and val_y.parquet exist\n",
      "2025-09-13 19:08:35,119 - INFO - ✓ Verified: test_X.parquet and test_y.parquet exist\n",
      "2025-09-13 19:08:35,121 - INFO - ✓ Verified: feature_scaler.pkl exists\n",
      "2025-09-13 19:08:35,122 - INFO - ✓ Verified: target_scaler.pkl exists\n",
      "2025-09-13 19:08:35,122 - INFO - ✓ Verified: test_X_with_ids.parquet exists\n",
      "2025-09-13 19:08:35,123 - INFO - ✓ Verified: item_metadata.parquet exists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import polars as pl\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "SAVE_DIR = Path('/Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/modelnotebook/training_data')\n",
    "SAVE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SEQUENCE_LENGTH = 3\n",
    "FEATURE_COLS = [\n",
    "    'item_cnt_month', 'item_price', 'Return',\n",
    "    'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3',\n",
    "    'item_cnt_month_mean_category', 'item_cnt_month_mean_shop', 'item_cnt_month_mean_item',\n",
    "    'shop_id_mean_encode', 'item_id_mean_encode', 'item_category_id_mean_encode'\n",
    "]\n",
    "TARGET_COL = 'item_cnt_month'\n",
    "DATE_COL = 'date_block_num'\n",
    "GROUP_COLS = ['shop_id', 'item_id']\n",
    "\n",
    "def create_lag_features(df):\n",
    "    logger.info(\"Creating lag features...\")\n",
    "    df = df.sort_values([DATE_COL] + GROUP_COLS).copy()\n",
    "    for lag in [1, 2, 3]:\n",
    "        df[f'{TARGET_COL}_lag_{lag}'] = df.groupby(GROUP_COLS)[TARGET_COL].shift(lag)\n",
    "    return df\n",
    "\n",
    "def create_mean_encoded_features(df):\n",
    "    logger.info(\"Creating mean-encoded features...\")\n",
    "    df['item_cnt_month_mean_item'] = df.groupby([DATE_COL, 'item_id'])[TARGET_COL].transform('mean')\n",
    "    df['item_cnt_month_mean_shop'] = df.groupby([DATE_COL, 'shop_id'])[TARGET_COL].transform('mean')\n",
    "    df['item_cnt_month_mean_category'] = df.groupby([DATE_COL, 'item_category_id'])[TARGET_COL].transform('mean')\n",
    "    df['shop_id_mean_encode'] = df.groupby('shop_id')[TARGET_COL].transform('mean')\n",
    "    df['item_id_mean_encode'] = df.groupby('item_id')[TARGET_COL].transform('mean')\n",
    "    df['item_category_id_mean_encode'] = df.groupby('item_category_id')[TARGET_COL].transform('mean')\n",
    "    return df\n",
    "\n",
    "def create_sequences(df):\n",
    "    logger.info(\"Creating sequences...\")\n",
    "    missing_cols = [col for col in FEATURE_COLS if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        logger.error(f\"Missing columns in dataframe: {missing_cols}\")\n",
    "        raise KeyError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    sequences, targets, target_dates, identifiers = [], [], [], []\n",
    "    grouped = df.groupby(GROUP_COLS)\n",
    "    for (shop_id, item_id), group in grouped:\n",
    "        group_data = group[FEATURE_COLS].values\n",
    "        group_target = group[TARGET_COL].values\n",
    "        group_dates = group[DATE_COL].values\n",
    "        for i in range(len(group) - SEQUENCE_LENGTH):\n",
    "            sequences.append(group_data[i:i + SEQUENCE_LENGTH])\n",
    "            targets.append(group_target[i + SEQUENCE_LENGTH])\n",
    "            target_dates.append(group_dates[i + SEQUENCE_LENGTH])\n",
    "            identifiers.append({'shop_id': shop_id, 'item_id': item_id})\n",
    "    \n",
    "    if not sequences:\n",
    "        logger.warning(\"No sequences created.\")\n",
    "        return np.empty((0, SEQUENCE_LENGTH, len(FEATURE_COLS))), np.array([]), np.array([]), []\n",
    "    \n",
    "    logger.info(f\"Created {len(sequences):,} sequences\")\n",
    "    return np.array(sequences, dtype=np.float32), np.array(targets, dtype=np.float32), np.array(target_dates, dtype=np.int32), identifiers\n",
    "\n",
    "def time_based_split(sequences, targets, target_dates, identifiers, train_end=26, val_end=29):\n",
    "    logger.info(\"Splitting data...\")\n",
    "    train_idx = target_dates <= train_end\n",
    "    val_idx = (target_dates > train_end) & (target_dates <= val_end)\n",
    "    test_idx = target_dates > val_end\n",
    "\n",
    "    logger.info(f\"Train: {train_idx.sum():,}, Val: {val_idx.sum():,}, Test: {test_idx.sum():,}\")\n",
    "    return (\n",
    "        (sequences[train_idx], targets[train_idx], [identifiers[i] for i in range(len(identifiers)) if train_idx[i]]),\n",
    "        (sequences[val_idx], targets[val_idx], [identifiers[i] for i in range(len(identifiers)) if val_idx[i]]),\n",
    "        (sequences[test_idx], targets[test_idx], [identifiers[i] for i in range(len(identifiers)) if test_idx[i]])\n",
    "    )\n",
    "\n",
    "def save_sequences(X, y, prefix):\n",
    "    if len(X) == 0:\n",
    "        logger.warning(f\"{prefix} set is empty\")\n",
    "        return\n",
    "    \n",
    "    X_flat = X.reshape(X.shape[0], -1)\n",
    "    feature_names = [f\"{feat}_t{t}\" for t in range(SEQUENCE_LENGTH) for feat in FEATURE_COLS]\n",
    "    \n",
    "    try:\n",
    "        pl.DataFrame(X_flat, schema=feature_names).write_parquet(os.path.join(SAVE_DIR, f\"{prefix}_X.parquet\"))\n",
    "        pl.DataFrame(y, schema=[TARGET_COL]).write_parquet(os.path.join(SAVE_DIR, f\"{prefix}_y.parquet\"))\n",
    "        logger.info(f\"Saved {prefix} sequences to {SAVE_DIR}/{prefix}_[X|y].parquet\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save {prefix} sequences: {e}\")\n",
    "        raise\n",
    "\n",
    "def main(scaled_df):\n",
    "    try:\n",
    "        original_df = scaled_df.copy()\n",
    "        logger.info(f\"Original data shape: {original_df.shape[0]}\")\n",
    "    except NameError:\n",
    "        logger.error(\"Error: scaled_df is not defined. Please provide the DataFrame.\")\n",
    "        raise\n",
    "\n",
    "    # Verify required columns\n",
    "    required_cols = ['date_block_num', 'shop_id', 'item_id', 'item_category_id', 'item_cnt_month', 'item_price']\n",
    "    missing_cols = [col for col in required_cols if col not in original_df.columns]\n",
    "    if missing_cols:\n",
    "        logger.error(f\"Missing columns in scaled_df: {missing_cols}\")\n",
    "        raise KeyError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "    # Feature engineering\n",
    "    logger.info(\"Performing feature engineering...\")\n",
    "    engineered_df = original_df.copy()\n",
    "\n",
    "    # Add Return feature if missing\n",
    "    if 'Return' not in engineered_df.columns:\n",
    "        engineered_df['Return'] = (engineered_df[TARGET_COL] < 0).astype(np.int8)\n",
    "        logger.info(\"Created Return feature\")\n",
    "\n",
    "    # Clip target\n",
    "    engineered_df[TARGET_COL] = engineered_df[TARGET_COL].clip(0, 20)\n",
    "\n",
    "    # Create lag features if missing\n",
    "    lag_cols = [f'item_cnt_month_lag_{i}' for i in [1, 2, 3]]\n",
    "    if not all(col in engineered_df.columns for col in lag_cols):\n",
    "        engineered_df = create_lag_features(engineered_df)\n",
    "        logger.info(\"Created lag features\")\n",
    "\n",
    "    # Create mean-encoded features if missing\n",
    "    mean_cols = [\n",
    "        'item_cnt_month_mean_item', 'item_cnt_month_mean_shop', 'item_cnt_month_mean_category',\n",
    "        'shop_id_mean_encode', 'item_id_mean_encode', 'item_category_id_mean_encode'\n",
    "    ]\n",
    "    if not all(col in engineered_df.columns for col in mean_cols):\n",
    "        engineered_df = create_mean_encoded_features(engineered_df)\n",
    "        logger.info(\"Created mean-encoded features\")\n",
    "\n",
    "    # Fill NaNs\n",
    "    engineered_df = engineered_df.fillna(0)\n",
    "\n",
    "    # Prepare data (filter groups with enough data)\n",
    "    logger.info(\"Preparing data...\")\n",
    "    min_required = SEQUENCE_LENGTH + 1\n",
    "    item_counts = engineered_df.groupby(GROUP_COLS)[DATE_COL].count()\n",
    "    valid_items = item_counts[item_counts >= min_required].index\n",
    "    prepared_df = engineered_df[engineered_df.set_index(GROUP_COLS).index.isin(valid_items)]\n",
    "    logger.info(f\"Prepared data shape: {prepared_df.shape[0]:,}, valid groups: {len(valid_items):,}\")\n",
    "\n",
    "    # Scaling\n",
    "    logger.info(\"Scaling target and feature variables...\")\n",
    "    feature_cols_to_scale = [\n",
    "        'item_price', 'Return', 'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3',\n",
    "        'item_cnt_month_mean_shop', 'item_cnt_month_mean_item', 'item_cnt_month_mean_category',\n",
    "        'shop_id_mean_encode', 'item_id_mean_encode', 'item_category_id_mean_encode'\n",
    "    ]\n",
    "    \n",
    "    target_scaler = StandardScaler()\n",
    "    feature_scaler = StandardScaler()\n",
    "    \n",
    "    prepared_df[TARGET_COL] = target_scaler.fit_transform(prepared_df[[TARGET_COL]])\n",
    "    prepared_df[feature_cols_to_scale] = feature_scaler.fit_transform(prepared_df[feature_cols_to_scale])\n",
    "\n",
    "    # Save scalers\n",
    "    joblib.dump(target_scaler, SAVE_DIR / 'target_scaler.pkl')\n",
    "    joblib.dump(feature_scaler, SAVE_DIR / 'feature_scaler.pkl')\n",
    "    logger.info(f\"✓ Saved target_scaler.pkl and feature_scaler.pkl\")\n",
    "\n",
    "    # Create sequences\n",
    "    all_sequences, all_targets, target_dates, identifiers = create_sequences(prepared_df)\n",
    "\n",
    "    # Time-based splitting\n",
    "    (X_train, y_train, train_ids), (X_val, y_val, val_ids), (X_test, y_test, test_ids) = time_based_split(\n",
    "        all_sequences, all_targets, target_dates, identifiers\n",
    "    )\n",
    "\n",
    "    # Save sequences\n",
    "    save_sequences(X_train, y_train, 'train')\n",
    "    save_sequences(X_val, y_val, 'val')\n",
    "    save_sequences(X_test, y_test, 'test')\n",
    "\n",
    "    # Save test_X_with_ids.parquet\n",
    "    if test_ids:\n",
    "        test_ids_df = pd.DataFrame(test_ids)\n",
    "        test_ids_df.to_parquet(SAVE_DIR / 'test_X_with_ids.parquet', index=False)\n",
    "        logger.info(f\"✓ Saved test_X_with_ids.parquet\")\n",
    "    else:\n",
    "        logger.warning(\"Test set is empty; skipping test_X_with_ids.parquet\")\n",
    "\n",
    "    # Create and save item_metadata.parquet\n",
    "    logger.info(\"Creating and saving item_metadata.parquet...\")\n",
    "    item_metadata = prepared_df.groupby('item_id').agg({\n",
    "        'item_price': 'mean',\n",
    "        'item_category_id': 'first'\n",
    "    }).reset_index()\n",
    "    item_metadata['item_category_name'] = item_metadata['item_category_id'].map(\n",
    "        {i: f\"Category_{i}\" for i in item_metadata['item_category_id'].unique()}\n",
    "    )\n",
    "    item_metadata.to_parquet(SAVE_DIR / 'item_metadata.parquet', index=False)\n",
    "    logger.info(f\"✓ Saved item_metadata.parquet\")\n",
    "\n",
    "    # Log data shapes and date ranges\n",
    "    logger.info(\"\\n=== Data Shapes ===\")\n",
    "    logger.info(f\"Original: {original_df.shape}\")\n",
    "    logger.info(f\"Prepared: {prepared_df.shape}\")\n",
    "    logger.info(f\"Train: {X_train.shape}, y: {y_train.shape}\")\n",
    "    logger.info(f\"Val: {X_val.shape}, y: {y_val.shape}\")\n",
    "    logger.info(f\"Test: {X_test.shape}, y: {y_test.shape}\")\n",
    "\n",
    "    logger.info(\"\\nDate ranges:\")\n",
    "    logger.info(f\"Train: {target_dates[target_dates <= 26].min() if target_dates.size > 0 else 'N/A'} to {target_dates[target_dates <= 26].max() if target_dates.size > 0 else 'N/A'}\")\n",
    "    logger.info(f\"Val: {target_dates[(target_dates > 26) & (target_dates <= 29)].min() if target_dates[(target_dates > 26) & (target_dates <= 29)].size > 0 else 'N/A'} to {target_dates[(target_dates > 26) & (target_dates <= 29)].max() if target_dates[(target_dates > 26) & (target_dates <= 29)].size > 0 else 'N/A'}\")\n",
    "    logger.info(f\"Test: {target_dates[target_dates > 29].min() if target_dates[target_dates > 29].size > 0 else 'N/A'} to {target_dates[target_dates > 29].max() if target_dates[target_dates > 29].size > 0 else 'N/A'}\")\n",
    "\n",
    "    logger.info(\"\\nSaved feature columns:\")\n",
    "    logger.info([f\"{feat}_t{t}\" for t in range(SEQUENCE_LENGTH) for feat in FEATURE_COLS])\n",
    "\n",
    "    # Verify saved files\n",
    "    for prefix in ['train', 'val', 'test']:\n",
    "        x_path = SAVE_DIR / f\"{prefix}_X.parquet\"\n",
    "        y_path = SAVE_DIR / f\"{prefix}_y.parquet\"\n",
    "        if x_path.exists() and y_path.exists():\n",
    "            logger.info(f\"✓ Verified: {x_path.name} and {y_path.name} exist\")\n",
    "        else:\n",
    "            logger.error(f\"✗ Missing: {x_path.name} or {y_path.name}\")\n",
    "    for f in ['feature_scaler.pkl', 'target_scaler.pkl', 'test_X_with_ids.parquet', 'item_metadata.parquet']:\n",
    "        if (SAVE_DIR / f).exists():\n",
    "            logger.info(f\"✓ Verified: {f} exists\")\n",
    "        else:\n",
    "            logger.error(f\"✗ Missing: {f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # If running standalone, scaled_df must be defined\n",
    "        main(scaled_df)\n",
    "    except NameError:\n",
    "        logger.error(\"scaled_df is not defined. Please provide the DataFrame when calling main().\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Preprocessing failed: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9d02a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2c035bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 14:05:08,845 - INFO - Using device: mps\n",
      "2025-09-14 14:05:08,850 - INFO - Initiating program execution\n",
      "2025-09-14 14:05:08,852 - INFO - Loading prerequisite files for prediction...\n",
      "2025-09-14 14:05:08,962 - INFO - Loading feature_scaler.pkl for price unscaling.\n",
      "2025-09-14 14:05:08,963 - INFO - Loading metadata...\n",
      "2025-09-14 14:05:09,029 - INFO - Initializing datasets and dataloaders...\n",
      "2025-09-14 14:05:10,097 - INFO - Initializing model...\n",
      "2025-09-14 14:05:10,545 - INFO - Starting model training...\n",
      "Epoch 1: 100%|██████████| 4482/4482 [01:05<00:00, 68.44it/s]\n",
      "2025-09-14 14:06:18,478 - INFO - Epoch 1, Train MSE: 0.4659, RMSE: 0.6826, MAE: 0.3685, Val MSE: 0.2638, RMSE: 0.5136, MAE: 0.2594\n",
      "Epoch 2: 100%|██████████| 4482/4482 [01:05<00:00, 68.35it/s]\n",
      "2025-09-14 14:07:26,532 - INFO - Epoch 2, Train MSE: 0.3765, RMSE: 0.6136, MAE: 0.2892, Val MSE: 0.2684, RMSE: 0.5180, MAE: 0.2520\n",
      "Epoch 3: 100%|██████████| 4482/4482 [01:05<00:00, 68.61it/s]\n",
      "2025-09-14 14:08:34,256 - INFO - Epoch 3, Train MSE: 0.3659, RMSE: 0.6049, MAE: 0.2820, Val MSE: 0.2426, RMSE: 0.4925, MAE: 0.2592\n",
      "Epoch 4: 100%|██████████| 4482/4482 [01:05<00:00, 68.22it/s]\n",
      "2025-09-14 14:09:42,409 - INFO - Epoch 4, Train MSE: 0.3637, RMSE: 0.6031, MAE: 0.2804, Val MSE: 0.2671, RMSE: 0.5168, MAE: 0.2541\n",
      "Epoch 5: 100%|██████████| 4482/4482 [01:05<00:00, 68.89it/s]\n",
      "2025-09-14 14:10:49,830 - INFO - Epoch 5, Train MSE: 0.3592, RMSE: 0.5993, MAE: 0.2740, Val MSE: 0.2275, RMSE: 0.4770, MAE: 0.2038\n",
      "Epoch 6: 100%|██████████| 4482/4482 [01:05<00:00, 68.78it/s]\n",
      "2025-09-14 14:11:57,426 - INFO - Epoch 6, Train MSE: 0.3519, RMSE: 0.5932, MAE: 0.2696, Val MSE: 0.2579, RMSE: 0.5078, MAE: 0.2411\n",
      "Epoch 7: 100%|██████████| 4482/4482 [01:05<00:00, 68.03it/s]\n",
      "2025-09-14 14:13:05,720 - INFO - Epoch 7, Train MSE: 0.3507, RMSE: 0.5922, MAE: 0.2680, Val MSE: 0.2430, RMSE: 0.4929, MAE: 0.2373\n",
      "Epoch 8: 100%|██████████| 4482/4482 [01:05<00:00, 68.22it/s]\n",
      "2025-09-14 14:14:13,818 - INFO - Epoch 8, Train MSE: 0.3468, RMSE: 0.5889, MAE: 0.2659, Val MSE: 0.2424, RMSE: 0.4923, MAE: 0.2329\n",
      "Epoch 9: 100%|██████████| 4482/4482 [01:04<00:00, 69.63it/s]\n",
      "2025-09-14 14:15:20,821 - INFO - Epoch 9, Train MSE: 0.3446, RMSE: 0.5870, MAE: 0.2645, Val MSE: 0.2452, RMSE: 0.4951, MAE: 0.2285\n",
      "Epoch 10: 100%|██████████| 4482/4482 [01:07<00:00, 65.99it/s]\n",
      "2025-09-14 14:16:31,087 - INFO - Epoch 10, Train MSE: 0.3412, RMSE: 0.5841, MAE: 0.2630, Val MSE: 0.2316, RMSE: 0.4812, MAE: 0.2132\n",
      "2025-09-14 14:16:31,088 - INFO - Early stopping triggered after 10 epochs\n",
      "2025-09-14 14:16:31,355 - INFO - Loading best model from epoch with loss: 0.2275\n",
      "2025-09-14 14:16:31,375 - INFO - Starting prediction and interpretation...\n",
      "Predicting: 100%|██████████| 605/605 [00:06<00:00, 86.57it/s]\n",
      "2025-09-14 14:16:38,391 - INFO - Lookup success: Shops 100.00%, Items 100.00%, Prices 53.42%\n",
      "2025-09-14 14:16:39,029 - INFO - Final predictions saved to /Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/results/final_predictions.csv\n",
      "2025-09-14 14:16:39,048 - INFO - Generating dashboard metrics by category...\n",
      "2025-09-14 14:16:39,073 - INFO - Dashboard summary saved to /Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/results/dashboard_category_summary.csv\n",
      "2025-09-14 14:16:39,611 - INFO - Program executed successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "\n",
    "# Setup logging\n",
    "log_dir = Path('/Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/logs')\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_dir / f'run_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "class SalesDataset(Dataset):\n",
    "    def __init__(self, X_file, y_file, sequence_length=3):\n",
    "        if not Path(X_file).exists() or not Path(y_file).exists():\n",
    "            raise FileNotFoundError(f\"Data files not found: {X_file}, {y_file}\")\n",
    "        self.X = pl.read_parquet(X_file)\n",
    "        feature_cols = [\n",
    "            'item_cnt_month', 'item_price', 'Return',\n",
    "            'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3',\n",
    "            'item_cnt_month_mean_shop', 'item_cnt_month_mean_item', 'item_cnt_month_mean_category',\n",
    "            'shop_id_mean_encode', 'item_id_mean_encode', 'item_category_id_mean_encode'\n",
    "        ]\n",
    "        input_cols = [f\"{feat}_t{t}\" for t in range(sequence_length) for feat in feature_cols]\n",
    "        available_cols = self.X.columns\n",
    "        missing_cols = [col for col in input_cols if col not in available_cols]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns in {X_file}: {missing_cols}\")\n",
    "        self.y = pl.read_parquet(y_file).to_numpy().astype(np.float32).reshape(-1, 1)\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        if len(self.X) != len(self.y):\n",
    "            raise ValueError(f\"X and y length mismatch: {len(self.X)} vs {len(self.y)}\")\n",
    "        self.sequence_length = sequence_length\n",
    "        self.numerical_cols = input_cols\n",
    "        numerical_data = self.X.select(self.numerical_cols).to_numpy().astype(np.float32)\n",
    "        numerical_data = np.nan_to_num(numerical_data, nan=0.0)\n",
    "        self.y = np.nan_to_num(self.y, nan=0.0)\n",
    "        numerical_data = numerical_data.reshape(len(self.X), sequence_length, len(feature_cols))\n",
    "        self.numerical = numerical_data\n",
    "        self.identifiers = self.indices\n",
    "        self.dates = [datetime(2013, 1, 1) + timedelta(days=30 * (int(idx) % 120)) for idx in self.indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        numerical = torch.tensor(self.numerical[idx], dtype=torch.float32)\n",
    "        target = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        identifiers = torch.tensor([self.identifiers[idx]], dtype=torch.int32)\n",
    "        dates = self.dates[idx]\n",
    "        return {'numerical': numerical, 'target': target, 'identifiers': identifiers, 'dates': dates}\n",
    "\n",
    "class HALSTM(nn.Module):\n",
    "    def __init__(self, numerical_dim=12, hidden_dim=128, num_layers=2, num_heads=4, dropout=0.3, l2_lambda=0.01):\n",
    "        super(HALSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.input_dim = numerical_dim\n",
    "        self.lstm = nn.LSTM(self.input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.lstm_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.mha = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.mha_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.gate = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc_shared = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.positional_encoding = torch.zeros(3, hidden_dim, device=device)\n",
    "        position = torch.arange(0, 3, dtype=torch.float, device=device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2, device=device).float() * (-torch.log(torch.tensor(10000.0, device=device)) / hidden_dim))\n",
    "        self.positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LSTM):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight' in name: nn.init.xavier_normal_(param)\n",
    "                    elif 'bias' in name: nn.init.constant_(param, 0)\n",
    "\n",
    "    def forward(self, numerical):\n",
    "        batch_size, seq_len, _ = numerical.size()\n",
    "        x = self.dropout(numerical)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        lstm_out = self.lstm_norm(lstm_out)\n",
    "        lstm_out = lstm_out + self.positional_encoding[:seq_len, :].unsqueeze(0)\n",
    "        mha_out, mha_weights = self.mha(lstm_out, lstm_out, lstm_out)\n",
    "        mha_out = self.mha_norm(mha_out)\n",
    "        combined = torch.cat([lstm_out[:, -1, :], mha_out[:, -1, :]], dim=-1)\n",
    "        gate_val = self.sigmoid(self.gate(combined))\n",
    "        fused = gate_val * lstm_out[:, -1, :] + (1 - gate_val) * mha_out[:, -1, :]\n",
    "        shared = self.relu(self.fc_shared(fused))\n",
    "        output = self.fc_out(shared)\n",
    "        return output, {'mha_weights': mha_weights, 'gate_weights': gate_val}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    if not batch: return {}\n",
    "    return {\n",
    "        'numerical': torch.stack([item['numerical'] for item in batch]),\n",
    "        'target': torch.stack([item['target'] for item in batch]),\n",
    "        'identifiers': torch.stack([item['identifiers'] for item in batch]),\n",
    "        'dates': [item['dates'] for item in batch]\n",
    "    }\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=15, lr=0.0005, accum_steps=2):\n",
    "    criterion_mse = nn.MSELoss().to(device)\n",
    "    criterion_mae = nn.L1Loss().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=model.l2_lambda)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=num_epochs, steps_per_epoch=len(train_loader)//accum_steps)\n",
    "    scaler = torch.cuda.amp.GradScaler() if device.type in ['cuda', 'mps'] else None\n",
    "    \n",
    "    metrics = {'epoch': [], 'train_mse': [], 'train_rmse': [], 'train_mae': [], 'val_mse': [], 'val_rmse': [], 'val_mae': []}\n",
    "    best_val_loss = float('inf')\n",
    "    output_dir = Path('/Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/results')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    patience = 5\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_mse = 0\n",
    "        train_mae = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "            numerical = batch['numerical'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            \n",
    "            if scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output, _ = model(numerical)\n",
    "                    mse_loss = criterion_mse(output, target) / accum_steps\n",
    "                    mae_loss = criterion_mae(output, target) / accum_steps\n",
    "                    loss = mse_loss\n",
    "                scaler.scale(loss).backward()\n",
    "                if (batch_idx + 1) % accum_steps == 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                    scaler.step(optimizer)\n",
    "                    scheduler.step()\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "            else:\n",
    "                output, _ = model(numerical)\n",
    "                mse_loss = criterion_mse(output, target) / accum_steps\n",
    "                mae_loss = criterion_mae(output, target) / accum_steps\n",
    "                loss = mse_loss\n",
    "                loss.backward()\n",
    "                if (batch_idx + 1) % accum_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += mse_loss.item() * accum_steps\n",
    "            train_mse += mse_loss.item() * accum_steps\n",
    "            train_mae += mae_loss.item() * accum_steps\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_mse /= len(train_loader)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        train_mae /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_mse = 0\n",
    "        val_mae = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                numerical = batch['numerical'].to(device)\n",
    "                target = batch['target'].to(device)\n",
    "                output, _ = model(numerical)\n",
    "                mse_loss = criterion_mse(output, target)\n",
    "                mae_loss = criterion_mae(output, target)\n",
    "                val_loss += mse_loss.item()\n",
    "                val_mse += mse_loss.item()\n",
    "                val_mae += mae_loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_mse /= len(val_loader)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        val_mae /= len(val_loader)\n",
    "        \n",
    "        metrics['epoch'].append(epoch + 1)\n",
    "        metrics['train_mse'].append(train_mse)\n",
    "        metrics['train_rmse'].append(train_rmse)\n",
    "        metrics['train_mae'].append(train_mae)\n",
    "        metrics['val_mse'].append(val_mse)\n",
    "        metrics['val_rmse'].append(val_rmse)\n",
    "        metrics['val_mae'].append(val_mae)\n",
    "        logger.info(f\"Epoch {epoch+1}, Train MSE: {train_mse:.4f}, RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, \"\n",
    "                    f\"Val MSE: {val_mse:.4f}, RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), output_dir / 'best_ha_lstm.pth')\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_df.to_csv(output_dir / 'training_metrics.csv', index=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(metrics['epoch'], metrics['train_rmse'], label='Train RMSE')\n",
    "    plt.plot(metrics['epoch'], metrics['val_rmse'], label='Val RMSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('Training and Validation RMSE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(metrics['epoch'], metrics['train_mae'], label='Train MAE')\n",
    "    plt.plot(metrics['epoch'], metrics['val_mae'], label='Val MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title('Training and Validation MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'metrics_plot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(f\"Loading best model from epoch with loss: {best_val_loss:.4f}\")\n",
    "    model.load_state_dict(torch.load(output_dir / 'best_ha_lstm.pth'))\n",
    "    return model, metrics_df\n",
    "\n",
    "def predict(model, test_loader, test_df_with_ids, item_metadata_df, shops_df, item_category_dict, scaler, feature_scaler=None):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    interpret_outputs = []\n",
    "    successful_shop_lookups = 0\n",
    "    successful_item_lookups = 0\n",
    "    successful_price_lookups = 0\n",
    "    total_samples = 0\n",
    "    negative_price_count = 0\n",
    "    \n",
    "    item_metadata_dict = item_metadata_df.set_index('item_id').to_dict('index') if not item_metadata_df.empty else {}\n",
    "    \n",
    "    # Use placeholder shop names if shops_df is unavailable\n",
    "    if shops_df is None or shops_df.empty:\n",
    "        logger.warning(\"shops_df is unavailable. Using placeholder shop names.\")\n",
    "        shop_metadata_dict = {i: f\"Shop_{i}\" for i in test_df_with_ids['shop_id'].unique()}\n",
    "    else:\n",
    "        shop_metadata_dict = shops_df.set_index('shop_id')['shop_name'].to_dict()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Predicting\")):\n",
    "            numerical = batch['numerical'].to(device)\n",
    "            identifiers = batch['identifiers']\n",
    "            dates = batch['dates']\n",
    "            \n",
    "            output, attn_dict = model(numerical)\n",
    "            \n",
    "            unscaled_preds = scaler.inverse_transform(output.cpu().numpy())\n",
    "            \n",
    "            mha_weights = attn_dict['mha_weights'][:, -1, :].cpu().numpy()\n",
    "            gate_weights = attn_dict['gate_weights'].cpu().numpy()\n",
    "            \n",
    "            batch_size = len(unscaled_preds)\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                original_row_index = identifiers[i][0].item()\n",
    "                row_data = test_df_with_ids.iloc[original_row_index]\n",
    "                shop_id = int(row_data['shop_id'])\n",
    "                item_id = int(row_data['item_id'])\n",
    "                \n",
    "                predicted_demand = max(0, round(unscaled_preds[i][0]))\n",
    "                \n",
    "                category_name = item_category_dict.get(item_id, f\"Category_{row_data.get('item_category_id', 'Unknown')}\")\n",
    "                if category_name != 'Unknown':\n",
    "                    successful_item_lookups += 1\n",
    "                \n",
    "                metadata = item_metadata_dict.get(item_id, {})\n",
    "                unit_price = metadata.get('item_price', 0)\n",
    "                if unit_price == 0 and feature_scaler is not None:\n",
    "                    numerical_i = numerical[i].cpu().numpy()\n",
    "                    prices = numerical_i[:, 1].reshape(-1, 1)\n",
    "                    unscaled_prices = feature_scaler.inverse_transform(prices)\n",
    "                    unit_price = unscaled_prices[-1]\n",
    "                    if unit_price == 0:\n",
    "                        non_zero = unscaled_prices[unscaled_prices > 0]\n",
    "                        unit_price = np.mean(non_zero) if len(non_zero) > 0 else 0\n",
    "                unit_price = max(0, unit_price)\n",
    "                if unit_price < 0:\n",
    "                    negative_price_count += 1\n",
    "                if unit_price > 0:\n",
    "                    successful_price_lookups += 1\n",
    "                \n",
    "                # FIXED: Lookup shop_name from shop_metadata_dict\n",
    "                shop_name = shop_metadata_dict.get(shop_id, f\"Shop_{shop_id}\")\n",
    "                if shop_name != f\"Shop_{shop_id}\":\n",
    "                    successful_shop_lookups += 1\n",
    "                \n",
    "                total_sales = unit_price * predicted_demand\n",
    "                \n",
    "                pred_dict = {\n",
    "                    'shop_id': shop_id,\n",
    "                    'shop_name': shop_name,\n",
    "                    'item_id': item_id,\n",
    "                    'item_category_name': category_name,\n",
    "                    'predicted_product_demand': predicted_demand,\n",
    "                    'predicted_total_sales': total_sales\n",
    "                }\n",
    "                predictions.append(pred_dict)\n",
    "                \n",
    "                lstm_reliance_score = np.mean(gate_weights[i])\n",
    "                attention_t2 = mha_weights[i][0]\n",
    "                attention_t1 = mha_weights[i][1]\n",
    "                attention_t0 = mha_weights[i][2]\n",
    "                \n",
    "                interpret_outputs.append({\n",
    "                    'timestamp_reference': dates[i].isoformat(),\n",
    "                    'shop_id': shop_id,\n",
    "                    'shop_name': shop_name,\n",
    "                    'item_id': item_id,\n",
    "                    'forecasted_value_unscaled': unscaled_preds[i][0],\n",
    "                    'lstm_trend_reliance': lstm_reliance_score,\n",
    "                    'attention_t_minus_2': attention_t2,\n",
    "                    'attention_t_minus_1': attention_t1,\n",
    "                    'attention_t_minus_0': attention_t0\n",
    "                })\n",
    "    \n",
    "    logger.info(f\"Lookup success: Shops {successful_shop_lookups/total_samples*100:.2f}%, Items {successful_item_lookups/total_samples*100:.2f}%, Prices {successful_price_lookups/total_samples*100:.2f}%\")\n",
    "    if negative_price_count > 0:\n",
    "        logger.warning(f\"Encountered {negative_price_count} negative prices before clamping to 0\")\n",
    "    \n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    interpret_df = pd.DataFrame(interpret_outputs)\n",
    "    \n",
    "    output_dir = Path('/Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/results')\n",
    "    pred_df.to_csv(output_dir / 'final_predictions.csv', index=False)\n",
    "    interpret_df.to_csv(output_dir / 'final_interpretability_outputs.csv', index=False)\n",
    "    \n",
    "    logger.info(f\"Final predictions saved to {output_dir / 'final_predictions.csv'}\")\n",
    "    \n",
    "    return pred_df, interpret_df\n",
    "\n",
    "def generate_dashboard_metrics(pred_df):\n",
    "    if pred_df.empty:\n",
    "        logger.warning(\"Prediction dataframe is empty, cannot generate dashboard metrics.\")\n",
    "        return\n",
    "    logger.info(\"Generating dashboard metrics by category...\")\n",
    "    category_summary = pred_df.groupby('item_category_name').agg(\n",
    "        total_demand=('predicted_product_demand', 'sum'),\n",
    "        total_revenue=('predicted_total_sales', 'sum')\n",
    "    ).reset_index().sort_values(by='total_revenue', ascending=False)\n",
    "    output_dir = Path('/Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/results')\n",
    "    category_summary.to_csv(output_dir / 'dashboard_category_summary.csv', index=False)\n",
    "    logger.info(f\"Dashboard summary saved to {output_dir / 'dashboard_category_summary.csv'}\")\n",
    "\n",
    "def visualize_results(pred_df, y_test_unscaled):\n",
    "    output_dir = Path('/Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/results')\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.kdeplot(pred_df['predicted_product_demand'], label='Predicted Demand', clip=(0, 20))\n",
    "    sns.kdeplot(y_test_unscaled, label='Actual Demand', clip=(0, 20))\n",
    "    plt.title('Prediction vs. Actual Distribution (Unscaled)')\n",
    "    plt.legend()\n",
    "    plt.savefig(output_dir / 'prediction_visualization.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    data_dir = Path('/Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/modelnotebook/training_data')\n",
    "    raw_data_dir = Path('/Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/raw_data')\n",
    "\n",
    "    train_X_path = data_dir / 'train_X.parquet'\n",
    "    train_y_path = data_dir / 'train_y.parquet'\n",
    "    val_X_path = data_dir / 'val_X.parquet'\n",
    "    val_y_path = data_dir / 'val_y.parquet'\n",
    "    test_X_path = data_dir / 'test_X.parquet'\n",
    "    test_y_path = data_dir / 'test_y.parquet'\n",
    "    test_X_with_ids_path = data_dir / 'test_X_with_ids.parquet'\n",
    "    item_metadata_path = data_dir / 'item_metadata.parquet'\n",
    "    scaler_path = data_dir / 'target_scaler.pkl'\n",
    "    feature_scaler_path = data_dir / 'feature_scaler.pkl'\n",
    "    shops_path = data_dir / 'shops.csv'\n",
    "    items_path = data_dir / 'items.csv'\n",
    "    categories_path = data_dir / 'item_categories.csv'\n",
    "\n",
    "    required_files = [\n",
    "        train_X_path, train_y_path, val_X_path, val_y_path,\n",
    "        test_X_path, test_y_path, test_X_with_ids_path,\n",
    "        item_metadata_path, scaler_path, feature_scaler_path,\n",
    "        shops_path, items_path, categories_path\n",
    "    ]\n",
    "    for f in required_files:\n",
    "        if not f.exists():\n",
    "            raise FileNotFoundError(f\"CRITICAL: Required data file not found at {f}. Please ensure all files are present or run preprocess_data.py.\")\n",
    "\n",
    "    logger.info(\"Loading prerequisite files for prediction...\")\n",
    "    test_df_with_ids = pd.read_parquet(test_X_with_ids_path)\n",
    "    item_metadata_df = pd.read_parquet(item_metadata_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    logger.info(\"Loading feature_scaler.pkl for price unscaling.\")\n",
    "    feature_scaler = joblib.load(feature_scaler_path)\n",
    "    \n",
    "    logger.info(\"Loading metadata...\")\n",
    "    shops_df = pd.read_csv(shops_path)\n",
    "    items_df = pd.read_csv(items_path)\n",
    "    categories_df = pd.read_csv(categories_path)\n",
    "    item_category_dict = items_df.merge(categories_df, on='item_category_id').set_index('item_id')['item_category_name'].to_dict()\n",
    "\n",
    "    batch_size = 128\n",
    "    num_epochs = 15\n",
    "    lr = 0.0005\n",
    "    accum_steps = 2\n",
    "    \n",
    "    logger.info(\"Initializing datasets and dataloaders...\")\n",
    "    train_dataset = SalesDataset(train_X_path, train_y_path)\n",
    "    val_dataset = SalesDataset(val_X_path, val_y_path)\n",
    "    test_dataset = SalesDataset(test_X_path, test_y_path)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    logger.info(\"Initializing model...\")\n",
    "    model = HALSTM(numerical_dim=12, hidden_dim=128, num_layers=2, num_heads=4, dropout=0.3, l2_lambda=0.01).to(device)\n",
    "    \n",
    "    logger.info(\"Starting model training...\")\n",
    "    model, metrics_df = train_model(model, train_loader, val_loader, num_epochs, lr, accum_steps)\n",
    "    \n",
    "    logger.info(\"Starting prediction and interpretation...\")\n",
    "    pred_df, interpret_df = predict(model, test_loader, test_df_with_ids, item_metadata_df, shops_df, item_category_dict, scaler, feature_scaler)\n",
    "    \n",
    "    generate_dashboard_metrics(pred_df)\n",
    " \n",
    "    y_test_scaled = pl.read_parquet(test_y_path).to_numpy()\n",
    "    y_test_unscaled = scaler.inverse_transform(y_test_scaled).flatten()\n",
    "    visualize_results(pred_df, y_test_unscaled)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(\"Initiating program execution\")\n",
    "        main()\n",
    "        logger.info(\"Program executed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Program execution failed: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59434a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   item_id  item_price  item_category_id item_category_name\n",
      "0        1    2.376879                76        Category_76\n",
      "1       28   -0.431686                30        Category_30\n",
      "2       30   -0.661427                40        Category_40\n",
      "3       31    0.099377                37        Category_37\n",
      "4       32   -0.822099                40        Category_40\n"
     ]
    }
   ],
   "source": [
    "item_metadata_df = pd.read_parquet('/Users/mohammednihal/Desktop/XAI/Cashflow-and-demand-forecating-/modelnotebook/training_data/item_metadata.parquet')\n",
    "print(item_metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58a925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
