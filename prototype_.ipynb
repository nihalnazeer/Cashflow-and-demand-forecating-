{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01845c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five rows of the data:\n",
      "         date  date_block_num  shop_id  item_id  item_price  item_cnt_day  \\\n",
      "0  02.01.2013               0       59    22154      999.00           1.0   \n",
      "1  03.01.2013               0       25     2552      899.00           1.0   \n",
      "2  05.01.2013               0       25     2552      899.00          -1.0   \n",
      "3  06.01.2013               0       25     2554     1709.05           1.0   \n",
      "4  15.01.2013               0       25     2555     1099.00           1.0   \n",
      "\n",
      "                                  item_name  item_category_id  \\\n",
      "0                    ANNOUNCEMENT 2012 (BD)                37   \n",
      "1  DEEP PURPLE  The House Of Blue Light  LP                58   \n",
      "2  DEEP PURPLE  The House Of Blue Light  LP                58   \n",
      "3  DEEP PURPLE  Who Do You Think We Are  LP                58   \n",
      "4            DEEP PURPE 30 Very Best Of 2CD                56   \n",
      "\n",
      "                 item_category_name            shop_name  \n",
      "0                 Cinema - Blue-Ray  Jaroslavl TC Altair  \n",
      "1                     Music - Vinyl    Moscow ARK Atrium  \n",
      "2                     Music - Vinyl    Moscow ARK Atrium  \n",
      "3                     Music - Vinyl    Moscow ARK Atrium  \n",
      "4  Music - a CD of brand production    Moscow ARK Atrium  \n",
      "\n",
      "Data Shape: \n",
      "(2935849, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_path = pd.read_csv(\"/Users/mohammednihal/XAI-1/Predict Future Sales/merged_data.csv\")\n",
    "df = data_path\n",
    "\n",
    "print(\"First five rows of the data:\")\n",
    "print(df.head())\n",
    "print(\"\\nData Shape: \")\n",
    "print(df.shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa42e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6537eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset with 2,935,849 records\n",
      "Initial unique counts:\n",
      "item_id: 21807\n",
      "item_name: 21233\n",
      "item_name nulls before imputation: 84\n",
      "item_id with 'Unknown' item_name: 0\n",
      "Unique item_name per item_id:\n",
      "item_name\n",
      "1    21806\n",
      "0        1\n",
      "Name: count, dtype: int64\n",
      "Initial null counts:\n",
      "Column 'date': 0 nulls (0.00%)\n",
      "Column 'date_block_num': 0 nulls (0.00%)\n",
      "Column 'shop_id': 0 nulls (0.00%)\n",
      "Column 'item_id': 0 nulls (0.00%)\n",
      "Column 'item_price': 0 nulls (0.00%)\n",
      "Column 'item_cnt_day': 0 nulls (0.00%)\n",
      "Column 'item_name': 84 nulls (0.00%)\n",
      "Column 'item_category_id': 0 nulls (0.00%)\n",
      "Column 'item_category_name': 0 nulls (0.00%)\n",
      "Column 'shop_name': 0 nulls (0.00%)\n",
      "Created Return column. Total Returns: 7,541.0\n",
      "Converted date column to datetime format\n",
      "Imputed 84 missing item names with 'Unknown'\n",
      "Selected 54 shops after removing ['36', '11', '20', '8', '9', '40']\n",
      "Removed 14,017 records from shop selection\n",
      "Winsorization applied at 99th percentile for item_cnt_day after 3σ rolling-window detection\n",
      "Total records: 2,921,832\n",
      "Outliers (above 3σ): 2,072 (0.07%)\n",
      "Values clipped (>99th percentile): 48,150 (1.65%)\n",
      "Final unique counts:\n",
      "item_id: 21600\n",
      "item_name: 21027\n"
     ]
    }
   ],
   "source": [
    "def data_preprocessing(df):\n",
    "    expected_columns = ['date', 'date_block_num', 'shop_id', 'item_id', 'item_price', 'item_cnt_day',\n",
    "                       'item_name', 'item_category_id', 'item_category_name', 'shop_name']\n",
    "    if not all(col in df.columns for col in expected_columns):\n",
    "        missing = set(expected_columns) - set(df.columns)\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "    \n",
    "    print(f\"Preprocessing dataset with {len(df):,} records\")\n",
    "    \n",
    "    cleaned_df = df.copy()\n",
    "    # Initial checks\n",
    "    print(\"Initial unique counts:\")\n",
    "    print(f\"item_id: {cleaned_df['item_id'].nunique()}\")\n",
    "    print(f\"item_name: {cleaned_df['item_name'].nunique()}\")\n",
    "    print(\"item_name nulls before imputation:\", cleaned_df['item_name'].isna().sum())\n",
    "    print(\"item_id with 'Unknown' item_name:\", (cleaned_df['item_name'] == 'Unknown').sum())\n",
    "    print(\"Unique item_name per item_id:\")\n",
    "    name_counts = cleaned_df.groupby('item_id')['item_name'].nunique()\n",
    "    print(name_counts.value_counts())\n",
    "    if (name_counts > 1).any():\n",
    "        print(\"item_id with multiple item_name:\", name_counts[name_counts > 1].index.tolist())\n",
    "\n",
    "    cleaned_df['item_price'] = cleaned_df['item_price'].astype(np.float32)\n",
    "    cleaned_df['item_cnt_day'] = cleaned_df['item_cnt_day'].astype(np.float32)\n",
    "    \n",
    "    print(\"Initial null counts:\")\n",
    "    for col in cleaned_df.columns:\n",
    "        nulls = cleaned_df[col].isna().sum()\n",
    "        print(f\"Column '{col}': {nulls:,} nulls ({nulls/len(cleaned_df)*100:.2f}%)\")\n",
    "    \n",
    "    cleaned_df['Return'] = cleaned_df['item_cnt_day'].where(cleaned_df['item_cnt_day'] < 0, 0).abs().astype(np.float32)\n",
    "    cleaned_df['item_cnt_day'] = cleaned_df['item_cnt_day'].clip(lower=0)\n",
    "    print(f\"Created Return column. Total Returns: {cleaned_df['Return'].sum():,}\")\n",
    "    \n",
    "    cleaned_df['date'] = pd.to_datetime(cleaned_df['date'], format='%d.%m.%Y')\n",
    "    print(\"Converted date column to datetime format\")\n",
    "    \n",
    "    # Handle item_name\n",
    "    if 'item_name' in cleaned_df.columns and cleaned_df['item_name'].isna().any():\n",
    "        item_name_nulls = cleaned_df['item_name'].isna().sum()\n",
    "        cleaned_df['item_name'] = cleaned_df['item_name'].fillna('Unknown')\n",
    "        print(f\"Imputed {item_name_nulls:,} missing item names with 'Unknown'\")\n",
    "    \n",
    "    # Ensure string types\n",
    "    cleaned_df['shop_id'] = cleaned_df['shop_id'].astype(str)\n",
    "    cleaned_df['item_id'] = cleaned_df['item_id'].astype(str)\n",
    "    cleaned_df['item_name'] = cleaned_df['item_name'].astype(str)\n",
    "    \n",
    "    # Fix multiple item_name per item_id\n",
    "    name_counts = cleaned_df.groupby('item_id')['item_name'].nunique()\n",
    "    if (name_counts > 1).any():\n",
    "        print(f\"Warning: {name_counts[name_counts > 1].count()} item_id(s) have multiple item_name values. Taking most frequent.\")\n",
    "        most_frequent = cleaned_df.groupby('item_id')['item_name'].agg(lambda x: x.mode()[0]).reset_index()\n",
    "        cleaned_df = cleaned_df.drop(columns='item_name').merge(most_frequent, on='item_id', how='left')\n",
    "    \n",
    "    shop_stats = cleaned_df.groupby('shop_id').size().reset_index(name='count')\n",
    "    shops_to_remove = shop_stats.nsmallest(6, 'count')['shop_id'].tolist()\n",
    "    remove_records = cleaned_df[cleaned_df['shop_id'].isin(shops_to_remove)].shape[0]\n",
    "    cleaned_df = cleaned_df[~cleaned_df['shop_id'].isin(shops_to_remove)]\n",
    "    print(f\"Selected {cleaned_df['shop_id'].nunique()} shops after removing {shops_to_remove}\")\n",
    "    print(f\"Removed {remove_records:,} records from shop selection\")\n",
    "    \n",
    "    cleaned_df = cleaned_df.sort_values(['shop_id', 'item_id', 'date'])\n",
    "    \n",
    "    def winsorize_with_rolling_stats(group):\n",
    "        group = group.set_index('date').sort_index()\n",
    "        rolling_mean = group['item_cnt_day'].rolling(window='30D', min_periods=1).mean()\n",
    "        rolling_std = group['item_cnt_day'].rolling(window='30D', min_periods=1).std()\n",
    "        upper_3sigma = rolling_mean + 3 * rolling_std\n",
    "        \n",
    "        outliers = group['item_cnt_day'] > upper_3sigma\n",
    "        outlier_count = outliers.sum()\n",
    "        \n",
    "        winsor_limit = group['item_cnt_day'].quantile(0.99)\n",
    "        clipped = group['item_cnt_day'].clip(upper=winsor_limit)\n",
    "        clipped_count = (group['item_cnt_day'] > winsor_limit).sum()\n",
    "        \n",
    "        return clipped.reset_index(drop=True), outlier_count, clipped_count\n",
    "        \n",
    "    total_records = len(cleaned_df)\n",
    "    item_cnt_day_winsorized = []\n",
    "    total_outlier = 0\n",
    "    total_clipped = 0\n",
    "    \n",
    "    for (shop_id, item_id), group in cleaned_df.groupby(['shop_id', 'item_id']):\n",
    "        clipped_series, outliers, clipped = winsorize_with_rolling_stats(group.copy())\n",
    "        item_cnt_day_winsorized.extend(clipped_series)\n",
    "        total_outlier += outliers\n",
    "        total_clipped += clipped\n",
    "        \n",
    "    cleaned_df['item_cnt_day'] = item_cnt_day_winsorized\n",
    "    \n",
    "    outlier_percentage = (total_outlier / total_records * 100) if total_records > 0 else 0\n",
    "    clipped_percentage = (total_clipped / total_records * 100) if total_records > 0 else 0\n",
    "\n",
    "    print(\"Winsorization applied at 99th percentile for item_cnt_day after 3σ rolling-window detection\")\n",
    "    print(f\"Total records: {total_records:,}\")\n",
    "    print(f\"Outliers (above 3σ): {total_outlier:,} ({outlier_percentage:.2f}%)\")\n",
    "    print(f\"Values clipped (>99th percentile): {total_clipped:,} ({clipped_percentage:.2f}%)\")\n",
    "    \n",
    "    # Final checks\n",
    "    print(\"Final unique counts:\")\n",
    "    print(f\"item_id: {cleaned_df['item_id'].nunique()}\")\n",
    "    print(f\"item_name: {cleaned_df['item_name'].nunique()}\")\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "cleaned_data = data_preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3737910b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'date_block_num',\n",
       " 'shop_id',\n",
       " 'item_id',\n",
       " 'item_price',\n",
       " 'item_cnt_day',\n",
       " 'item_name',\n",
       " 'item_category_id',\n",
       " 'item_category_name',\n",
       " 'shop_name',\n",
       " 'Return']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6528415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly data sample:\n",
      "   date_block_num shop_id item_id  item_category_id  item_cnt_month  \\\n",
      "0               0       0    1000                67             5.0   \n",
      "1               0       0    1001                67             2.0   \n",
      "2               0       0   10012                40             1.0   \n",
      "3               0       0    1002                67             2.0   \n",
      "4               0       0    1003                67             2.0   \n",
      "\n",
      "   item_price  Return  \n",
      "0        58.0     0.0  \n",
      "1        58.0     0.0  \n",
      "2        76.0     0.0  \n",
      "3        58.0     0.0  \n",
      "4        58.0     0.0  \n",
      "Monthly data shape: (1601409, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def aggregate_to_monthly(df_clean):\n",
    "    # Aggregate to monthly level\n",
    "    monthly_data = df_clean.groupby(['date_block_num', 'shop_id', 'item_id', 'item_category_id']).agg({\n",
    "        'item_cnt_day': 'sum',\n",
    "        'item_price': 'mean',\n",
    "        'Return': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    monthly_data.rename(columns={'item_cnt_day': 'item_cnt_month'}, inplace=True)\n",
    "    \n",
    "    # Clip item_cnt_month\n",
    "    monthly_data['item_cnt_month'] = monthly_data['item_cnt_month'].clip(0, 20).astype(np.float32)\n",
    "    \n",
    "    return monthly_data\n",
    "\n",
    "# Assuming df_clean is loaded\n",
    "monthly_data = aggregate_to_monthly(cleaned_data)\n",
    "\n",
    "# Save for debugging\n",
    "monthly_data.to_parquet(\"lstm_data/monthly_data.parquet\", index=False)\n",
    "\n",
    "print(\"Monthly data sample:\")\n",
    "print(monthly_data.head())\n",
    "print(f\"Monthly data shape: {monthly_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a724ed50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date_block_num',\n",
       " 'shop_id',\n",
       " 'item_id',\n",
       " 'item_cnt_month',\n",
       " 'Return',\n",
       " 'item_price']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "monthly_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6988cbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineering on monthly_data with 1,601,409 rows\n",
      "Imputed 418,619 NaN values in 'item_cnt_month_lag_1' with median value 1.0000\n",
      "Imputed 694,941 NaN values in 'item_cnt_month_lag_2' with median value 1.0000\n",
      "Imputed 890,881 NaN values in 'item_cnt_month_lag_3' with median value 1.0000\n",
      "\n",
      "Item_price statistics in engineered_df:\n",
      "count    1.601409e+06\n",
      "mean     6.066872e+00\n",
      "std      9.899732e-01\n",
      "min      8.617770e-02\n",
      "25%      5.298317e+00\n",
      "50%      5.991465e+00\n",
      "75%      6.789816e+00\n",
      "max      8.517393e+00\n",
      "Name: item_price, dtype: float64\n",
      "\n",
      "Sample of engineered DataFrame:\n",
      "  shop_id item_id  date_block_num  item_category_id  item_cnt_month  Return  \\\n",
      "0       0    1000               0                67             5.0       0   \n",
      "1       0    1000               1                67             4.0       0   \n",
      "2       0   10004               1                40             1.0       0   \n",
      "3       0    1001               0                67             2.0       0   \n",
      "4       0   10012               0                40             1.0       0   \n",
      "5       0   10012               1                40             2.0       0   \n",
      "6       0    1002               0                67             2.0       0   \n",
      "7       0    1003               0                67             2.0       0   \n",
      "8       0   10033               1                55             1.0       0   \n",
      "9       0   10038               1                40             1.0       0   \n",
      "\n",
      "   item_price  item_cnt_month_lag_1  item_cnt_month_lag_2  \\\n",
      "0    4.077538                   1.0                   1.0   \n",
      "1    4.077538                   5.0                   1.0   \n",
      "2    4.174387                   1.0                   1.0   \n",
      "3    4.077538                   1.0                   1.0   \n",
      "4    4.343805                   1.0                   1.0   \n",
      "5    4.343805                   1.0                   1.0   \n",
      "6    4.077538                   1.0                   1.0   \n",
      "7    4.077538                   1.0                   1.0   \n",
      "8    4.709530                   1.0                   1.0   \n",
      "9    4.248495                   1.0                   1.0   \n",
      "\n",
      "   item_cnt_month_lag_3  item_cnt_month_mean_category  \\\n",
      "0                   1.0                      1.654111   \n",
      "1                   1.0                      1.664591   \n",
      "2                   1.0                      1.824594   \n",
      "3                   1.0                      1.654111   \n",
      "4                   1.0                      1.767606   \n",
      "5                   1.0                      1.824594   \n",
      "6                   1.0                      1.654111   \n",
      "7                   1.0                      1.654111   \n",
      "8                   1.0                      1.312519   \n",
      "9                   1.0                      1.824594   \n",
      "\n",
      "   item_cnt_month_mean_shop  item_cnt_month_mean_item  item_price  \n",
      "0                  2.263845                  2.058125    4.077538  \n",
      "1                  2.384922                  2.481667    4.077538  \n",
      "2                  2.384922                  1.076923    4.174387  \n",
      "3                  2.263845                  1.263158    4.077538  \n",
      "4                  2.263845                  1.071429    4.343805  \n",
      "5                  2.384922                  1.142857    4.343805  \n",
      "6                  2.263845                  1.832222    4.077538  \n",
      "7                  2.263845                  1.553125    4.077538  \n",
      "8                  2.384922                  1.000000    4.709530  \n",
      "9                  2.384922                  1.000000    4.248495  \n",
      "Engineered data shape: (1601409, 13)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_lag_features(df, group_cols, value_cols, lags=[1, 2, 3]):\n",
    "    df = df.sort_values(group_cols + ['date_block_num']).copy()\n",
    "    for col in value_cols:\n",
    "        for lag in lags:\n",
    "            lag_col = f\"{col}_lag_{lag}\"\n",
    "            df[lag_col] = df.groupby(group_cols)[col].shift(lag)\n",
    "    return df\n",
    "\n",
    "def impute_nans_with_median_then_mean(df, cols):\n",
    "    imputation_values = {}\n",
    "    for col in cols:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            median_val = df[col].median()\n",
    "            if median_val != 0:\n",
    "                fill_val = median_val\n",
    "                method = 'median'\n",
    "            else:\n",
    "                mean_val = df[col].mean()\n",
    "                if mean_val != 0:\n",
    "                    fill_val = mean_val\n",
    "                    method = 'mean'\n",
    "                else:\n",
    "                    fill_val = 0\n",
    "                    method = 'zero'\n",
    "            df[col] = df[col].fillna(fill_val).astype(np.float32)\n",
    "            imputation_values[col] = (fill_val, method)\n",
    "            print(f\"Imputed {nan_count:,} NaN values in '{col}' with {method} value {fill_val:.4f}\")\n",
    "        else:\n",
    "            imputation_values[col] = (None, None)\n",
    "    return df, imputation_values\n",
    "\n",
    "# ---- APPLY FEATURE ENGINEERING ----\n",
    "monthly_data = pd.read_parquet(\"lstm_data/monthly_data.parquet\")\n",
    "engineered_df = monthly_data.copy()\n",
    "\n",
    "print(f\"Starting feature engineering on monthly_data with {len(engineered_df):,} rows\")\n",
    "\n",
    "group_cols = ['shop_id', 'item_id']\n",
    "value_cols = ['item_cnt_month']\n",
    "lags = [1, 2, 3]\n",
    "\n",
    "# Clip item_cnt_month\n",
    "engineered_df['item_cnt_month'] = engineered_df['item_cnt_month'].clip(0, 20)\n",
    "\n",
    "# Make Return binary\n",
    "engineered_df['Return'] = (engineered_df['Return'] > 0).astype(np.int8)\n",
    "\n",
    "# Log-transform item_price with tighter clipping\n",
    "engineered_df['item_price'] = np.log1p(engineered_df['item_price'].clip(lower=0, upper=5000)).astype(np.float32)\n",
    "\n",
    "# Create lag features\n",
    "engineered_df = create_lag_features(engineered_df, group_cols, value_cols, lags)\n",
    "\n",
    "# Add mean encodings\n",
    "category_means = engineered_df.groupby(['item_category_id', 'date_block_num'])['item_cnt_month'].mean().reset_index()\n",
    "category_means.rename(columns={'item_cnt_month': 'item_cnt_month_mean_category'}, inplace=True)\n",
    "engineered_df = engineered_df.merge(category_means, on=['item_category_id', 'date_block_num'], how='left')\n",
    "\n",
    "shop_means = engineered_df.groupby(['shop_id', 'date_block_num'])['item_cnt_month'].mean().reset_index()\n",
    "shop_means.rename(columns={'item_cnt_month': 'item_cnt_month_mean_shop'}, inplace=True)\n",
    "engineered_df = engineered_df.merge(shop_means, on=['shop_id', 'date_block_num'], how='left')\n",
    "\n",
    "item_means = engineered_df.groupby(['item_id', 'date_block_num'])['item_cnt_month'].mean().reset_index()\n",
    "item_means.rename(columns={'item_cnt_month': 'item_cnt_month_mean_item'}, inplace=True)\n",
    "engineered_df = engineered_df.merge(item_means, on=['item_id', 'date_block_num'], how='left')\n",
    "\n",
    "# Impute missing values\n",
    "features_to_impute = [\n",
    "    'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3',\n",
    "    'item_cnt_month_mean_category', 'item_cnt_month_mean_shop', 'item_cnt_month_mean_item',\n",
    "    'item_price'\n",
    "]\n",
    "engineered_df, imputation_info = impute_nans_with_median_then_mean(engineered_df, features_to_impute)\n",
    "\n",
    "# Save\n",
    "engineered_df.to_parquet(\"lstm_data/engineered_df.parquet\", index=False)\n",
    "\n",
    "# Verify item_price\n",
    "print(\"\\nItem_price statistics in engineered_df:\")\n",
    "print(engineered_df['item_price'].describe())\n",
    "\n",
    "# Preview\n",
    "print(\"\\nSample of engineered DataFrame:\")\n",
    "print(engineered_df[['shop_id', 'item_id', 'date_block_num', 'item_category_id', 'item_cnt_month', 'Return', 'item_price'] + features_to_impute].head(10))\n",
    "print(f\"Engineered data shape: {engineered_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bcddcb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date_block_num',\n",
       " 'shop_id',\n",
       " 'item_id',\n",
       " 'item_category_id',\n",
       " 'item_cnt_month',\n",
       " 'item_price',\n",
       " 'Return',\n",
       " 'item_cnt_month_lag_1',\n",
       " 'item_cnt_month_lag_2',\n",
       " 'item_cnt_month_lag_3',\n",
       " 'item_cnt_month_mean_category',\n",
       " 'item_cnt_month_mean_shop',\n",
       " 'item_cnt_month_mean_item']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engineered_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9b055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return value counts:\n",
      "Return\n",
      "0    1594181\n",
      "1       7228\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Date_block_num value counts (first 10):\n",
      "date_block_num\n",
      "0    62238\n",
      "1    59132\n",
      "2    63302\n",
      "3    54637\n",
      "4    53296\n",
      "5    56196\n",
      "6    58035\n",
      "7    58022\n",
      "8    51575\n",
      "9    50463\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Item_price statistics in scaled_df:\n",
      "count    1.601409e+06\n",
      "mean     1.507174e-01\n",
      "std      7.141147e-01\n",
      "min     -4.163441e+00\n",
      "25%     -4.036773e-01\n",
      "50%      9.632267e-02\n",
      "75%      6.722114e-01\n",
      "max      1.918395e+00\n",
      "Name: item_price, dtype: float64\n",
      "\n",
      "Sample of scaled_df after scaling and encoding:\n",
      "   shop_id  item_id  date_block_num  item_category_id  item_cnt_month  Return  \\\n",
      "0        0       32               0                40        1.531888       0   \n",
      "1        0       33               0                37        0.371638       0   \n",
      "2        0       35               0                40       -0.401862       0   \n",
      "3        0       43               0                40       -0.401862       0   \n",
      "4        0       51               0                57       -0.015112       0   \n",
      "5        0       61               0                43       -0.401862       0   \n",
      "6        0       75               0                40       -0.401862       0   \n",
      "7        0       88               0                40       -0.401862       0   \n",
      "8        0       95               0                40       -0.401862       0   \n",
      "9        0       96               0                40       -0.401862       0   \n",
      "\n",
      "   item_price  item_cnt_month_lag_1  item_cnt_month_lag_2  \\\n",
      "0   -0.328397             -0.375761             -0.344595   \n",
      "1   -0.004134             -0.375761             -0.344595   \n",
      "2   -0.248507             -0.375761             -0.344595   \n",
      "3   -0.328397             -0.375761             -0.344595   \n",
      "4   -0.717201             -0.375761             -0.344595   \n",
      "5   -0.418250             -0.375761             -0.344595   \n",
      "6   -1.092212             -0.375761             -0.344595   \n",
      "7   -1.092212             -0.375761             -0.344595   \n",
      "8   -0.425649             -0.375761             -0.344595   \n",
      "9   -1.150732             -0.375761             -0.344595   \n",
      "\n",
      "   item_cnt_month_lag_3  item_cnt_month_mean_shop  item_cnt_month_mean_item  \\\n",
      "0             -0.314134                  0.600264                  2.351771   \n",
      "1             -0.314134                  0.600264                  0.006324   \n",
      "2             -0.314134                  0.600264                  0.628654   \n",
      "3             -0.314134                  0.600264                 -0.549458   \n",
      "4             -0.314134                  0.600264                 -0.218961   \n",
      "5             -0.314134                  0.600264                 -0.549458   \n",
      "6             -0.314134                  0.600264                 -0.549458   \n",
      "7             -0.314134                  0.600264                 -0.549458   \n",
      "8             -0.314134                  0.600264                 -0.549458   \n",
      "9             -0.314134                  0.600264                 -0.398374   \n",
      "\n",
      "   item_cnt_month_mean_category  shop_id_mean_encode  item_id_mean_encode  \\\n",
      "0                     -0.267539             0.110591             0.271668   \n",
      "1                     -0.601106             0.110591            -0.194529   \n",
      "2                     -0.267539             0.110591             0.367717   \n",
      "3                     -0.267539             0.110591            -0.401862   \n",
      "4                     -0.728130             0.110591            -0.238736   \n",
      "5                     -0.862468             0.110591            -0.401862   \n",
      "6                     -0.267539             0.110591            -0.368947   \n",
      "7                     -0.267539             0.110591            -0.401862   \n",
      "8                     -0.267539             0.110591            -0.401862   \n",
      "9                     -0.267539             0.110591            -0.344566   \n",
      "\n",
      "   item_category_id_mean_encode  \n",
      "0                     -0.082734  \n",
      "1                     -0.209486  \n",
      "2                     -0.082734  \n",
      "3                     -0.082734  \n",
      "4                     -0.267926  \n",
      "5                     -0.312478  \n",
      "6                     -0.082734  \n",
      "7                     -0.082734  \n",
      "8                     -0.082734  \n",
      "9                     -0.082734  \n",
      "\n",
      "Data shape:\n",
      "scaled_df: (1601409, 16)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import joblib\n",
    "\n",
    "# ---- COPY AND PREP ----\n",
    "scaled_df = engineered_df.copy()\n",
    "\n",
    "# Ensure correct data types\n",
    "scaled_df['shop_id'] = scaled_df['shop_id'].astype('int16')\n",
    "scaled_df['item_id'] = scaled_df['item_id'].astype('int32')\n",
    "scaled_df['date_block_num'] = scaled_df['date_block_num'].astype('int8')\n",
    "scaled_df['item_category_id'] = scaled_df['item_category_id'].astype('int32')\n",
    "scaled_df['Return'] = scaled_df['Return'].astype('int8')\n",
    "\n",
    "# ---- DEFINE COLUMNS ----\n",
    "numerical_cols = [\n",
    "    'item_cnt_month',\n",
    "    'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3',\n",
    "    'item_cnt_month_mean_shop', 'item_cnt_month_mean_item', 'item_cnt_month_mean_category'\n",
    "]\n",
    "price_col = ['item_price']\n",
    "categorical_cols = ['shop_id', 'item_id', 'item_category_id']\n",
    "\n",
    "# Verify columns exist\n",
    "missing_numerical = [col for col in numerical_cols + price_col if col not in scaled_df.columns]\n",
    "missing_categorical = [col for col in categorical_cols if col not in scaled_df.columns]\n",
    "if missing_numerical:\n",
    "    print(f\"Warning: Missing numerical columns: {missing_numerical}\")\n",
    "if missing_categorical:\n",
    "    print(f\"Warning: Missing categorical columns: {missing_categorical}\")\n",
    "\n",
    "# ---- CHRONOLOGICAL SPLIT FOR SCALING/ENCODING ----\n",
    "train_df = scaled_df[scaled_df['date_block_num'] <= 26].copy()\n",
    "val_df = scaled_df[(scaled_df['date_block_num'] > 26) & (scaled_df['date_block_num'] <= 29)].copy()\n",
    "test_df = scaled_df[scaled_df['date_block_num'] > 29].copy()\n",
    "\n",
    "# ---- APPLY STANDARD SCALING TO NUMERICAL COLUMNS ----\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[numerical_cols])  # Fit only on training data\n",
    "train_df[numerical_cols] = scaler.transform(train_df[numerical_cols]).astype(np.float32)\n",
    "val_df[numerical_cols] = scaler.transform(val_df[numerical_cols]).astype(np.float32)\n",
    "test_df[numerical_cols] = scaler.transform(test_df[numerical_cols]).astype(np.float32)\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'lstm_data/scaler.joblib')\n",
    "\n",
    "# ---- APPLY ROBUST SCALING TO item_price ----\n",
    "price_scaler = RobustScaler()\n",
    "price_scaler.fit(train_df[price_col])  # Fit only on training data\n",
    "train_df['item_price'] = price_scaler.transform(train_df[price_col]).astype(np.float32)\n",
    "val_df['item_price'] = price_scaler.transform(val_df[price_col]).astype(np.float32)\n",
    "test_df['item_price'] = price_scaler.transform(test_df[price_col]).astype(np.float32)\n",
    "\n",
    "# Save price scaler\n",
    "joblib.dump(price_scaler, 'lstm_data/price_scaler.joblib')\n",
    "\n",
    "# Mean encoding for shop_id, item_id, item_category_id\n",
    "for col in categorical_cols:\n",
    "    mean_encoded = train_df.groupby(col)['item_cnt_month'].mean().to_dict()\n",
    "    train_df[f'{col}_mean_encode'] = train_df[col].map(mean_encoded).astype(np.float32)\n",
    "    val_df[f'{col}_mean_encode'] = val_df[col].map(mean_encoded).fillna(train_df['item_cnt_month'].mean()).astype(np.float32)\n",
    "    test_df[f'{col}_mean_encode'] = test_df[col].map(mean_encoded).fillna(train_df['item_cnt_month'].mean()).astype(np.float32)\n",
    "\n",
    "\n",
    "scaled_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "# ---- SORT BY date_block_num, shop_id, item_id ----\n",
    "scaled_df = scaled_df.sort_values(['date_block_num', 'shop_id', 'item_id']).reset_index(drop=True)\n",
    "\n",
    "# ---- SAVE OUTPUT ----\n",
    "scaled_df.to_parquet(\"lstm_data/scaled_df.parquet\", index=False)\n",
    "\n",
    "# ---- VERIFICATION ----\n",
    "print(\"Return value counts:\")\n",
    "print(scaled_df['Return'].value_counts())\n",
    "print(\"\\nDate_block_num value counts (first 10):\")\n",
    "print(scaled_df['date_block_num'].value_counts().sort_index().head(10))\n",
    "print(\"\\nItem_price statistics in scaled_df:\")\n",
    "print(scaled_df['item_price'].describe())\n",
    "\n",
    "# ---- PREVIEW OUTPUT ----\n",
    "print(\"\\nSample of scaled_df after scaling and encoding:\")\n",
    "preview_cols = ['shop_id', 'item_id', 'date_block_num', 'item_category_id', 'item_cnt_month', 'Return', 'item_price', 'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3', 'item_cnt_month_mean_shop', 'item_cnt_month_mean_item', 'item_cnt_month_mean_category', 'shop_id_mean_encode', 'item_id_mean_encode', 'item_category_id_mean_encode']\n",
    "print(scaled_df[preview_cols].head(10))\n",
    "\n",
    "print(\"\\nData shape:\")\n",
    "print(f\"scaled_df: {scaled_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37fcd4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date_block_num',\n",
       " 'shop_id',\n",
       " 'item_id',\n",
       " 'item_category_id',\n",
       " 'item_cnt_month',\n",
       " 'item_price',\n",
       " 'Return',\n",
       " 'item_cnt_month_lag_1',\n",
       " 'item_cnt_month_lag_2',\n",
       " 'item_cnt_month_lag_3',\n",
       " 'item_cnt_month_mean_category',\n",
       " 'item_cnt_month_mean_shop',\n",
       " 'item_cnt_month_mean_item',\n",
       " 'shop_id_mean_encode',\n",
       " 'item_id_mean_encode',\n",
       " 'item_category_id_mean_encode']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d7f1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 09:03:39,824 - INFO - Created/Initialized directory: /Users/mohammednihal/XAI-1/training_data\n",
      "2025-06-02 09:03:39,866 - INFO - Original data shape: 1601409\n",
      "2025-06-02 09:03:39,872 - INFO - Preparing data...\n",
      "2025-06-02 09:03:40,176 - INFO - Prepared data shape: 1,146,719, valid groups: 145,397\n",
      "2025-06-02 09:03:40,179 - INFO - Creating sequences...\n",
      "2025-06-02 09:04:06,171 - INFO - Created 710,528 sequences\n",
      "2025-06-02 09:04:06,547 - INFO - Splitting data...\n",
      "2025-06-02 09:04:06,549 - INFO - Train: 573,640, Val: 59,474, Test: 77,414\n",
      "2025-06-02 09:04:07,085 - INFO - Saved train sequences to /Users/mohammednihal/XAI-1/training_data/train_[X|y].parquet\n",
      "2025-06-02 09:04:07,140 - INFO - Saved val sequences to /Users/mohammednihal/XAI-1/training_data/val_[X|y].parquet\n",
      "2025-06-02 09:04:07,197 - INFO - Saved test sequences to /Users/mohammednihal/XAI-1/training_data/test_[X|y].parquet\n",
      "2025-06-02 09:04:07,197 - INFO - \n",
      "=== Data Shapes ===\n",
      "2025-06-02 09:04:07,198 - INFO - Original: (1601409, 16)\n",
      "2025-06-02 09:04:07,198 - INFO - Prepared: (1146719, 16)\n",
      "2025-06-02 09:04:07,198 - INFO - Train: (573640, 3, 12), y: (573640,)\n",
      "2025-06-02 09:04:07,199 - INFO - Val: (59474, 3, 12), y: (59474,)\n",
      "2025-06-02 09:04:07,199 - INFO - Test: (77414, 3, 12), y: (77414,)\n",
      "2025-06-02 09:04:07,199 - INFO - \n",
      "Date ranges:\n",
      "2025-06-02 09:04:07,208 - INFO - Train: 3 to 26\n",
      "2025-06-02 09:04:07,212 - INFO - Val: 27 to 29\n",
      "2025-06-02 09:04:07,217 - INFO - Test: 30 to 33\n",
      "2025-06-02 09:04:07,218 - INFO - \n",
      "Saved feature columns:\n",
      "2025-06-02 09:04:07,218 - INFO - ['item_cnt_month_t0', 'item_price_t0', 'Return_t0', 'item_cnt_month_lag_1_t0', 'item_cnt_month_lag_2_t0', 'item_cnt_month_lag_3_t0', 'item_cnt_month_mean_category_t0', 'item_cnt_month_mean_shop_t0', 'item_cnt_month_mean_item_t0', 'shop_id_mean_encode_t0', 'item_id_mean_encode_t0', 'item_category_id_mean_encode_t0', 'item_cnt_month_t1', 'item_price_t1', 'Return_t1', 'item_cnt_month_lag_1_t1', 'item_cnt_month_lag_2_t1', 'item_cnt_month_lag_3_t1', 'item_cnt_month_mean_category_t1', 'item_cnt_month_mean_shop_t1', 'item_cnt_month_mean_item_t1', 'shop_id_mean_encode_t1', 'item_id_mean_encode_t1', 'item_category_id_mean_encode_t1', 'item_cnt_month_t2', 'item_price_t2', 'Return_t2', 'item_cnt_month_lag_1_t2', 'item_cnt_month_lag_2_t2', 'item_cnt_month_lag_3_t2', 'item_cnt_month_mean_category_t2', 'item_cnt_month_mean_shop_t2', 'item_cnt_month_mean_item_t2', 'shop_id_mean_encode_t2', 'item_id_mean_encode_t2', 'item_category_id_mean_encode_t2']\n",
      "2025-06-02 09:04:07,219 - INFO - ✓ Verified: train_X.parquet and train_y.parquet exist\n",
      "2025-06-02 09:04:07,219 - INFO - ✓ Verified: val_X.parquet and val_y.parquet exist\n",
      "2025-06-02 09:04:07,219 - INFO - ✓ Verified: test_X.parquet and test_y.parquet exist\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "SEQUENCE_LENGTH = 3\n",
    "FEATURE_COLS = [\n",
    "    'item_cnt_month', 'item_price', 'Return',\n",
    "    'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3',\n",
    "    'item_cnt_month_mean_category', 'item_cnt_month_mean_shop', 'item_cnt_month_mean_item',\n",
    "    'shop_id_mean_encode', 'item_id_mean_encode', 'item_category_id_mean_encode'\n",
    "]\n",
    "TARGET_COL = 'item_cnt_month'\n",
    "DATE_COL = 'date_block_num'\n",
    "GROUP_COLS = ['shop_id', 'item_id']\n",
    "SAVE_DIR = \"/Users/mohammednihal/XAI-1/training_data\"\n",
    "\n",
    "# Create output directory\n",
    "try:\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    logger.info(f\"Created/Initialized directory: {SAVE_DIR}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create directory {SAVE_DIR}: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "try:\n",
    "    original_df = scaled_df.copy()\n",
    "    logger.info(f\"Original data shape: {original_df.shape[0]}\")\n",
    "except NameError:\n",
    "    logger.error(\"Error: scaled_df is not defined. Please ensure it is loaded.\")\n",
    "    raise\n",
    "\n",
    "\n",
    "def prepare_data(df):\n",
    "    logger.info(\"Preparing data...\")\n",
    "    df = df.sort_values([*GROUP_COLS, DATE_COL]).copy()\n",
    "    min_required = SEQUENCE_LENGTH + 1\n",
    "    item_counts = df.groupby(GROUP_COLS)[DATE_COL].count()\n",
    "    valid_items = item_counts[item_counts >= min_required].index\n",
    "    filtered_df = df[df.set_index(GROUP_COLS).index.isin(valid_items)]\n",
    "    logger.info(f\"Prepared data shape: {filtered_df.shape[0]:,}, valid groups: {len(valid_items):,}\")\n",
    "    return filtered_df, len(valid_items)\n",
    "\n",
    "prepared_df, num_valid_groups = prepare_data(original_df)\n",
    "\n",
    "def create_sequences(df):\n",
    "    logger.info(\"Creating sequences...\")\n",
    "\n",
    "    missing_cols = [col for col in FEATURE_COLS if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        logger.error(f\"Missing columns in dataframe: {missing_cols}\")\n",
    "        raise KeyError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    sequences, targets, target_dates = [], [], []\n",
    "    grouped = df.groupby(GROUP_COLS)\n",
    "\n",
    "    for _, group in grouped:\n",
    "        group_data = group[FEATURE_COLS].values\n",
    "        group_target = group[TARGET_COL].values\n",
    "        group_dates = group[DATE_COL].values\n",
    "        \n",
    "        for i in range(len(group) - SEQUENCE_LENGTH):\n",
    "            sequences.append(group_data[i:i+SEQUENCE_LENGTH])\n",
    "            targets.append(group_target[i+SEQUENCE_LENGTH])\n",
    "            target_dates.append(group_dates[i+SEQUENCE_LENGTH])\n",
    "\n",
    "    if not sequences:\n",
    "        logger.warning(\"No sequences created.\")\n",
    "        return np.empty((0, SEQUENCE_LENGTH, len(FEATURE_COLS))), np.array([]), np.array([])\n",
    "\n",
    "    logger.info(f\"Created {len(sequences):,} sequences\")\n",
    "    return np.array(sequences, dtype=np.float32), np.array(targets, dtype=np.float32), np.array(target_dates, dtype=np.int32)\n",
    "\n",
    "all_sequences, all_targets, target_dates = create_sequences(prepared_df)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. TIME-BASED SPLITTING\n",
    "# -----------------------------\n",
    "def time_based_split(sequences, targets, target_dates, train_end=26, val_end=29):\n",
    "    logger.info(\"Splitting data...\")\n",
    "    train_idx = target_dates <= train_end\n",
    "    val_idx = (target_dates > train_end) & (target_dates <= val_end)\n",
    "    test_idx = target_dates > val_end\n",
    "\n",
    "    logger.info(f\"Train: {train_idx.sum():,}, Val: {val_idx.sum():,}, Test: {test_idx.sum():,}\")\n",
    "    return (\n",
    "        (sequences[train_idx], targets[train_idx]),\n",
    "        (sequences[val_idx], targets[val_idx]),\n",
    "        (sequences[test_idx], targets[test_idx])\n",
    "    )\n",
    "\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = time_based_split(\n",
    "    all_sequences, all_targets, target_dates\n",
    ")\n",
    "\n",
    "\n",
    "def save_sequences(X, y, prefix):\n",
    "    if len(X) == 0:\n",
    "        logger.warning(f\"{prefix} set is empty\")\n",
    "        return\n",
    "    \n",
    "    X_flat = X.reshape(X.shape[0], -1)\n",
    "    feature_names = [f\"{feat}_t{t}\" for t in range(SEQUENCE_LENGTH) for feat in FEATURE_COLS]\n",
    "    \n",
    "    try:\n",
    "        pd.DataFrame(X_flat, columns=feature_names).to_parquet(\n",
    "            os.path.join(SAVE_DIR, f\"{prefix}_X.parquet\"), index=False\n",
    "        )\n",
    "        pd.DataFrame(y, columns=[TARGET_COL]).to_parquet(\n",
    "            os.path.join(SAVE_DIR, f\"{prefix}_y.parquet\"), index=False\n",
    "        )\n",
    "        logger.info(f\"Saved {prefix} sequences to {SAVE_DIR}/{prefix}_[X|y].parquet\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save {prefix} sequences: {e}\")\n",
    "        raise\n",
    "\n",
    "save_sequences(X_train, y_train, 'train')\n",
    "save_sequences(X_val, y_val, 'val')\n",
    "save_sequences(X_test, y_test, 'test')\n",
    "\n",
    "logger.info(\"\\n=== Data Shapes ===\")\n",
    "logger.info(f\"Original: {original_df.shape}\")\n",
    "logger.info(f\"Prepared: {prepared_df.shape}\")\n",
    "logger.info(f\"Train: {X_train.shape}, y: {y_train.shape}\")\n",
    "logger.info(f\"Val: {X_val.shape}, y: {y_val.shape}\")\n",
    "logger.info(f\"Test: {X_test.shape}, y: {y_test.shape}\")\n",
    "\n",
    "logger.info(\"\\nDate ranges:\")\n",
    "logger.info(f\"Train: {target_dates[target_dates <= 26].min()} to {target_dates[target_dates <= 26].max()}\")\n",
    "logger.info(f\"Val: {target_dates[(target_dates > 26) & (target_dates <= 29)].min()} to {target_dates[(target_dates > 26) & (target_dates <= 29)].max()}\")\n",
    "logger.info(f\"Test: {target_dates[target_dates > 29].min()} to {target_dates[target_dates > 29].max()}\")\n",
    "\n",
    "logger.info(\"\\nSaved feature columns:\")\n",
    "logger.info([f\"{feat}_t{t}\" for t in range(SEQUENCE_LENGTH) for feat in FEATURE_COLS])\n",
    "\n",
    "for prefix in ['train', 'val', 'test']:\n",
    "    x_path = Path(SAVE_DIR) / f\"{prefix}_X.parquet\"\n",
    "    y_path = Path(SAVE_DIR) / f\"{prefix}_y.parquet\"\n",
    "    if x_path.exists() and y_path.exists():\n",
    "        logger.info(f\"✓ Verified: {x_path.name} and {y_path.name} exist\")\n",
    "    else:\n",
    "        logger.error(f\"✗ Missing: {x_path.name} or {y_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2c035bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 09:05:24,715 - INFO - Using device: mps\n",
      "2025-06-02 09:05:24,723 - INFO - Initiating program execution\n",
      "2025-06-02 09:05:24,725 - INFO - Loading sample of test data for verification...\n",
      "2025-06-02 09:05:24,855 - INFO - Test data sample:\n",
      "shape: (5, 36)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ item_cnt_ ┆ item_pric ┆ Return_t0 ┆ item_cnt_ ┆ … ┆ item_cnt_ ┆ shop_id_m ┆ item_id_m ┆ item_cat │\n",
      "│ month_t0  ┆ e_t0      ┆ ---       ┆ month_lag ┆   ┆ month_mea ┆ ean_encod ┆ ean_encod ┆ egory_id │\n",
      "│ ---       ┆ ---       ┆ f32       ┆ _1_t0     ┆   ┆ n_item_t2 ┆ e_t2      ┆ e_t2      ┆ _mean_en │\n",
      "│ f32       ┆ f32       ┆           ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ code_t…  │\n",
      "│           ┆           ┆           ┆ f32       ┆   ┆ f32       ┆ f32       ┆ f32       ┆ ---      │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ f32      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ -0.401862 ┆ 0.499485  ┆ 0.0       ┆ 0.804708  ┆ … ┆ -0.39393  ┆ -0.055747 ┆ 0.308244  ┆ -0.20948 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 6        │\n",
      "│ -0.015112 ┆ -0.611196 ┆ 0.0       ┆ 0.017729  ┆ … ┆ -0.071024 ┆ -0.055747 ┆ 0.271668  ┆ -0.08273 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 4        │\n",
      "│ -0.401862 ┆ -0.403677 ┆ 0.0       ┆ -0.375761 ┆ … ┆ -0.443699 ┆ -0.055747 ┆ -0.194529 ┆ -0.20948 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 6        │\n",
      "│ -0.401862 ┆ -0.403677 ┆ 0.0       ┆ -0.375761 ┆ … ┆ -0.417259 ┆ -0.055747 ┆ -0.194529 ┆ -0.20948 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 6        │\n",
      "│ -0.401862 ┆ 1.618739  ┆ 0.0       ┆ 0.017729  ┆ … ┆ 0.532169  ┆ -0.055747 ┆ 0.453643  ┆ 0.131595 │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "2025-06-02 09:05:24,895 - INFO - Test data shape: (77414, 36)\n",
      "2025-06-02 09:05:24,900 - INFO - Test y shape: (77414, 1)\n",
      "2025-06-02 09:05:25,020 - INFO - Columns in /Users/mohammednihal/XAI-1/training_data/train_X.parquet: ['item_cnt_month_t0', 'item_price_t0', 'Return_t0', 'item_cnt_month_lag_1_t0', 'item_cnt_month_lag_2_t0', 'item_cnt_month_lag_3_t0', 'item_cnt_month_mean_category_t0', 'item_cnt_month_mean_shop_t0', 'item_cnt_month_mean_item_t0', 'shop_id_mean_encode_t0', 'item_id_mean_encode_t0', 'item_category_id_mean_encode_t0', 'item_cnt_month_t1', 'item_price_t1', 'Return_t1', 'item_cnt_month_lag_1_t1', 'item_cnt_month_lag_2_t1', 'item_cnt_month_lag_3_t1', 'item_cnt_month_mean_category_t1', 'item_cnt_month_mean_shop_t1', 'item_cnt_month_mean_item_t1', 'shop_id_mean_encode_t1', 'item_id_mean_encode_t1', 'item_category_id_mean_encode_t1', 'item_cnt_month_t2', 'item_price_t2', 'Return_t2', 'item_cnt_month_lag_1_t2', 'item_cnt_month_lag_2_t2', 'item_cnt_month_lag_3_t2', 'item_cnt_month_mean_category_t2', 'item_cnt_month_mean_shop_t2', 'item_cnt_month_mean_item_t2', 'shop_id_mean_encode_t2', 'item_id_mean_encode_t2', 'item_category_id_mean_encode_t2']\n",
      "2025-06-02 09:05:25,042 - INFO - Dataset size: 573640, X shape: (573640, 36), y shape: (573640, 1)\n",
      "2025-06-02 09:05:25,514 - INFO - Columns in /Users/mohammednihal/XAI-1/training_data/val_X.parquet: ['item_cnt_month_t0', 'item_price_t0', 'Return_t0', 'item_cnt_month_lag_1_t0', 'item_cnt_month_lag_2_t0', 'item_cnt_month_lag_3_t0', 'item_cnt_month_mean_category_t0', 'item_cnt_month_mean_shop_t0', 'item_cnt_month_mean_item_t0', 'shop_id_mean_encode_t0', 'item_id_mean_encode_t0', 'item_category_id_mean_encode_t0', 'item_cnt_month_t1', 'item_price_t1', 'Return_t1', 'item_cnt_month_lag_1_t1', 'item_cnt_month_lag_2_t1', 'item_cnt_month_lag_3_t1', 'item_cnt_month_mean_category_t1', 'item_cnt_month_mean_shop_t1', 'item_cnt_month_mean_item_t1', 'shop_id_mean_encode_t1', 'item_id_mean_encode_t1', 'item_category_id_mean_encode_t1', 'item_cnt_month_t2', 'item_price_t2', 'Return_t2', 'item_cnt_month_lag_1_t2', 'item_cnt_month_lag_2_t2', 'item_cnt_month_lag_3_t2', 'item_cnt_month_mean_category_t2', 'item_cnt_month_mean_shop_t2', 'item_cnt_month_mean_item_t2', 'shop_id_mean_encode_t2', 'item_id_mean_encode_t2', 'item_category_id_mean_encode_t2']\n",
      "2025-06-02 09:05:25,516 - INFO - Dataset size: 59474, X shape: (59474, 36), y shape: (59474, 1)\n",
      "2025-06-02 09:05:25,598 - INFO - Columns in /Users/mohammednihal/XAI-1/training_data/test_X.parquet: ['item_cnt_month_t0', 'item_price_t0', 'Return_t0', 'item_cnt_month_lag_1_t0', 'item_cnt_month_lag_2_t0', 'item_cnt_month_lag_3_t0', 'item_cnt_month_mean_category_t0', 'item_cnt_month_mean_shop_t0', 'item_cnt_month_mean_item_t0', 'shop_id_mean_encode_t0', 'item_id_mean_encode_t0', 'item_category_id_mean_encode_t0', 'item_cnt_month_t1', 'item_price_t1', 'Return_t1', 'item_cnt_month_lag_1_t1', 'item_cnt_month_lag_2_t1', 'item_cnt_month_lag_3_t1', 'item_cnt_month_mean_category_t1', 'item_cnt_month_mean_shop_t1', 'item_cnt_month_mean_item_t1', 'shop_id_mean_encode_t1', 'item_id_mean_encode_t1', 'item_category_id_mean_encode_t1', 'item_cnt_month_t2', 'item_price_t2', 'Return_t2', 'item_cnt_month_lag_1_t2', 'item_cnt_month_lag_2_t2', 'item_cnt_month_lag_3_t2', 'item_cnt_month_mean_category_t2', 'item_cnt_month_mean_shop_t2', 'item_cnt_month_mean_item_t2', 'shop_id_mean_encode_t2', 'item_id_mean_encode_t2', 'item_category_id_mean_encode_t2']\n",
      "2025-06-02 09:05:25,599 - INFO - Dataset size: 77414, X shape: (77414, 36), y shape: (77414, 1)\n",
      "Epoch 1: 100%|██████████| 4482/4482 [01:04<00:00, 69.12it/s]\n",
      "2025-06-02 09:06:33,531 - INFO - Epoch 1, Train MSE: 0.5871, RMSE: 0.7662, MAE: 0.4395, Val MSE: 0.2987, RMSE: 0.5465, MAE: 0.3066\n",
      "Epoch 2: 100%|██████████| 4482/4482 [01:01<00:00, 72.30it/s]\n",
      "2025-06-02 09:07:37,820 - INFO - Epoch 2, Train MSE: 0.4705, RMSE: 0.6860, MAE: 0.3873, Val MSE: 0.2955, RMSE: 0.5436, MAE: 0.2851\n",
      "Epoch 3: 100%|██████████| 4482/4482 [01:01<00:00, 72.57it/s]\n",
      "2025-06-02 09:08:41,839 - INFO - Epoch 3, Train MSE: 0.4574, RMSE: 0.6763, MAE: 0.3809, Val MSE: 0.2918, RMSE: 0.5402, MAE: 0.2944\n",
      "Epoch 4: 100%|██████████| 4482/4482 [01:01<00:00, 72.48it/s]\n",
      "2025-06-02 09:09:45,936 - INFO - Epoch 4, Train MSE: 0.4511, RMSE: 0.6716, MAE: 0.3777, Val MSE: 0.2912, RMSE: 0.5396, MAE: 0.2889\n",
      "Epoch 5: 100%|██████████| 4482/4482 [01:01<00:00, 72.73it/s]\n",
      "2025-06-02 09:10:49,806 - INFO - Epoch 5, Train MSE: 0.4443, RMSE: 0.6665, MAE: 0.3747, Val MSE: 0.2978, RMSE: 0.5457, MAE: 0.2988\n",
      "Epoch 6: 100%|██████████| 4482/4482 [01:13<00:00, 60.67it/s]\n",
      "2025-06-02 09:12:05,954 - INFO - Epoch 6, Train MSE: 0.4399, RMSE: 0.6632, MAE: 0.3730, Val MSE: 0.2861, RMSE: 0.5349, MAE: 0.2926\n",
      "Epoch 7: 100%|██████████| 4482/4482 [01:02<00:00, 71.42it/s]\n",
      "2025-06-02 09:13:10,988 - INFO - Epoch 7, Train MSE: 0.4368, RMSE: 0.6609, MAE: 0.3715, Val MSE: 0.2887, RMSE: 0.5374, MAE: 0.2920\n",
      "Epoch 8: 100%|██████████| 4482/4482 [01:12<00:00, 61.82it/s]\n",
      "2025-06-02 09:14:26,911 - INFO - Epoch 8, Train MSE: 0.4330, RMSE: 0.6580, MAE: 0.3703, Val MSE: 0.2905, RMSE: 0.5390, MAE: 0.2896\n",
      "Epoch 9: 100%|██████████| 4482/4482 [01:24<00:00, 53.15it/s]\n",
      "2025-06-02 09:15:54,345 - INFO - Epoch 9, Train MSE: 0.4316, RMSE: 0.6570, MAE: 0.3697, Val MSE: 0.2911, RMSE: 0.5395, MAE: 0.2909\n",
      "Epoch 10: 100%|██████████| 4482/4482 [01:07<00:00, 66.39it/s]\n",
      "2025-06-02 09:17:04,096 - INFO - Epoch 10, Train MSE: 0.4282, RMSE: 0.6543, MAE: 0.3687, Val MSE: 0.2899, RMSE: 0.5384, MAE: 0.2961\n",
      "Epoch 11: 100%|██████████| 4482/4482 [01:12<00:00, 62.03it/s]\n",
      "2025-06-02 09:18:18,623 - INFO - Epoch 11, Train MSE: 0.4264, RMSE: 0.6530, MAE: 0.3680, Val MSE: 0.3050, RMSE: 0.5522, MAE: 0.3179\n",
      "2025-06-02 09:18:18,625 - INFO - Early stopping triggered after 11 epochs\n",
      "Predicting: 100%|██████████| 605/605 [00:05<00:00, 101.84it/s]\n",
      "2025-06-02 09:18:43,658 - INFO - Test MSE: 0.3310, RMSE: 0.5753, MAE: 0.3000\n",
      "2025-06-02 09:18:45,128 - INFO - Program executed successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Setup logging\n",
    "log_dir = Path('logs')\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_dir / f'run_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "class SalesDataset(Dataset):\n",
    "    def __init__(self, X_file, y_file, sequence_length=3):\n",
    "        if not Path(X_file).exists() or not Path(y_file).exists():\n",
    "            raise FileNotFoundError(f\"Data files not found: {X_file}, {y_file}\")\n",
    "        \n",
    "        self.X_file = X_file\n",
    "        self.y_file = y_file\n",
    "        \n",
    "        # Load data\n",
    "        self.X = pl.read_parquet(X_file)\n",
    "        feature_cols = [\n",
    "            'item_cnt_month', 'item_price', 'Return',\n",
    "            'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3',\n",
    "            'item_cnt_month_mean_shop', 'item_cnt_month_mean_item', 'item_cnt_month_mean_category',\n",
    "            'shop_id_mean_encode', 'item_id_mean_encode', 'item_category_id_mean_encode'\n",
    "        ]\n",
    "        input_cols = [f\"{feat}_t{t}\" for t in range(sequence_length) for feat in feature_cols]\n",
    "        \n",
    "        # Verify columns\n",
    "        available_cols = self.X.columns\n",
    "        logger.info(f\"Columns in {X_file}: {available_cols}\")\n",
    "        missing_cols = [col for col in input_cols if col not in available_cols]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns in {X_file}: {missing_cols}\")\n",
    "        \n",
    "        self.y = pl.read_parquet(y_file).to_numpy().astype(np.float32).reshape(-1, 1)\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        \n",
    "        logger.info(f\"Dataset size: {len(self.indices)}, X shape: {self.X.shape}, y shape: {self.y.shape}\")\n",
    "        self.dates = [datetime(2013, 1, 1) + timedelta(days=30 * (int(idx) % 120)) for idx in self.indices]\n",
    "        if len(self.X) != len(self.y):\n",
    "            raise ValueError(f\"X and y length mismatch: {len(self.X)} vs {len(self.y)}\")\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.numerical_cols = input_cols\n",
    "        \n",
    "        numerical_data = self.X.select(self.numerical_cols).to_numpy().astype(np.float32)\n",
    "        numerical_data = np.nan_to_num(numerical_data, nan=0.0)\n",
    "        self.y = np.nan_to_num(self.y, nan=0.0)\n",
    "        \n",
    "        numerical_data = numerical_data.reshape(len(self.X), sequence_length, len(feature_cols))\n",
    "        self.numerical = numerical_data\n",
    "        \n",
    "        self.identifiers = self.indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        numerical = torch.tensor(self.numerical[idx], dtype=torch.float32)  # Shape: (3, 12)\n",
    "        target = torch.tensor(self.y[idx], dtype=torch.float32)  # Shape: (1,)\n",
    "        identifiers = torch.tensor([self.identifiers[idx]], dtype=torch.int32)\n",
    "        dates = self.dates[idx]\n",
    "        return {\n",
    "            'numerical': numerical, 'target': target, 'identifiers': identifiers, 'dates': dates\n",
    "        }\n",
    "\n",
    "class HALSTM(nn.Module):\n",
    "    def __init__(self, numerical_dim=12, hidden_dim=128, num_layers=2, num_heads=4, dropout=0.3, l2_lambda=0.01):\n",
    "        super(HALSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.l2_lambda = l2_lambda\n",
    "        \n",
    "        self.input_dim = numerical_dim\n",
    "        self.lstm = nn.LSTM(self.input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.lstm_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.mha = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.mha_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.gate = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc_shared = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.positional_encoding = torch.zeros(3, hidden_dim).to(device)\n",
    "        position = torch.arange(0, 3, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / hidden_dim))\n",
    "        self.positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LSTM):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        nn.init.xavier_normal_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        nn.init.constant_(param, 0)\n",
    "    \n",
    "    def forward(self, numerical):\n",
    "        batch_size, seq_len, _ = numerical.size()\n",
    "        x = self.dropout(numerical)\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        lstm_out = self.lstm_norm(lstm_out)\n",
    "        \n",
    "        lstm_out = lstm_out + self.positional_encoding[:seq_len, :].unsqueeze(0)\n",
    "        \n",
    "        mha_out, mha_weights = self.mha(lstm_out, lstm_out, lstm_out)\n",
    "        mha_out = self.mha_norm(mha_out)\n",
    "        \n",
    "        combined = torch.cat([lstm_out[:, -1, :], mha_out[:, -1, :]], dim=-1)\n",
    "        gate = self.sigmoid(self.gate(combined))\n",
    "        fused = gate * lstm_out[:, -1, :] + (1 - gate) * mha_out[:, -1, :]\n",
    "        \n",
    "        shared = self.relu(self.fc_shared(fused))\n",
    "        output = self.fc_out(shared)\n",
    "        \n",
    "        return output, {'mha_weights': mha_weights, 'gate_weights': gate}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    if not batch:\n",
    "        return {}\n",
    "    return {\n",
    "        'numerical': torch.stack([item['numerical'] for item in batch]),\n",
    "        'target': torch.stack([item['target'] for item in batch]),\n",
    "        'identifiers': torch.stack([item['identifiers'] for item in batch]),\n",
    "        'dates': [item['dates'] for item in batch]\n",
    "    }\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=15, lr=0.0005, accum_steps=2):\n",
    "    criterion_mse = nn.MSELoss().to(device)\n",
    "    criterion_mae = nn.L1Loss().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=model.l2_lambda)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=num_epochs, steps_per_epoch=len(train_loader)//accum_steps)\n",
    "    scaler = torch.cuda.amp.GradScaler() if device.type in ['cuda', 'mps'] else None\n",
    "    \n",
    "    metrics = {'epoch': [], 'train_mse': [], 'train_rmse': [], 'train_mae': [], 'val_mse': [], 'val_rmse': [], 'val_mae': []}\n",
    "    best_val_loss = float('inf')\n",
    "    output_dir = Path('results')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    patience = 5\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_mse = 0\n",
    "        train_mae = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "            numerical = batch['numerical'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            \n",
    "            if scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output, _ = model(numerical)\n",
    "                    mse_loss = criterion_mse(output, target) / accum_steps\n",
    "                    mae_loss = criterion_mae(output, target) / accum_steps\n",
    "                    loss = mse_loss\n",
    "                scaler.scale(loss).backward()\n",
    "                if (batch_idx + 1) % accum_steps == 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                    scaler.step(optimizer)\n",
    "                    scheduler.step()\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "            else:\n",
    "                output, _ = model(numerical)\n",
    "                mse_loss = criterion_mse(output, target) / accum_steps\n",
    "                mae_loss = criterion_mae(output, target) / accum_steps\n",
    "                loss = mse_loss\n",
    "                loss.backward()\n",
    "                if (batch_idx + 1) % accum_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += mse_loss.item() * accum_steps\n",
    "            train_mse += mse_loss.item() * accum_steps\n",
    "            train_mae += mae_loss.item() * accum_steps\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_mse /= len(train_loader)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        train_mae /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_mse = 0\n",
    "        val_mae = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                numerical = batch['numerical'].to(device)\n",
    "                target = batch['target'].to(device)\n",
    "                output, _ = model(numerical)\n",
    "                mse_loss = criterion_mse(output, target)\n",
    "                mae_loss = criterion_mae(output, target)\n",
    "                val_loss += mse_loss.item()\n",
    "                val_mse += mse_loss.item()\n",
    "                val_mae += mae_loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_mse /= len(val_loader)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        val_mae /= len(val_loader)\n",
    "        \n",
    "        metrics['epoch'].append(epoch + 1)\n",
    "        metrics['train_mse'].append(train_mse)\n",
    "        metrics['train_rmse'].append(train_rmse)\n",
    "        metrics['train_mae'].append(train_mae)\n",
    "        metrics['val_mse'].append(val_mse)\n",
    "        metrics['val_rmse'].append(val_rmse)\n",
    "        metrics['val_mae'].append(val_mae)\n",
    "        logger.info(f\"Epoch {epoch+1}, Train MSE: {train_mse:.4f}, RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, \"\n",
    "                    f\"Val MSE: {val_mse:.4f}, RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), output_dir / 'best_ha_lstm.pth')\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_df.to_csv(output_dir / 'training_metrics.csv', index=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(metrics['epoch'], metrics['train_rmse'], label='Train RMSE')\n",
    "    plt.plot(metrics['epoch'], metrics['val_rmse'], label='Val RMSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('Training and Validation RMSE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(metrics['epoch'], metrics['train_mae'], label='Train MAE')\n",
    "    plt.plot(metrics['epoch'], metrics['val_mae'], label='Val MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title('Training and Validation MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'metrics_plot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    model.load_state_dict(torch.load(output_dir / 'best_ha_lstm.pth'))\n",
    "    return model, metrics_df\n",
    "\n",
    "def predict(model, test_loader, dataset):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    interpret_outputs = []\n",
    "    modalities = ['numerical']\n",
    "    \n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_mae = nn.L1Loss()\n",
    "    test_mse = 0\n",
    "    test_mae = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Predicting\")):\n",
    "            numerical = batch['numerical'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            identifiers = batch['identifiers']\n",
    "            dates = batch['dates']\n",
    "            \n",
    "            output, attn_dict = model(numerical)\n",
    "            preds = output.cpu().numpy()\n",
    "            \n",
    "            mse_loss = criterion_mse(output, target)\n",
    "            mae_loss = criterion_mae(output, target)\n",
    "            batch_size = output.size(0)\n",
    "            test_mse += mse_loss.item() * batch_size\n",
    "            test_mae += mae_loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            mha_weights = attn_dict['mha_weights'][:, -1, :].cpu().numpy()\n",
    "            gate_weights = attn_dict['gate_weights'].cpu().numpy()\n",
    "            \n",
    "            for i in range(len(preds)):\n",
    "                pred_dict = {\n",
    "                    'index': identifiers[i][0].item(),\n",
    "                    'item_cnt_month': preds[i][0]\n",
    "                }\n",
    "                predictions.append(pred_dict)\n",
    "                \n",
    "                interpret_outputs.append({\n",
    "                    'timestamp_reference': dates[i].isoformat() if isinstance(dates[i], datetime) else str(dates[i]),\n",
    "                    'forecasted_value': preds[i].tolist(),\n",
    "                    'fusion_weights': gate_weights[i].tolist(),\n",
    "                    'attention_weights': mha_weights[i].tolist(),\n",
    "                    'input_sequence_dates': [dates[i].isoformat()] if isinstance(dates[i], datetime) else [str(dates[i])],\n",
    "                    'modalities_used': modalities,\n",
    "                    'gating_decision_output': gate_weights[i].tolist()\n",
    "                })\n",
    "    \n",
    "    test_mse /= total_samples\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_mae /= total_samples\n",
    "    \n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    interpret_df = pd.DataFrame(interpret_outputs)\n",
    "    \n",
    "    output_dir = Path('results')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    pred_df.to_csv(output_dir / 'predictions.csv', index=False)\n",
    "    interpret_df.to_csv(output_dir / 'interpretability_outputs.csv', index=False)\n",
    "    \n",
    "    logger.info(f\"Test MSE: {test_mse:.4f}, RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}\")\n",
    "    \n",
    "    return pred_df, {'mse': test_mse, 'rmse': test_rmse, 'mae': test_mae}, interpret_df\n",
    "\n",
    "def visualize_results(pred_df, y_test):\n",
    "    output_dir = Path('results')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    if len(pred_df) != len(y_test):\n",
    "        logger.error(f\"Size mismatch: pred_df ({len(pred_df)}) vs y_test ({len(y_test)})\")\n",
    "        min_len = min(len(pred_df), len(y_test))\n",
    "        pred_df = pred_df.iloc[:min_len]\n",
    "        y_test = y_test[:min_len]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.kdeplot(pred_df['item_cnt_month'], label='Predicted item_cnt_month')\n",
    "    sns.kdeplot(y_test, label='Actual item_cnt_month')\n",
    "    plt.title('Prediction vs. Actual Distribution')\n",
    "    plt.xlabel('Item Count')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_test, pred_df['item_cnt_month'], alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual item_cnt_month')\n",
    "    plt.ylabel('Predicted item_cnt_month')\n",
    "    plt.title('Actual vs. Predicted Sales')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'prediction_visualization.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    data_dir = Path('/Users/mohammednihal/XAI-1/lstm_data')\n",
    "    train_X_path = data_dir / '/Users/mohammednihal/XAI-1/training_data/train_X.parquet'\n",
    "    train_y_path = data_dir / '/Users/mohammednihal/XAI-1/training_data/train_y.parquet'\n",
    "    val_X_path = data_dir / '/Users/mohammednihal/XAI-1/training_data/val_X.parquet'\n",
    "    val_y_path = data_dir / '/Users/mohammednihal/XAI-1/training_data/val_y.parquet'\n",
    "    test_X_path = data_dir / '/Users/mohammednihal/XAI-1/training_data/test_X.parquet'\n",
    "    test_y_path = data_dir / '/Users/mohammednihal/XAI-1/training_data/test_y.parquet'\n",
    "    \n",
    "    batch_size = 128\n",
    "    num_workers = 0\n",
    "    num_epochs = 15\n",
    "    lr = 0.0005\n",
    "    accum_steps = 2\n",
    "    \n",
    "    logger.info(\"Loading sample of test data for verification...\")\n",
    "    test_sample = pl.read_parquet(test_X_path).head(5)\n",
    "    logger.info(f\"Test data sample:\\n{test_sample}\")\n",
    "    logger.info(f\"Test data shape: {pl.read_parquet(test_X_path).shape}\")\n",
    "    logger.info(f\"Test y shape: {pl.read_parquet(test_y_path).shape}\")\n",
    "    \n",
    "    train_dataset = SalesDataset(train_X_path, train_y_path, sequence_length=3)\n",
    "    val_dataset = SalesDataset(val_X_path, val_y_path, sequence_length=3)\n",
    "    test_dataset = SalesDataset(test_X_path, test_y_path, sequence_length=3)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    model = HALSTM(numerical_dim=12, hidden_dim=128, num_layers=2, num_heads=4, dropout=0.3, l2_lambda=0.01).to(device)\n",
    "    \n",
    "    model, metrics_df = train_model(model, train_loader, val_loader, num_epochs, lr, accum_steps)\n",
    "    \n",
    "    pred_df, test_metrics, interpret_df = predict(model, test_loader, test_dataset)\n",
    "    \n",
    "    y_test = pl.read_parquet(test_y_path).to_pandas()['item_cnt_month'].values\n",
    "    visualize_results(pred_df, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(\"Initiating program execution\")\n",
    "        main()\n",
    "        logger.info(\"Program executed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Program execution failed: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
